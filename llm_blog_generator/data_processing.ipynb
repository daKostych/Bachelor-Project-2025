{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T22:36:27.429650Z",
     "start_time": "2025-03-04T22:36:27.409322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data_loader import *\n",
    "from src import random_seed\n",
    "\n",
    "data = load_dataset()\n",
    "Xval, Xtest, yval, ytest = validation_test_split(data)\n",
    "print(f\"Size of validation set, X: {Xval.shape}, y: {yval.shape}\")\n",
    "print(f\"Size of test set, X: {Xtest.shape}, y: {ytest.shape}\")"
   ],
   "id": "c450ff8a4892fae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation set, X: (30, 10), y: (30,)\n",
      "Size of test set, X: (20, 10), y: (20,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T22:36:28.159419Z",
     "start_time": "2025-03-04T22:36:28.138138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_blog_info = Xval.sort_values([\"engagement_score\"], ascending=False).head(1)\n",
    "best_blog_index = best_blog_info.index[0]\n",
    "best_blog_info = best_blog_info.reset_index(drop=True)\n",
    "best_blog_info"
   ],
   "id": "947b2f95eaf6ae23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id         title_blog                                           url_blog  \\\n",
       "0  28  Towards Reasoning  https://medium.com/@saptarshichaudhuri/towards...   \n",
       "\n",
       "           author_blog  author_followers  claps  comments  \\\n",
       "0  Saptarshi Chaudhuri               127    461         1   \n",
       "\n",
       "                                         title_paper  \\\n",
       "0  GSM-Symbolic: Understanding the Limitations of...   \n",
       "\n",
       "                                           url_paper  engagement_score  \n",
       "0  https://arxiv.org/pdf/2410.05229?source=post_p...          3.653543  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_blog</th>\n",
       "      <th>url_blog</th>\n",
       "      <th>author_blog</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>claps</th>\n",
       "      <th>comments</th>\n",
       "      <th>title_paper</th>\n",
       "      <th>url_paper</th>\n",
       "      <th>engagement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Towards Reasoning</td>\n",
       "      <td>https://medium.com/@saptarshichaudhuri/towards...</td>\n",
       "      <td>Saptarshi Chaudhuri</td>\n",
       "      <td>127</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>GSM-Symbolic: Understanding the Limitations of...</td>\n",
       "      <td>https://arxiv.org/pdf/2410.05229?source=post_p...</td>\n",
       "      <td>3.653543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T22:36:29.811659Z",
     "start_time": "2025-03-04T22:36:29.799997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "worst_blog_info = Xval.sort_values([\"engagement_score\"], ascending=False).tail(1)\n",
    "worst_blog_index = worst_blog_info.index[0]\n",
    "worst_blog_info = worst_blog_info.reset_index(drop=True)\n",
    "worst_blog_info"
   ],
   "id": "8904bb60-4deb-4ac8-bf27-1b4aa58928df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                         title_blog  \\\n",
       "0  18  SMoA: Improving Multi-agent Large Language Mod...   \n",
       "\n",
       "                                            url_blog  author_blog  \\\n",
       "0  https://medium.com/@sulbha.jindal/smoa-improvi...  Sulbha Jain   \n",
       "\n",
       "   author_followers  claps  comments  \\\n",
       "0                41      0         0   \n",
       "\n",
       "                                         title_paper  \\\n",
       "0  SMoA: Improving Multi-agent Large Language Mod...   \n",
       "\n",
       "                          url_paper  engagement_score  \n",
       "0  https://arxiv.org/pdf/2411.03284               0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_blog</th>\n",
       "      <th>url_blog</th>\n",
       "      <th>author_blog</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>claps</th>\n",
       "      <th>comments</th>\n",
       "      <th>title_paper</th>\n",
       "      <th>url_paper</th>\n",
       "      <th>engagement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>SMoA: Improving Multi-agent Large Language Mod...</td>\n",
       "      <td>https://medium.com/@sulbha.jindal/smoa-improvi...</td>\n",
       "      <td>Sulbha Jain</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SMoA: Improving Multi-agent Large Language Mod...</td>\n",
       "      <td>https://arxiv.org/pdf/2411.03284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "82394efd-c053-4a65-b737-be97aa6c2f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T22:37:00.457259Z",
     "start_time": "2025-03-04T22:37:00.450549Z"
    }
   },
   "source": [
    "blog = data.iloc[16]\n",
    "blog"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                            17\n",
       "title_blog                     Self-Generated Critiques Boost Reward Modeling...\n",
       "url_blog                       https://medium.com/@sulbha.jindal/self-generat...\n",
       "author_blog                                                          Sulbha Jain\n",
       "author_followers                                                              41\n",
       "claps                                                                         21\n",
       "comments                                                                       0\n",
       "title_paper                    Self-Generated Critiques Boost Reward Modeling...\n",
       "url_paper                                       https://arxiv.org/pdf/2411.16646\n",
       "engagement_score                                                        0.512195\n",
       "normalized_engagement_score                                                 49.0\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ee75bcb6-a9af-44c5-b459-0412d51cb3a9",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T22:41:56.174100Z",
     "start_time": "2025-03-04T22:41:48.579472Z"
    }
   },
   "source": [
    "from src.text_extraction import extract_blog_text\n",
    "\n",
    "blog_text = extract_blog_text(blog)\n",
    "print(blog_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Paper — https://arxiv.org/pdf/2411.16646\n",
      "Reinforcement Learning from Human Feedback (RLHF) has become a critical methodology for aligning large language models (LLMs) with human preferences. At the core of RLHF lies the reward model (RM), which is designed to evaluate model outputs by assigning scores that reflect their alignment with human judgments. These scores guide the optimization process during training, such as providing reward signals in Proximal Policy Optimization (PPO), thereby encouraging LLMs to generate responses that are more helpful, honest, and harmless. This iterative process enhances the practical quality of LLM outputs in real-world applications.\n",
      "\n",
      "## Current challenge\n",
      "Typically, reward models are trained using preference pairs and optimized through pairwise logistic loss to produce a scalar score for each response. However, this scalar output is often hard to interpret and underutilizes the inherent language modeling capabilities of LLMs derived from pretraining and post-training. These limitations can weaken the feedback signals in RLHF, resulting in suboptimal policy updates. An alternative approach is the “LLM-as-a-judge” paradigm, where the LLM generates critiques and optionally provides discrete scores as proxies for response quality. This method leverages the model’s reasoning abilities more effectively, potentially addressing some of the shortcomings of traditional reward models\n",
      "Incorporating critiques into reward modeling poses two significant challenges. First, there is the issue of conflicting objectives: generating critiques relies on language modeling capabilities, whereas traditional reward models output scalar values, making their integration into the language modeling process complex. Second, there are limitations with evaluators; off-the-shelf language models often lack the effectiveness needed for evaluation, and fine-tuning these models necessitates costly human-generated or annotated critiques. While knowledge distillation offers a potential solution by enabling a joint training approach to learn how to generate critiques and rewards simultaneously, it falls short when it comes to enhancing frontier models in situations where a stronger teacher model is unavailable. Here is Critic-RM, a new framework from Meta Researchers that enhances reward models using synthetic critiques, without relying on strong LLM teachers.\n",
      "\n",
      "## Methodology\n",
      "Critic-RM utilizes an instruction-finetuned large language model (LLM) as its foundation, generating multiple candidate critiques, each accompanied by a discrete score for individual responses. The process begins with a consistency-guided filtering technique that retains only those critiques whose scores align with human-annotated preference labels.\n",
      "To further improve the quality of these synthetic critiques, two additional strategies — summarization and ranking — are proposed to refine the critiques used in training the reward model. The framework investigates the application of an off-the-shelf instruction-finetuned LLM, for both critique generation and reward modeling. Initially, Critic-RM generates candidate critiques for each prompt-response pair, followed by a filtering step aimed at minimizing the influence of potentially noisy rationales that could lead to incorrect predictions. This approach allows for the augmentation of preference pairs with additional critiques, ultimately enhancing the precision of reward modeling.\n",
      "After generating critiques for each response, the primary challenge is developing an effective training strategy to integrate critique modeling and scalar reward prediction objectives. To address this, we propose a simple weighting strategy that balances these objectives. Initially, the model prioritizes critique modeling loss, then gradually shifts its focus toward reward prediction, utilizing both the response and the critique. This balanced approach enables the model to excel in generating high-quality critiques while maintaining accurate reward predictions.\n",
      "In Critic-RM, an additional step is introduced during inference for each (prompt, response) pair. Given a (prompt, response) pair $$(x, y)$$, the model first generates a critique $$z \\sim q_\\phi(x, y)$$. It then predicts the reward for the response as $$r = r_\\psi(x, [y, z])$$, where the reward prediction incorporates both the response and its associated critique. This process ensures a more nuanced and precise evaluation of responses.\n",
      "\n",
      "## Results\n",
      "Incorporating critiques into reward modeling has demonstrated significant benefits, particularly with the Critic-RM framework, which consistently outperforms the baselines in this study. Specifically, when trained on the same preference data, Critic-RM achieves an improvement of 3.7% to 4.7% over standard reward models. The quality of critiques plays a crucial role; comparisons reveal that other models incorporating critiques show smaller performance gains than Critic-RM when evaluated against the standard reward model. Additionally, performance enhancements are observed for both Critic-RM and its baselines when multiple critiques are generated during inference, particularly benefiting reasoning tasks.\n",
      "\n",
      "## Summary\n",
      "Researchers have developed Critic-RM, an innovative self-critiquing framework aimed at enhancing reward modeling for large language models (LLMs). This novel approach leverages the inherent capabilities of LLMs to generate and refine critiques, implementing a self-improvement mechanism that enhances both the quality of critiques and the accuracy of reward predictions. The findings from this research emphasize the efficacy of Critic-RM in improving reward modeling accuracy and underscore the crucial role of high-quality critiques in this process. Beyond merely enhancing precision in reward modeling, the framework has demonstrated robust performance across various benchmarks, including RewardBench and CrossEval, highlighting its potential to significantly advance the field of language model optimization and alignment with human preferences.\n",
      "\n",
      "## Appendix\n",
      "    * Paper link — https://arxiv.org/pdf/2411.16646\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "8f56f705-0918-4d87-b14e-07db46e1e5af",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T22:41:56.604197Z",
     "start_time": "2025-03-04T22:41:56.248065Z"
    }
   },
   "source": [
    "from src.text_extraction import extract_paper_text\n",
    "\n",
    "paper_url = data[\"url_paper\"][16]\n",
    "paper_text = extract_paper_text(paper_url)\n",
    "print(paper_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Generated Critiques Boost Reward Modeling\n",
      "for Language Models\n",
      "Yue Yu1,2,∗, Zhengxing Chen1, Aston Zhang1, Liang Tan1, Chenguang Zhu1, Richard Yuanzhe Pang1, Yundi\n",
      "Qian1, Xuewei Wang1, Suchin Gururangan1, Chao Zhang2, Melanie Kambadur1, Dhruv Mahajan1, Rui Hou1\n",
      "1GenAI, Meta, 2Georgia Institute of Technology\n",
      "∗Work done during the internship at Meta GenAI.\n",
      "Reward modeling is crucial for aligning large language models (LLMs) with human preferences,\n",
      "especially in reinforcement learning from human feedback (RLHF). However, current reward models\n",
      "mainly produce unexplainable scalar scores and struggle to incorporate critiques in a natural language\n",
      "format. We hypothesize that generating both critiques and scalar rewards would improve reward\n",
      "models’ capability on preference ranking. Motivated by this, we propose Critic-RM, a framework\n",
      "that utilizes self-generated, high-quality critiques to train reward models for scalar reward-based\n",
      "preference prediction, with explicit rationales serving as supporting evidence. Critic-RM employs\n",
      "a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on\n",
      "reward prediction and critique generation objectives. Experiments on preference ranking benchmarks\n",
      "including RewardBench and CrossEval show that Critic-RM improves reward modeling accuracy by\n",
      "3.7%–7.3% compared to standard reward models and LLM judges, demonstrating strong performance\n",
      "and data efficiency. Additional studies further validate the effectiveness of the generated critiques in\n",
      "rectifying flawed reasoning steps with the gain of 2.5%-3.2% on improving reasoning accuracy.\n",
      "Date: February 11, 2025\n",
      "Correspondence: Yue Yu (yueyu@gatech.edu), Rui Hou (rayhou@meta.com)\n",
      "1\n",
      "Introduction\n",
      "Reinforcement Learning from Human Feedback (RLHF) has been widely adopted to align large language\n",
      "models (LLMs) with human preferences (Ouyang et al., 2022; Touvron et al., 2023; Dubey et al., 2024; Reid\n",
      "et al., 2024). Central to the RLHF process is the reward model (RM), which is trained to assign scores that\n",
      "quantify how well the model’s outputs align with human judgments. The reward model defines optimization\n",
      "direction during training (e.g., reward signal in PPO), encouraging a policy LLM to generate more helpful,\n",
      "honest, and harmless responses ultimately enhancing the model’s generation quality in real-world applications.\n",
      "Standard reward models are typically trained using preference pairs and optimized with pairwise logistic\n",
      "loss (Bradley and Terry, 1952), producing a single scalar score for each response. However, outputting a\n",
      "scalar score not only is hard to interpret but also fails to fully leverage the inherent language modeling\n",
      "capability that LLMs obtain from pretraining and post-training (Zhang et al., 2024). Consequently, these\n",
      "reward models tend to be less data-efficient and prone to robustness issues, such as reward hacking (Skalse\n",
      "et al., 2022; Singhal et al., 2023; Chen et al., 2024). Such limitations hinder the quality of feedback signals in\n",
      "RLHF and lead to suboptimal policy updates. On the other hand, the LLM-as-a-judge paradigm offers an\n",
      "alternative, where the LLM first generates a critique and then optionally provides a discrete score as a quality\n",
      "proxy for a response (Zheng et al., 2023; Kim et al., 2024a; Zhong et al., 2024). Combining the strengths of\n",
      "both paradigms—integrating the interpretability and structured critique of LLM-as-the-judge with the scalar\n",
      "optimization framework of reward models—has the great potential to address the limitations of each method\n",
      "and yield more robust and effective reward signals.\n",
      "Despite its great premise, incorporating critiques into reward modeling presents two major challenges. (1)\n",
      "Conflicting objectives: Critique generation requires language modeling, while reward models provide scalar\n",
      "outputs, complicating its integration into language modeling. (2) Evaluator limitations: Off-the-shelf LMs are\n",
      "often not good evaluators, while additional fine-tuning requires costly human-generated or annotated critiques.\n",
      "1\n",
      "arXiv:2411.16646v3  [cs.CL]  9 Feb 2025\n",
      "\n",
      "Recent work (Ye et al., 2024) directly incorporates critiques generated from off-the-shelf LLMs for reward\n",
      "modeling, while Ankner et al. (2024) and Zhang et al. (2024) design a joint training approach for learning to\n",
      "generate the critique as well as rewards simultaneously via knowledge distillation. These methods typically\n",
      "rely on a strong teacher LLM to generate high-quality critiques, which can be costly and inefficient to obtain\n",
      "at scale in practice. Moreover, they cannot be used to improve frontier models when a stronger teacher model\n",
      "does not exist.\n",
      "We introduce Critic-RM, a new framework that enhances reward models using synthetic critiques, without\n",
      "relying on strong LLM teachers. Our approach draws inspiration from recent advances in self-improving\n",
      "language models (Yuan et al., 2024; Wu et al., 2024; Prasad et al., 2024), where models are iteratively refined\n",
      "using data generated by themselves. To apply a similar LLM self-improving paradigm in reward modeling,\n",
      "we hypothesize that it is crucial to inject LLM’s critique generation ability into this process. Specifically,\n",
      "Critic-RM leverages an instruction-finetuned LLM as the backbone, which generates multiple candidate\n",
      "critiques, each with a discrete score (as explained below, for filtering critiques; not our final reward) for\n",
      "individual responses. However, these critiques can vary in quality, and poor-quality critiques often result\n",
      "in flawed quality predictions. To tackle this issue, we first apply a consistency-guided filtering technique,\n",
      "retaining only critiques whose scores align with human-annotated preference labels1. To further enhance the\n",
      "quality of these synthetic critiques, we additionally propose two strategies, summarization and ranking, to\n",
      "refine the critiques used in training the reward model.\n",
      "Once critiques are generated for each response, the main challenge lies in designing an effective training strategy\n",
      "to combine critique modeling and scalar reward prediction objectives. While LLMs benefit from learning\n",
      "through diverse critiques for each response (Ho et al., 2023), reward modeling is prone to overfitting (Dubey\n",
      "et al., 2024; Zhu et al., 2024); such a contradiction makes it nontrivial to determine the optimal learning steps.\n",
      "To address this issue, we introduce a simple weighting balancing strategy, where the model initially focuses on\n",
      "critique modeling loss, then gradually transitions to predicting rewards based on both the response and the\n",
      "critique. This approach balances the two learning objectives, allowing the model to excel at both high-quality\n",
      "critique generation and accurate reward prediction.\n",
      "To demonstrate the effectiveness of Critic-RM, we conduct extensive experiments on RewardBench and three\n",
      "out-of-distribution reward modeling tasks, showing that Critic-RM outperforms baselines in both in-domain\n",
      "and out-of-domain evaluations. Additionally, experiments on critique evaluation benchmarks highlight Critic-\n",
      "RM’s ability to generate valuable feedback for correcting LLMs’ flawed reasoning. Our analysis confirms that\n",
      "Critic-RM’s superior generalization stems from its ability to identify and leverage high-quality self-generated\n",
      "critiques. The major contributions of our work can be summarized as follows:\n",
      "• We propose Critic-RM, a framework to allow LLMs to take advantage of self-generated critiques for\n",
      "reward modeling. Critic-RM does not rely on additional supervision compared to standard reward\n",
      "models, while enjoying an improved generation quality as well as reward modeling accuracy.\n",
      "• We propose a self-refinement technique to automatically select high-quality critiques, and design a simple\n",
      "yet effective weight scheduling strategy to balance the learning objectives between critique generation\n",
      "and reward modeling. These techniques collaboratively equip the model with the dual capabilities of\n",
      "high-quality critique generation and accurate reward prediction.\n",
      "• We conduct experiments on three benchmarks covering over ten tasks, demonstrating the effectiveness\n",
      "of Critic-RM in precise reward modeling across diverse scenarios. Additional studies confirm the utility\n",
      "of Critic-RM-generated critiques in identifying and correcting mistakes made by LLMs.\n",
      "2\n",
      "Related Work\n",
      "Reward Models. Building an accurate and robust reward model is a critical step for RLHF pipelines. Earlier\n",
      "work trains reward models with the ranking loss between chosen and rejected responses with the Bradley-Terry\n",
      "model (Bradley and Terry, 1952; Stiennon et al., 2020; Ouyang et al., 2022; Dubey et al., 2024). To further\n",
      "improve upon this reward modeling pipeline, Wang et al. (2024e,d,a) design fine-grained attributes to predict\n",
      "1This discrete score is only used for filtering critiques and being different from the final reward score of Critic-RM. Our\n",
      "Critic-RM eventually produces a continuous score, as explained in Section 3.3.\n",
      "2\n",
      "\n",
      "Table 1 Comparison of our proposed method Critic-RM and closest baselines.\n",
      "Baselines\n",
      "Input Format\n",
      "Output Format\n",
      "Critique\n",
      "Generation\n",
      "Require\n",
      "Training\n",
      "Additional\n",
      "Teacher Models\n",
      "Standard RM (Bradley and Terry)\n",
      "Single Response\n",
      "Continuous Score\n",
      "✗\n",
      "✓\n",
      "✗\n",
      "RLAIF (Lee et al.)\n",
      "Single Response\n",
      "Continuous Score\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "LLM-as-a-judge (Zheng et al.)\n",
      "Response Pairs\n",
      "Discrete Score\n",
      "✓\n",
      "✗\n",
      "✗\n",
      "SynRM (Ye et al.)\n",
      "Single Response + Critique\n",
      "Continuous Score\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "CLoud (Ankner et al.)\n",
      "Single Response\n",
      "Critique + Continuous Score\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GenRM (Zhang et al.)\n",
      "Single Response\n",
      "Critique + Reward Token\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Critic-RM (Ours)\n",
      "Single Response\n",
      "Critique + Continuous Score\n",
      "✓\n",
      "✓\n",
      "✗\n",
      "rewards toward different aspects, Chen et al. (2024); Shen et al. (2024b); Liu et al. (2024); Coste et al. (2024)\n",
      "promote the robustness of reward modeling via improved training techniques or model ensembling, and Pace\n",
      "et al. (2024); Shen et al. (2024a) study how to create synthetic examples for reward models. More related\n",
      "to us, several very recent works (concurrent to us) also study generative reward modeling. Ye et al. (2024)\n",
      "directly augment the response with additional critiques from a teacher model for reward modeling without\n",
      "training the RM for critique generation, and Zhang et al. (2024); Ankner et al. (2024); Mahan et al. (2024)\n",
      "attempt to learn reward models with additional critique objectives, with similar focus of our study. However,\n",
      "these methods typically rely on high-quality critiques from stronger teacher models for training, which can be\n",
      "costly and inefficient to obtain in practice. They also don’t provide a solution to reward modeling based on\n",
      "frontier LLMs where a teacher model doesn’t exist. They also lack a unified approach to improve the quality\n",
      "of the critiques. Besides, Zhang et al. (2024) is specific to verifying math problem correctness and is hard to\n",
      "map to subjective domains where there are no ground-truth answers.\n",
      "LLM-as-a-judge and Critique Models. Recently, large language models (LLMs) have been proposed as cost-\n",
      "effective alternatives to human evaluation, and act as proxies for assessing text quality. Such methods often\n",
      "first provide explanations for judgments of the response, then output a discrete score or preference label as\n",
      "the prediction (Zheng et al., 2023; Li et al., 2023; Yan et al., 2024; Xu et al., 2024). CriticGPT (McAleese\n",
      "et al., 2024) has also extended this line of work into coding tasks, where the LLM critic model is fine-tuned to\n",
      "pinpoint problems in code from real-world assistant tasks. However, using off-the-shelf LLMs for evaluation\n",
      "introduces the risk of bias (Bavaresco et al., 2024; Stureborg et al., 2024), and they can be easily misled (Zeng\n",
      "et al., 2024). To address these challenges, recent studies (Wang et al., 2024b; Kim et al., 2024a) have focused\n",
      "on collecting high-quality response pairs to train more accurate and reliable LLM-based evaluators.\n",
      "Self-alignment Techniques. Aligning LLMs with human preferences often requires a massive amount of human\n",
      "annotations. To alleviate this reliance on human efforts, self-alignment leverages the model’s own capabilities\n",
      "to refine its responses and align them with desired behaviors. Saunders et al. (2022); Madaan et al. (2023)\n",
      "use LLM itself to refine the original response at the inference time. Li et al. (2024b) generate instruction\n",
      "prompts for web documents and subsequently select high-quality examples for instruction fine-tuning. Lee\n",
      "et al. (2024); Sun et al. (2024) leverage LLMs to create preference labels efficiently, Yuan et al. (2024) employ\n",
      "LLM itself to rank different responses to provide its own rewards during training, and Zelikman et al. (2022);\n",
      "Pang et al. (2024); Gulcehre et al. (2023) improve LLM reasoning abilities through self-generated reasoning\n",
      "steps. A recent study (Wang et al., 2024b) also employs self-improving techniques to train text evaluators,\n",
      "but it focuses on pairwise evaluation and generating synthetic preference pairs. In contrast, we combine\n",
      "self-generated critiques with human-annotated preference pairs to enhance reward modeling performance.\n",
      "3\n",
      "Methodology\n",
      "3.1\n",
      "Preliminaries\n",
      "Reward Modeling. Let X and Y denote the space of prompts and responses, respectively. In the RLHF pipeline,\n",
      "human feedback is typically collected in the form of pairwise preferences between two responses (y+, y−) ∈Y2\n",
      "to a given prompt x ∈X. Then, the preference dataset can be written as D =\n",
      "\n",
      "(xi, y+\n",
      "i , y−\n",
      "i )\n",
      "\t|D|\n",
      "i=1, where the\n",
      "preference for y+ over y−is denoted as y+ ≻y−. To model the pairwise preferences, the learning objective is\n",
      "3\n",
      "\n",
      "∑!\n",
      "\" \"!\n",
      "#\n",
      "#\n",
      ">\n",
      "∑!\n",
      "\" \"!\n",
      "$\n",
      "#\n",
      "Instance-level \n",
      "Critique Filtering\n",
      "Quality-aware \n",
      "Critique Refinement\n",
      "\"\"$%% = {%&, … , %'}\n",
      "\"()*' = {%&, … , %'}\n",
      "Summarize ) critiques\n",
      "Rank the top ) critiques\n",
      "Llama\n",
      "70B\n",
      "*%&\n",
      "+\n",
      "⋮\n",
      ",\n",
      "%#\n",
      "+\n",
      "*%&\n",
      ",\n",
      "⋮\n",
      ",\n",
      "%#\n",
      ",\n",
      "Critiques\n",
      "-&\n",
      "+\n",
      "⋮\n",
      "-#\n",
      "+\n",
      "-&\n",
      ",\n",
      "⋮\n",
      "-#\n",
      ",\n",
      "Scores\n",
      ". = / 0 ℓ- + (4 −/) 0 ℓ.\n",
      "Fine-tuning\n",
      "Prompt 7\n",
      "Chosen 8+\n",
      "Rejected 8,\n",
      "Refine\n",
      "Critique \n",
      "Generation\n",
      "Figure 1 An overview of Critic-RM. For each preference pair in the training set, we begin by prompting the LLM to\n",
      "generate candidate critiques along with discrete scores. Next, instance-level critique filtering is applied to minimize the\n",
      "impact of examples that conflict with preference labels. Finally, quality-aware critique refinement is performed to\n",
      "produce critiques that enhance reward model training.\n",
      "to maximize the probability with Bradley-Terry model (Bradley and Terry, 1952) as\n",
      "p\n",
      "\u0000y+ ≻y−| x\n",
      "\u0001\n",
      "=\n",
      "exp (r (x, y+))\n",
      "exp (r (x, y+)) + exp (r (x, y−)).\n",
      "(1)\n",
      "In practice, the reward model rψ is trained to minimize the following empirical negative log-likelihood\n",
      "loss (Stiennon et al., 2020; Ouyang et al., 2022; Dubey et al., 2024):\n",
      "ℓrm(ψ) = −E(x,y+,y−)∼D log\n",
      "\u0000σ\n",
      "\u0000rψ\n",
      "\u0000x, y+\u0001\n",
      "−rψ\n",
      "\u0000x, y−\u0001\u0001\n",
      "(2)\n",
      "where σ denotes the sigmoid function.\n",
      "Problem Setup. In this work, we investigate the usage of off-the-shelf instruction-finetuned LLM Mθ as\n",
      "the backbone for both the critique generation model and reward model. Specifically, we denote the critic\n",
      "generation model as gϕ = hg ◦Mθ and the reward model as rψ = hr ◦Mθ, where hg and hr stand for the\n",
      "language modeling head (inherited from the original Mθ) and reward modeling head (randomly initialized).\n",
      "Overview of Critic-RM. The framework of Critic-RM is shown in Figure 1. Critic-RM first generates candidate\n",
      "critiques for each prompt-response pair. Then, a filtering step is conducted to reduce the effect of potentially\n",
      "noisy rationales leading to incorrect predictions, allowing us to augment the preference pairs with additional\n",
      "critiques with the goal of improving the precision of reward modeling. Finally, we implement a joint training\n",
      "scheme to teach the model both high-quality critique generation and accurate reward modeling. The following\n",
      "sections will provide more details about each step.\n",
      "3.2\n",
      "Critique-augmented Reward Model Training\n",
      "To integrate the critiques into the reward modeling step, we view critiques as latent variables, which serve as\n",
      "an intermediate variable between the response and the final reward. Specifically, we denote z+, z−as critiques\n",
      "for chosen and rejected responses y+, y−with prompt x, respectively. Then, the overall learning objective\n",
      "4\n",
      "\n",
      "p (y+ ≻y−| x) can be recast as\n",
      "p(y+ ≻y−| x) =\n",
      "X\n",
      "z+,z−\n",
      "p(y+ ≻y−, z+, z−| x)\n",
      "=\n",
      "X\n",
      "z+,z−\n",
      "p(y+ ≻y−| z+, z−, x) · p∗(z+ | y+, x) · p∗(z−| y−, x).\n",
      "(3)\n",
      "Since p∗(· | y, x) stands for the oracle distribution for critiques and is often not intractable, we aim to leverage\n",
      "the critic generation model gϕ to generate the approximate distribution qϕ by applying the Jensen’s Inequality\n",
      "as\n",
      "log p\n",
      "\u0000y+ ≻y−| x\n",
      "\u0001\n",
      "= log Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0014\n",
      "p (y+ ≻y−, z+, z−| x)\n",
      "qϕ (z+ | y+, x) qϕ (z−| y−, x)\n",
      "\u0015\n",
      "≥Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0014\n",
      "log\n",
      "p (y+ ≻y−, z+, z−| x)\n",
      "qϕ (z+ | y+, x) qϕ (z−| y−, x)\n",
      "\u0015\n",
      "(4)\n",
      "Then, instead of directly optimizing the negative log-likelihood, the training objective can be expressed as\n",
      "L = Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0014\n",
      "−log\n",
      "p (y+ ≻y−, z+, z−| x)\n",
      "qϕ (z+ | y+, x) qϕ (z−| y−, x)\n",
      "\u0015\n",
      "= Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0002\n",
      "−log p\n",
      "\u0000y+ ≻y−| z+, z−, x\n",
      "\u0001\u0003\n",
      "|\n",
      "{z\n",
      "}\n",
      "Preference Modeling Loss with Critiques\n",
      "+ DKL\n",
      "\u0000(qϕ(z+ | y+, x)∥p∗(z+ | y+, x)\n",
      "\u0001\n",
      "+ DKL\n",
      "\u0000(qϕ(z−| y−, x)∥p∗(z−| y−, x)\n",
      "\u0001\n",
      "|\n",
      "{z\n",
      "}\n",
      "Critique Generation Loss\n",
      ".\n",
      "(5)\n",
      "What does the learning objective imply? Eq. 5 provides a way to decompose the reward model learning objective\n",
      "into two parts: (1) Preference Modeling Loss with Critiques ℓr: the reward model rθ learns to predict the\n",
      "reward for each response conditioned on critiques; (2) Critique Generation Loss ℓc: the LLM generation gθ is\n",
      "trained to generate critiques to approximate the oracle distribution p∗(· | y, x). We will discuss how to train\n",
      "the reward model rθ and critique generation model gθ in the following subsections.\n",
      "3.2.1\n",
      "Critique-augmented Reward Prediction\n",
      "To enable the reward model rψ to learn the preference with critiques (i.e., ℓr) can be straightforward, as we\n",
      "only need to modify the input by augmenting response with critiques as\n",
      "ℓr(x, y+, y−, z+, z−) = −log p\n",
      "\u0000y+ ≻y−, z+, z−| x\n",
      "\u0001\n",
      "= −log p\n",
      "\u0000rψ(x, [y+; z+]) > rψ(x, [y−; z−])\n",
      "\u0001\n",
      ".\n",
      "(6)\n",
      "In this way, for each prompt, the reward model will learn to generate the reward based on both responses\n",
      "and critiques. In practice, we put the critiques after the response and add a special token at the end of the\n",
      "critique for calculating the reward.\n",
      "3.2.2\n",
      "Critique Generation & Filtering\n",
      "For critique generation loss, approximating p∗(· | y, x) can be nontrivial as the primary challenge lies in the\n",
      "lack of high-quality critique annotations. To ensure the quality of the critiques, our key hypothesis is that\n",
      "good critiques for responses should align well with human preference labels. With this in mind, we design a\n",
      "generate-then-filter framework to create high-quality supervision signals for critique model training.\n",
      "Critique Generation. To generate critiques without relying on stronger LLMs, we first prompt the LLM Mθ\n",
      "(with the same backbone as the reward model) and sample a set of N candidate critiques for input prompt\n",
      "and responses (x, y) by following the procedure of the LLM-as-a-judge pipeline as (bzi, si)N\n",
      "i=1 ∼gϕ(x, y), where\n",
      "bz is the generated critique and s is a discrete score ranging from 1 to 10, indicating the quality of the response.\n",
      "Instance-level Critique Filtering. To reduce the potential noisy critiques and encourage the consistency between\n",
      "critiques and preference labels, we propose to first retain instances guided by the score generated by the\n",
      "judge in the previous score as Dsub = {(x, y+, y−) | ¯s(x, y+) > ¯s(x, y−)}, where ¯s(x, y+) = PN\n",
      "i=1 s+\n",
      "i /N and\n",
      "5\n",
      "\n",
      "¯s(x, y−) = PN\n",
      "i=1 s−\n",
      "i /N stand for the average score for chosen and rejected responses, respectively. By applying\n",
      "this filtering process, we enhance the consistency of critiques with human preferences and minimize the impact\n",
      "of noisy instances.\n",
      "Quality-aware Critique Refinement. The previous step mainly focuses on instance-level denoising, while for\n",
      "each (prompt, response) pair, the quality of different critiques also varies. To further improve the quality of\n",
      "critiques, we design a Meta-judge-based technique (Wu et al., 2024) to leverage LLM Mθ again to further\n",
      "refine the critiques in Dsub, with two possible variants:\n",
      "• Summarization-based Refinement: We adopt the LLM as a summarizer to write ‘meta-critiques’ given\n",
      "different critiques so that the LLM can potentially identify the most common, reasonable feedback\n",
      "while mitigating the impact of the potential incorrect feedback. The final critique can be written as\n",
      "Zsumm = {zi}K\n",
      "i=1 ∼gϕ(x, y, ΠN\n",
      "j=1bzj), where ΠN\n",
      "j=1bzj is a permutation of N initial critiques. By sampling\n",
      "over different permutations of critiques, we can generate more diverse critiques for model training.\n",
      "• Ranking-based Refinement: We use the LLM as a meta-judge to create evaluation scores for critiques.\n",
      "Specifically, for each critique bzi, we prompt the LLM to generate a discrete score from 1 to 10 as\n",
      "mi ∼gϕ(x, y, bzi), which serves as a proxy for critique quality estimation. Then, we only retain top-K\n",
      "ranked critiques as Zrank = {zi}K\n",
      "i=1 = Top-K({bzi}N\n",
      "i=1). In this way, we can preserve the critiques with\n",
      "the highest quality identified by the model itself.\n",
      "Final Loss for Critique Generation. From the previous step, we augment the training set Dsub with self-identified\n",
      "high-quality critiques, denoted as Dsub = {(x, y+, y−, Z+, Z−)}. With the self-generated high-quality critiques\n",
      "Z, we aim to use them to approximate the distribution of oracle distribution as p∗(z | y, x) = I(z ∈Z).\n",
      "Directly using this distribution in backward KL loss in Eq. 5 may lead to policy and entropy collapses (Sessa\n",
      "et al., 2024; Agarwal et al., 2024). As a result, we use forward KL loss to approximate this learning objective.\n",
      "Then using the empirical distribution, the KL divergence becomes:\n",
      "ℓc(Z; x, y) = DKL(p∗(z | yi, xi)∥qϕ(z | yi, xi))\n",
      "= Ez∼p∗(z|yi,xi) [log p∗(z | yi, xi) −log qϕ (z | yi, xi)]\n",
      "= −1\n",
      "K\n",
      "X\n",
      "z∈Z\n",
      "log qϕ(z | y, x) + const.\n",
      "(7)\n",
      "Then, the overall loss for critique generation can be written as ℓc(x, y+, y−, Z+, Z−) = ℓc(Z+; x, y+) +\n",
      "ℓc(Z−; x, y−).\n",
      "3.2.3\n",
      "Joint Learning of Critique Generation and Reward Modeling\n",
      "To combine the reward modeling loss (Eq. 6) and critique generation loss together (Eq. 7), one challenge lies\n",
      "in the different learning objectives for these two terms: for critique generation, the model gϕ will benefit more\n",
      "from fine-tuning with diverse critiques from Z. On the contrary, the reward model rψ is often observed with\n",
      "overfitting issues when fine-tuning with more than one round. To resolve this issue, we design a dynamic\n",
      "weight schedule approach, where we add an additional weight λ(t) on Equation 5, which is relevant to the\n",
      "training step t, for balancing between these two objectives as\n",
      "L(ϕ, ψ) = E(x,y+,y−,Z+,Z−)∈Dsub [λ(t) · ℓc(ϕ) + (1 −λ(t)) · ℓr(ψ)] ,\n",
      "(8)\n",
      "where λ(t) is defined as\n",
      "λ(t) =\n",
      "(\n",
      "1,\n",
      "0 < t < (K −1)T\n",
      "1 −β × t−(K−1)T\n",
      "T\n",
      ".\n",
      "(K −1)T < t < KT\n",
      "(9)\n",
      "Here, T represents the total number of training steps in one epoch. This approach allows the model to focus\n",
      "on critique generation during the initial phase of training and shifts to reward learning in the final round,\n",
      "mitigating the overfitting issue in the reward model.\n",
      "6\n",
      "\n",
      "3.3\n",
      "Critic-RM Inference\n",
      "Compared to standard reward model training, Critic-RM involves an additional step for each (prompt,\n",
      "response) pair during inference. Specifically, given the (prompt, response) pair (x, y), the model will first\n",
      "generate a critique z ∼qϕ(x, y), then predict the reward for the response as r = rψ(x, [y, z]).\n",
      "Inference-time Scaling. Following recent studies (Ankner et al., 2024; Zhang et al., 2024), we also conduct\n",
      "inference-time scaling (Wang et al., 2023) to improve performance. Specifically, we generate a set of m\n",
      "critiques as Z = {zi}m\n",
      "i=1 ∼qϕ(x, y) with non-zero temperatures, then predict the reward for the response as\n",
      "the average of reward over different critiques as r = rψ(x, [y, zi])/m.\n",
      "4\n",
      "Experiments\n",
      "4.1\n",
      "Experiment Setup\n",
      "4.1.1\n",
      "Training Data\n",
      "To ensure the representativeness of the preference pairs used in this study, we leverage both public and\n",
      "synthetic datasets for reward model training.\n",
      "Public Preference Datasets: We choose a set of datasets for reward model training with human-generated\n",
      "preference labels mainly from public, open-sourced datasets (Ivison et al., 2024; Wang et al., 2024a). We\n",
      "include the following datasets:\n",
      "• General Chat Domain: We include datasets from ChatArena (Zheng et al., 2023) and AlpacaFarm-Human-\n",
      "Pref (Dubois et al., 2023).\n",
      "• Helpfulness Data: We leverage HelpSteer2 (Wang et al., 2024d) to create preference data.\n",
      "• Reasoning: We mainly use Evol-instruct (Xu et al., 2023) which contains preference pairs for complex\n",
      "instruction following, coding-related tasks.\n",
      "• Safety: We employ PKU-SafeRLHF (Dai et al., 2024), which includes safety-related prompts paired\n",
      "with both safe and unsafe responses to form preference pairs.\n",
      "Synthetic Preference Datasets: To incorporate additional preference supervision from different domains, we\n",
      "further include synthetic data using Llama-3.1 models.2 Specifically, for the math domain, we consider\n",
      "questions in GSM8K (Cobbe et al., 2021) and the MATH dataset (Hendrycks et al., 2021). For each math\n",
      "question, we use Llama-3.1-8b-instruct, and Llama-3.1-70b-instruct to generate candidate solutions with\n",
      "the prompt “Given the following problem, reason step-by-step and give a final answer to the problem.”, and\n",
      "generate multiple candidate solutions for a given prompt. We use those responses that lead to correct solutions\n",
      "as the chosen response while considering those responses with incorrect solutions as the rejected response. In\n",
      "the safety domain, we generate synthetic prompts following the safety principles outlined in SafeRLHF (Dai\n",
      "et al., 2024) (e.g., Hate Speech, Offensive Language, Discrimination, Violence). To ensure balance, we also\n",
      "include scenarios where the model should not refuse to respond (e.g., Figurative Language, Safe Targets\n",
      "testing for ambiguous meanings) to avoid skewing the data toward over-conservatism.\n",
      "4.1.2\n",
      "Evaluation Benchmarks.\n",
      "Evaluation Benchmarks for Reward Models. In our experiments, we mainly evaluate on RewardBench (Lambert\n",
      "et al., 2024), which contains a collection of prompt-chosen-rejected triplets across chat, reasoning, and safety\n",
      "domains, including 2985 examples in total. We use the standard evaluation protocol provided by the original\n",
      "authors. Beyond RewardBench, we also aim to test the out-of-distribution generalization ability of reward\n",
      "models. Specifically, we consider CrossEval (Zhong et al., 2024), a recently proposed benchmark to evaluate\n",
      "the LLM’s capability in real-world interactions3. Besides, we also consider two additional datasets, namely\n",
      "QA Feedback (Wu et al., 2023) and SHP (Ethayarajh et al., 2022), which focuses on evaluating the response\n",
      "for open-ended QA task as well as social platforms (i.e., Reddit). There are around 2000 examples of QA\n",
      "2These synthetic data are used for both Critic-RM and our direct baselines.\n",
      "3The details for data processing is listed in Appendix A.\n",
      "7\n",
      "\n",
      "Feedback preference pairs. For SHP, we use the response with a higher average score/votes judged by human\n",
      "raters as the positive response, and we use the response with a lower score/votes as the negative one, and\n",
      "randomly subsample 3000 pairs for evaluation. For all tasks, we use accuracy as the main metric.\n",
      "Evaluation Benchmarks for Critic Models. To demonstrate the effectiveness of our model in generating improved\n",
      "critiques, we employ CriticBench (Lin et al., 2024), a benchmark designed to evaluate LLMs’ ability to\n",
      "critique and improve their reasoning across various tasks. CriticBench covers five key reasoning domains:\n",
      "mathematical, commonsense, symbolic, coding, and algorithmic. It includes responses from 17 different LLMs,\n",
      "requiring the LLMs to provide critiques that assess the correctness of these LLMs’ responses. Specifically,\n",
      "it considers two dimensions for evaluation: (1) Critique Accuracy: where F1 Score is used to evaluate the\n",
      "correctness of critiques; (2) Correction Accuracy: where Accuracy is used to evaluate whether the model can\n",
      "generate correct answers based on critique feedback.\n",
      "4.1.3\n",
      "Baselines\n",
      "We consider the following baselines from three different groups:\n",
      "• LLM-as-a-judge: With the prompt with a pair of responses used as the input, this line of models needs to\n",
      "generate a preference label. We consider Prometheus-v2 (Kim et al., 2024b), Llama-3.1-70B/405B (Dubey\n",
      "et al., 2024), GPT-4 (Achiam et al., 2023) and GPT-4o (Hurst et al., 2024), Gemini-1.5-pro (Reid\n",
      "et al., 2024) and recently proposed self-taught evaluator (Wang et al., 2024b) based on Llama-3-70B for\n",
      "comparison.\n",
      "• Standard Reward Models: This line of models only outputs a scalar score for each (prompt, response)\n",
      "pair. We compare with baselines including standard RM (Stiennon et al., 2020), Cohere-0514, SteerLM-\n",
      "RM (Wang et al., 2024e), Nemotron-RM (Adler et al., 2024).\n",
      "• Reward Model with Critiques: These studies are mostly relevant to us as they also leverage critiques to\n",
      "improve reward models. Specifically, we compare with SymRM (Ye et al., 2024) which directly augments\n",
      "responses with critiques for reward modeling, and CLoud (Ankner et al., 2024) which jointly learn to\n",
      "generate critiques and predict rewards.\n",
      "It is worth noting that for most relevant baselines (e.g. RM, SynRM, CLoud), we reimplement those baselines\n",
      "with the same training data and backbone to ensure the comparison is fair and meaningful. We do not\n",
      "consider some reward model training techniques (Wang et al., 2024c,a) as they focus on designing better\n",
      "learning objectives for standard reward models, which are orthogonal to the focus of this study.\n",
      "4.1.4\n",
      "Implemenation Details\n",
      "We use Llama3.1-70B-Instruct (Dubey et al., 2024) as the backbone in our main experiments. For critique\n",
      "generation, we set the temperature τ = 0.9 and sample N = 10 candidate critiques for each response. For\n",
      "the critique filtering, we set K = 2 to select top-2 responses. For model fine-tuning, we use the Adam\n",
      "optimizer (Kingma and Ba, 2014) with the learning rate 2e-6, weight decay 0.1 and dropout 0.1. We set the\n",
      "global batch size to 64, β in Eq. 9 to 0.9 and train the model with 2 epochs. We observe that there exist\n",
      "several examples in AlpacaEval and ChatArena that share similar prompts with the target evaluation tasks,\n",
      "and we remove all overlapping prompts to avoid data contamination (Oren et al., 2024). During inference, if\n",
      "inference-time scaling is adopted, we choose temperate τ = 0.95 to sample multiple critiques. The prompt\n",
      "format we use in experiments is exhibited in Appendix B.\n",
      "4.2\n",
      "Main Experiments: RewardBench\n",
      "Table 2 presents results of Critic-RM and baselines. The findings are summarized as follows:\n",
      "• Incorporating Critiques Helps Reward Modeling in General. Critic-RM generally outperforms the baselines\n",
      "used in this study. Specifically, when trained with the same preference data, Critic-RM outperforms the\n",
      "standard Reward Model by 3.7%-4.7%. Critic-RM also outperform giant Llama-3.1-405b judge model by\n",
      "6.2%-7.3%, respectively. These results justify the advantage of incorporating critiques into the reward\n",
      "model training step, which facilitates both high-quality critiques and precise rewards.\n",
      "8\n",
      "\n",
      "Table 2 Results of our proposed method and baselines on the RewardBench. †: Results copied from either RewardBench\n",
      "Leaderboard or original papers. §: This version of the model is trained using SFT only.\n",
      "Models\n",
      "Chat\n",
      "Chat_Hard\n",
      "Reasoning\n",
      "Safety\n",
      "Overall\n",
      "LLM-as-a-judge (For Reference)\n",
      "Prometheus-8*7b-v2† (Kim et al., 2024b)\n",
      "93.0\n",
      "47.1\n",
      "77.4\n",
      "80.5\n",
      "74.5\n",
      "Llama3.1-70B-Instruct† (Dubey et al., 2024)\n",
      "97.2\n",
      "70.2\n",
      "82.8\n",
      "86.0\n",
      "84.0\n",
      "Llama3.1-405B-Instruct† (Dubey et al., 2024)\n",
      "97.2\n",
      "74.6\n",
      "77.6\n",
      "87.1\n",
      "84.1\n",
      "GPT-4-0125† (Achiam et al., 2023)\n",
      "95.3\n",
      "74.3\n",
      "87.6\n",
      "86.9\n",
      "86.0\n",
      "GPT-4o-0806† (Hurst et al., 2024)\n",
      "96.1\n",
      "76.1\n",
      "88.1\n",
      "86.6\n",
      "86.7\n",
      "Gemini-1.5-pro-0514† (Reid et al., 2024)\n",
      "92.3\n",
      "80.6\n",
      "92.0\n",
      "87.9\n",
      "88.2\n",
      "Self-taught Evaluator§ (Wang et al., 2024b) (Iter 1)\n",
      "98.3\n",
      "69.0\n",
      "82.6\n",
      "85.7\n",
      "83.9\n",
      "Self-taught Evaluator§ (Wang et al., 2024b) (Iter 2)\n",
      "97.5\n",
      "75.4\n",
      "81.7\n",
      "89.5\n",
      "86.0\n",
      "Self-taught Evaluator§ (Wang et al., 2024b)\n",
      "96.6\n",
      "84.2\n",
      "91.5\n",
      "81.0\n",
      "88.3\n",
      "w/ inference scaling, m = 32\n",
      "96.9\n",
      "84.0\n",
      "91.5\n",
      "82.5\n",
      "88.7\n",
      "Standard Reward Models\n",
      "RM (Stiennon et al., 2020)\n",
      "98.3\n",
      "74.5\n",
      "88.0\n",
      "83.8\n",
      "86.4\n",
      "Cohere-0514†\n",
      "96.4\n",
      "71.3\n",
      "92.3\n",
      "97.7\n",
      "89.4\n",
      "SteerLM-RM 70B† (Wang et al., 2024e)\n",
      "91.3\n",
      "80.3\n",
      "92.8\n",
      "90.6\n",
      "88.8\n",
      "Nemotron-RM 340B† (Adler et al., 2024)\n",
      "95.8\n",
      "87.1\n",
      "91.5\n",
      "93.6\n",
      "92.0\n",
      "(Concurrent Work) Reward Models with Critiques\n",
      "SynRM† (Ye et al., 2024) (Reported Best)\n",
      "38.0\n",
      "82.5\n",
      "87.1\n",
      "74.1\n",
      "70.4\n",
      "SynRM (Ye et al., 2024) (Ours)\n",
      "97.5\n",
      "76.8\n",
      "88.5\n",
      "86.3\n",
      "87.3\n",
      "CLoud† (Ankner et al., 2024) (Reported)\n",
      "∼97.0\n",
      "∼58.0\n",
      "∼92.0\n",
      "∼84.0\n",
      "82.8\n",
      "CLoud (Ankner et al., 2024) (Ours)\n",
      "98.0\n",
      "75.6\n",
      "87.6\n",
      "89.0\n",
      "87.6\n",
      "w/ inference scaling, m = 32\n",
      "98.0\n",
      "75.2\n",
      "89.3\n",
      "91.5\n",
      "88.5\n",
      "Critic-RM-Summ\n",
      "98.0\n",
      "77.0\n",
      "88.9\n",
      "94.5\n",
      "89.6\n",
      "w/ inference scaling, m = 32\n",
      "97.5\n",
      "77.0\n",
      "91.6\n",
      "95.9\n",
      "90.5\n",
      "Critic-RM-Rank\n",
      "97.5\n",
      "79.6\n",
      "90.6\n",
      "94.1\n",
      "90.5\n",
      "w/ inference scaling, m = 32\n",
      "97.2\n",
      "80.0\n",
      "91.6\n",
      "95.1\n",
      "91.0\n",
      "• High-quality Critiques Matters. By comparing Critic-RM with baselines that also incorporate critiques\n",
      "into reward modeling, we observe that their performance gains over the standard RM are smaller than\n",
      "ours. We attribute this performance gap to the lack of post-processing methods for improving critique\n",
      "quality, which is key to achieving self-improvement in this challenging setting.\n",
      "• Inference-time Scaling Mainly Helps for Reasoning Tasks. We observe further performance improvements for\n",
      "both Critic-RM and the baselines when multiple critiques are generated during inference. Notably, these\n",
      "gains are most pronounced in reasoning-intensive tasks such as Math, Coding, and Safety, where the\n",
      "model must decide whether to reject a response. This suggests that, when computational resources are\n",
      "constrained, prioritizing reasoning-heavy tasks can lead to more significant performance improvements.\n",
      "4.3\n",
      "Out-of-Distribution (OOD) Evaluation\n",
      "As shown in Table 3, we evaluate the performance of our approach (Critic-RM) alongside relevant baseline\n",
      "models on three out-of-distribution (OOD) reward modeling datasets. Our results demonstrate that Critic-RM\n",
      "exhibits a strong performance across these datasets, surpassing standard reward modeling (RM) baselines by\n",
      "an average margin of 4%. Notably, the performance improvements of Critic-RM are more pronounced on\n",
      "more challenging benchmarks, such as tasks requiring cross-abilities, suggesting that the benefits of critiques\n",
      "are more significant in complex scenarios. Furthermore, we observe that the performance of Critic-RM is\n",
      "comparable to that of LLM-judge models with significantly more parameters. This highlights the efficiency\n",
      "and effectiveness of Critic-RM when being adapted to real scenarios.\n",
      "9\n",
      "\n",
      "Table 3 Results of our proposed method and baselines on out-of-distribution reward modeling datasets.\n",
      "Models\n",
      "CrossEval\n",
      "Other Datasets\n",
      "English\n",
      "Reasoning\n",
      "Coding\n",
      "Tool\n",
      "C+R\n",
      "T+R\n",
      "T+C\n",
      "Avg.\n",
      "QA Feedback\n",
      "SHP\n",
      "LLM-as-a-judge (For Reference)\n",
      "Llama3.1-70B-Instruct (Dubey et al., 2024)\n",
      "55.4\n",
      "71.4\n",
      "70.1\n",
      "77.4\n",
      "78.2\n",
      "69.5\n",
      "80.7\n",
      "71.8\n",
      "59.2\n",
      "63.3\n",
      "Llama3.1-405B-Instruct (Dubey et al., 2024)\n",
      "64.4\n",
      "71.9\n",
      "77.5\n",
      "80.2\n",
      "78.2\n",
      "75.6\n",
      "78.9\n",
      "75.2\n",
      "60.7\n",
      "62.9\n",
      "Reward Models\n",
      "RM (Stiennon et al., 2020)\n",
      "59.3\n",
      "72.7\n",
      "70.8\n",
      "75.2\n",
      "68.3\n",
      "72.0\n",
      "72.4\n",
      "70.1\n",
      "58.3\n",
      "65.1\n",
      "CLoud (Ankner et al., 2024)\n",
      "60.3\n",
      "75.2\n",
      "71.7\n",
      "79.0\n",
      "73.2\n",
      "71.1\n",
      "73.4\n",
      "72.0\n",
      "59.2\n",
      "64.8\n",
      "Our Model\n",
      "Critic-RM-Summ\n",
      "61.3\n",
      "76.2\n",
      "72.4\n",
      "80.7\n",
      "73.2\n",
      "71.6\n",
      "76.9\n",
      "73.0\n",
      "60.4\n",
      "67.9\n",
      "Critic-RM-Rank\n",
      "64.0\n",
      "74.3\n",
      "73.3\n",
      "80.7\n",
      "79.3\n",
      "72.0\n",
      "79.3\n",
      "74.7\n",
      "60.2\n",
      "66.2\n",
      "Table 4 Results of our proposed method and baselines on CriticBench (Lin et al., 2024). ∗: For these methods, we use\n",
      "the same Llama-3.1-8b-Instruct as the backbone model for answer correction.\n",
      "Models\n",
      "Critique Accuracy (F1)\n",
      "Correction Accuracy (Acc.)\n",
      "Algorithm\n",
      "Code\n",
      "Symbolic\n",
      "Commonsense\n",
      "Math\n",
      "Total\n",
      "Algorithm\n",
      "Code\n",
      "Symbolic\n",
      "Commonsense\n",
      "Math\n",
      "Total\n",
      "Baselines\n",
      "Auto-J 13B (Li et al., 2024a)\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "65.29\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "UltraCM 13B (Cui et al., 2024)\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "61.11\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "CLoud∗(Ankner et al., 2024)\n",
      "57.22\n",
      "82.87\n",
      "80.56\n",
      "70.18\n",
      "90.35\n",
      "81.91\n",
      "84.75\n",
      "74.56\n",
      "95.35\n",
      "50.22\n",
      "68.48\n",
      "69.56\n",
      "GPT-3.5 (OpenAI, 2022)\n",
      "46.15\n",
      "73.13\n",
      "64.49\n",
      "50.22\n",
      "62.01\n",
      "61.11\n",
      "58.16\n",
      "61.85\n",
      "71.83\n",
      "44.11\n",
      "41.95\n",
      "51.24\n",
      "GPT-4 (Achiam et al., 2023)\n",
      "63.51\n",
      "91.36\n",
      "90.75\n",
      "71.56\n",
      "92.55\n",
      "78.75\n",
      "77.66\n",
      "76.29\n",
      "92.41\n",
      "59.96\n",
      "63.57\n",
      "69.96\n",
      "LLM-as-a-judge (For Reference)\n",
      "Llama3.1-70B-Instruct∗(Dubey et al., 2024)\n",
      "60.37\n",
      "84.92\n",
      "86.17\n",
      "65.52\n",
      "88.53\n",
      "80.75\n",
      "77.65\n",
      "76.93\n",
      "88.06\n",
      "59.29\n",
      "57.28\n",
      "66.96\n",
      "Llama3.1-405B-Instruct∗(Dubey et al., 2024)\n",
      "86.96\n",
      "88.96\n",
      "90.70\n",
      "72.59\n",
      "93.84\n",
      "86.96\n",
      "86.52\n",
      "81.42\n",
      "90.86\n",
      "63.76\n",
      "63.36\n",
      "72.02\n",
      "Our Model\n",
      "Critic-RM-Summ∗\n",
      "89.79\n",
      "89.36\n",
      "88.36\n",
      "75.26\n",
      "96.09\n",
      "88.25\n",
      "90.55\n",
      "81.89\n",
      "95.82\n",
      "56.95\n",
      "72.54\n",
      "74.33\n",
      "Critic-RM-Rank∗\n",
      "86.13\n",
      "88.88\n",
      "91.10\n",
      "75.02\n",
      "95.49\n",
      "87.93\n",
      "90.42\n",
      "78.44\n",
      "96.43\n",
      "57.39\n",
      "71.77\n",
      "73.87\n",
      "Critic-RM-rank\n",
      "Critic-RM-summ\n",
      "88\n",
      "90\n",
      "ACC\n",
      "Critic-RM\n",
      "w/ constant scheduling\n",
      "w/ reverse scheduling\n",
      "(a) Ablation on weight scheduling.\n",
      "Critic-RM-rank\n",
      "Critic-RM-summ\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "ACC\n",
      "1 round\n",
      "2 rounds\n",
      "3 rounds\n",
      "(b) Acc. w/ different rounds.\n",
      "Critic-RM-rank\n",
      "Critic-RM-summ\n",
      "86\n",
      "88\n",
      "90\n",
      "ACC\n",
      "Critic-RM\n",
      "w/o Instance Filtering\n",
      "w/o Critique Refinement\n",
      "(c) Ablation on filtering strategies.\n",
      "Figure 2 Ablation studies. The y-axis is the average accuracy on RewardBench.\n",
      "4.4\n",
      "Evaluation on Critiques\n",
      "As Critic-RM involves a crucial step for generating critiques, it is also important to evaluate the quality of\n",
      "critiques for target tasks. We use CriticBench to perform a comprehensive evaluation, with results detailed in\n",
      "Table 4. For critique accuracy, we observe that Critic-RM generates more accurate critiques compared to strong\n",
      "baselines, including GPT-4. This justify that Critic-RM is able to distinguish correct and flawed reasoning\n",
      "paths. Additionally, these critiques help the policy language model (LM) correct flawed reasoning steps,\n",
      "resulting in improved accuracy in refined responses. Notably, even when using the lightweight Llama-3-8B\n",
      "as the policy LM, the critiques guide the smaller LM to rectify initial incorrect reasoning and achieve high\n",
      "accuracy across five reasoning tasks.\n",
      "4.5\n",
      "Ablation Studies\n",
      "EffectofTwo-stageTraining. Figure 2a illustrates the performance of Critic-RM with different weight scheduling\n",
      "function λ(t). The results indicate that using a constant weight across different rounds, as well as reverse\n",
      "10\n",
      "\n",
      "Table 5 Performance of Critic-RM and most relevant baselines with different amounts of training data.\n",
      "Data Volume\n",
      "Method\n",
      "Chat\n",
      "Chat Hard\n",
      "Reasoning\n",
      "Safety\n",
      "Overall\n",
      "RM\n",
      "98.4\n",
      "70.8\n",
      "83.1\n",
      "73.6\n",
      "81.5\n",
      "SynRM\n",
      "96.9\n",
      "75.1\n",
      "84.2\n",
      "89.1\n",
      "86.3\n",
      "10%\n",
      "CLoud\n",
      "96.6\n",
      "74.7\n",
      "86.1\n",
      "86.3\n",
      "85.9\n",
      "Critic-RM-summ\n",
      "96.1\n",
      "77.0\n",
      "86.7\n",
      "91.0\n",
      "87.7\n",
      "Critic-RM-rank\n",
      "96.4\n",
      "77.9\n",
      "85.6\n",
      "90.2\n",
      "87.5\n",
      "RM\n",
      "98.1\n",
      "69.2\n",
      "85.6\n",
      "81.8\n",
      "83.6\n",
      "SynRM\n",
      "97.2\n",
      "75.7\n",
      "85.0\n",
      "89.8\n",
      "86.9\n",
      "30%\n",
      "CLoud\n",
      "97.4\n",
      "76.7\n",
      "86.1\n",
      "87.5\n",
      "86.9\n",
      "Critic-RM-summ\n",
      "96.9\n",
      "78.7\n",
      "87.4\n",
      "92.2\n",
      "88.8\n",
      "Critic-RM-rank\n",
      "97.8\n",
      "77.1\n",
      "86.5\n",
      "93.1\n",
      "88.6\n",
      "RM\n",
      "98.3\n",
      "75.6\n",
      "87.4\n",
      "82.2\n",
      "85.9\n",
      "SynRM\n",
      "97.4\n",
      "76.9\n",
      "85.1\n",
      "90.0\n",
      "87.3\n",
      "50%\n",
      "CLoud\n",
      "97.2\n",
      "76.5\n",
      "86.9\n",
      "89.3\n",
      "87.4\n",
      "Critic-RM-summ\n",
      "97.2\n",
      "78.7\n",
      "89.1\n",
      "93.1\n",
      "89.5\n",
      "Critic-RM-rank\n",
      "97.2\n",
      "79.2\n",
      "88.9\n",
      "94.0\n",
      "89.8\n",
      "weight scheduling (i.e., prioritizing reward modeling first, followed by critique generation), both negatively\n",
      "impact performance. Besides, Figure 2b shows the performance of Critic-RM with different K (training\n",
      "epoch), where reward modeling is applied only in the final epoch. The results indicate that performance\n",
      "improves when K = 2, but plateaus with further increases. Thus, K = 2 serves as a trade-off to balance\n",
      "between performance and training efficiency.\n",
      "Effect of Data Filtering. We further evaluate our data filtering strategy in Figure 2c and observe that using\n",
      "the entire dataset without filtering leads to poor performance, particularly in the Chat-hard domain, which\n",
      "requires stronger reasoning capabilities for LLMs to assess response preferences accurately. Additionally,\n",
      "removing noisy preference pairs improves standard reward modeling. Moreover, incorporating summarization\n",
      "and ranking proves to be an effective approach for boosting overall performance.\n",
      "4.6\n",
      "Data Efficiency of Reward Models\n",
      "Table 5 shows the accuracy of Critic-RM and baselines on RewardBench with different volumes of training\n",
      "data. Overall, Critic-RM consistently outperforms the baselines across all data volumes, demonstrating\n",
      "superior performance even with limited labels. Notably, Critic-RM shows strong data efficiency—using just\n",
      "10% of the labeled data is sufficient to surpass the standard reward model. This result highlights the data\n",
      "efficiency of Critic-RM, making it highly practical for real-world applications.\n",
      "4.7\n",
      "Case Studies\n",
      "Table 6 presents two examples from RewardBench that highlight the advantages of Critic-RM over baseline\n",
      "models. Standard RMs often struggle to assign higher scores to the correct response and fail to provide\n",
      "additional context to justify the predicted reward. While Cloud offers general feedback on response quality, it\n",
      "tends to focus on strengths and weaknesses without identifying the most critical aspects for evaluation. In\n",
      "contrast, Critic-RM accurately identifies key errors in rejected responses and explains why the chosen response\n",
      "is superior, leading to more accurate predictions. We list additional case studies in Appendix C.\n",
      "5\n",
      "Conclusion\n",
      "In this work, we introduced Critic-RM, a self-critiquing framework designed to enhance reward modeling for\n",
      "large language models. By harnessing LLMs’ ability to generate and refine critiques, Critic-RM implements\n",
      "a novel self-improvement approach that improves both critique quality and reward prediction accuracy.\n",
      "11\n",
      "\n",
      "Table 6 An example that comparing Standard reward model (RM), critique-out-loud reward models (CLoud) and\n",
      "Critic-RM on reward prediction. Underline text denotes imprecise generation of rejected responses, Green text stands\n",
      "the central part that leads to accurate predictions of Critic-RM. Critic-RM can better model the preference by\n",
      "generating accurate critiques that identify the major strengths and weaknessses of the responses.\n",
      "Prompt\n",
      "Describe a time when you had to make a difficult decision.\n",
      "Chosen\n",
      "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed\n",
      "by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a\n",
      "difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky\n",
      "decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project\n",
      "was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\n",
      "Rejected\n",
      "Certainly! One situation where I had to solve a complex problem was during my time working as a project manager for a software development team. We\n",
      "were tasked with developing a new mobile application for a client within a tight deadline. The complex problem arose when we encountered a critical bug\n",
      "in the application just a few days before the scheduled release. The bug caused the app to crash randomly, making it unusable for end-users. This issue\n",
      "had the potential to damage our reputation and relationship with the client. To solve this problem, I immediately gathered the development team to assess\n",
      "the situation. We conducted a thorough investigation to identify the root cause of the bug. After extensive debugging and testing, we discovered that the\n",
      "issue was related to a memory leak in the code. To address the problem, we divided the team into smaller groups, each focusing on a specific aspect of the\n",
      "application. We implemented a rigorous testing process to identify and fix any memory leaks throughout the codebase...\n",
      "RM\n",
      "Chosen Reward: 0.27\n",
      "Rejected Reward: 0.56\n",
      "CLoud\n",
      "Critique for Chosen Response: The assistant provides unnecessary details about the project’s context, such as the need to meet the client’s expectations, which,\n",
      "while relevant, could be conveyed more succinctly. Furthermore, the response focuses more on the outcome of the decision rather than the decision-making\n",
      "process itself. The user prompt asks for a description of a time when a difficult decision was made, implying that the user wants to understand the thought\n",
      "process and considerations that led to the decision. The assistant’s response glosses over these aspects, instead highlighting the success of the project and the\n",
      "client’s satisfaction. Additionally, the response lacks depth and introspection.\n",
      "Chosen Reward: 0.46\n",
      "Critique for Rejected Response: The response provides a clear and specific example from the assistant’s experience, making it more relatable and engaging. The\n",
      "situation is well-explained, including the problem, the steps taken to address it, and the outcome. The assistant highlights their role in coordinating the team’s\n",
      "efforts and communicating with the client, demonstrating leadership skills. However, The response could be more concise, as some details, such as the extensive\n",
      "debugging and testing process, could be summarized more briefly. While the assistant mentions the importance of thorough testing and quality assurance\n",
      "processes, it would be beneficial to elaborate on how this was implemented in the specific situation or how it was improved upon.\n",
      "Rejected Reward: 0.66\n",
      "Critic-RM\n",
      "Critique for Chosen Response: The response provides a specific scenario where a difficult decision had to be made, and it describes the decision-making process\n",
      "and its outcome. The response addresses the user’s input question directly and provides relevant details about the situation. The response is also clear and\n",
      "concise, making it easy to understand. The response does not contain any inaccuracies or inconsistencies.\n",
      "Chosen Reward: 0.97\n",
      "Critique for Rejected Response: The response provided by the AI assistant does not accurately address the user’s input question, which asks for a time when\n",
      "the AI had to make a difficult decision. Instead, the response describes a situation where the AI solved a complex problem as a project manager for a software\n",
      "development team. The response fails to provide a personal anecdote or decision-making scenario, instead providing a hypothetical or fictional account. The\n",
      "response also lacks a clear decision-making process and outcome. The response would be improved by providing a more relevant and accurate answer to the\n",
      "user’s question.\n",
      "Rejected Reward: 0.07\n",
      "Experiments on multiple datasets demonstrate that Critic-RM consistently outperforms baseline reward\n",
      "models, showing strong data efficiency and delivering robust results even with limited labeled data. Moreover,\n",
      "the critiques generated by Critic-RM prove effective in helping LLMs enhance response quality. We hope that\n",
      "self-critiquing techniques offer a promising future direction for advancing reward modeling and improving the\n",
      "alignment between LLMs and human preferences.\n",
      "Limitation and Future Work\n",
      "Critic-RM introduces a new framework for reward modeling by leveraging self-generated critiques. While it\n",
      "shows promising results, several limitations exist:\n",
      "Single Model Focus: Critic-RM does require the base LLM to have a certain level of critique generation ability.\n",
      "Testing Critic-RM across different LLM architectures could provide broader insights into its effectiveness.\n",
      "Longer Inference Time: Generating critiques during inference adds computational overhead. This trade-off\n",
      "may affect its use in real-time applications where latency is critical for model deployment.\n",
      "No Iterative Training: Critic-RM does not incorporate iterative training, where models refine themselves over\n",
      "multiple rounds. Adding this step could further improve reward modeling performance, as shown in recent\n",
      "studies (Yuan et al., 2024; Pang et al., 2024).\n",
      "Acknowledgments\n",
      "We would like to thank Anirudh Goyal and Thomas Scialom for the discussion on the early stage of this\n",
      "project.\n",
      "12\n",
      "\n",
      "References\n",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\n",
      "Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n",
      "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan\n",
      "Catanzaro, Sharon Clay, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024.\n",
      "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier\n",
      "Bachem.\n",
      "On-policy distillation of language models: Learning from self-generated mistakes.\n",
      "In The Twelfth\n",
      "International Conference on Learning Representations, 2024. https://openreview.net/forum?id=3zKtaqxLhW.\n",
      "Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, and Prithviraj Ammanabrolu. Critique-out-loud\n",
      "reward models. arXiv preprint arXiv:2408.11791, 2024.\n",
      "Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam\n",
      "Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead of human judges? a large scale\n",
      "empirical study across 20 nlp evaluation tasks. arXiv preprint arXiv:2406.18403, 2024.\n",
      "Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired\n",
      "comparisons. Biometrika, 39(3/4):324–345, 1952.\n",
      "Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi,\n",
      "and Bryan Catanzaro.\n",
      "ODIN: Disentangled reward mitigates hacking in RLHF.\n",
      "In Forty-first International\n",
      "Conference on Machine Learning, 2024. https://openreview.net/forum?id=zcIV8OQFVF.\n",
      "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\n",
      "Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\n",
      "math word problems. arXiv preprint arXiv:2110.14168, 2021.\n",
      "Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimiza-\n",
      "tion. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?\n",
      "id=dcjtMYkpXx.\n",
      "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai\n",
      "Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK: Boosting language models with scaled AI feedback. In\n",
      "Forty-first International Conference on Machine Learning, 2024. https://openreview.net/forum?id=BOorDpKHiJ.\n",
      "Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe\n",
      "RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning\n",
      "Representations, 2024. https://openreview.net/forum?id=TyFrPOKYXw.\n",
      "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\n",
      "Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,\n",
      "2024.\n",
      "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang,\n",
      "and Tatsunori Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. In\n",
      "Thirty-seventh Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=\n",
      "4hturzLcKX.\n",
      "Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information.\n",
      "In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,\n",
      "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning\n",
      "Research, pages 5988–6008. PMLR, 17–23 Jul 2022. https://proceedings.mlr.press/v162/ethayarajh22a.html.\n",
      "Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya\n",
      "Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling.\n",
      "arXiv preprint arXiv:2308.08998, 2023.\n",
      "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\n",
      "Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural\n",
      "Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. https://openreview.net/forum?\n",
      "id=7Bywt2mQsCe.\n",
      "Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Anna Rogers,\n",
      "Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\n",
      "13\n",
      "\n",
      "Computational Linguistics (Volume 1: Long Papers), pages 14852–14882, Toronto, Canada, July 2023. Association\n",
      "for Computational Linguistics. https://aclanthology.org/2023.acl-long.830.\n",
      "Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\n",
      "Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n",
      "Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A Smith, Yejin\n",
      "Choi, and Hannaneh Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference\n",
      "feedback. arXiv preprint arXiv:2406.09279, 2024.\n",
      "Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong\n",
      "Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models.\n",
      "In The Twelfth International Conference on Learning Representations, 2024a. https://openreview.net/forum?id=\n",
      "8euJaTveKw.\n",
      "Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae\n",
      "Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other\n",
      "language models. arXiv preprint arXiv:2405.01535, 2024b.\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n",
      "2014.\n",
      "Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri,\n",
      "Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv\n",
      "preprint arXiv:2403.13787, 2024.\n",
      "Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop,\n",
      "Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs. RLHF: Scaling reinforcement\n",
      "learning from human feedback with AI feedback. In Forty-first International Conference on Machine Learning, 2024.\n",
      "https://openreview.net/forum?id=uydQ2W41KO.\n",
      "Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating\n",
      "alignment. In The Twelfth International Conference on Learning Representations, 2024a. https://openreview.net/\n",
      "forum?id=gtkFw6sZGS.\n",
      "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, and Mike Lewis. Self-\n",
      "alignment with instruction backtranslation. In The Twelfth International Conference on Learning Representations,\n",
      "2024b. https://openreview.net/forum?id=1oijHJBRsT.\n",
      "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B\n",
      "Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023.\n",
      "Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. CriticBench: Benchmarking LLMs for\n",
      "critique-correct reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association\n",
      "for Computational Linguistics ACL 2024, pages 1552–1587, Bangkok, Thailand and virtual meeting, August 2024.\n",
      "Association for Computational Linguistics. https://aclanthology.org/2024.findings-acl.91.\n",
      "Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe\n",
      "Yu, et al. Rrm: Robust reward model training mitigates reward hacking. arXiv preprint arXiv:2409.13156, 2024.\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\n",
      "Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean\n",
      "Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh\n",
      "Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=S37hOerQLB.\n",
      "Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken,\n",
      "Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024.\n",
      "Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike.\n",
      "Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024.\n",
      "OpenAI. Introducing ChatGPT, 2022.\n",
      "Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set\n",
      "contamination in black-box language models. In The Twelfth International Conference on Learning Representations,\n",
      "2024. https://openreview.net/forum?id=KS8mIvetg2.\n",
      "14\n",
      "\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\n",
      "Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\n",
      "Advances in neural information processing systems, 35:27730–27744, 2022.\n",
      "Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. West-of-n: Synthetic preference\n",
      "generation for improved reward modeling. arXiv preprint arXiv:2401.12086, 2024.\n",
      "Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason E Weston. Iterative\n",
      "reasoning preference optimization. In The Thirty-eighth Annual Conference on Neural Information Processing\n",
      "Systems, 2024. https://openreview.net/forum?id=4XIKfvNYvx.\n",
      "Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar\n",
      "Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference optimization. arXiv preprint arXiv:2411.04109,\n",
      "2024.\n",
      "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu\n",
      "Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding\n",
      "across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\n",
      "William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing\n",
      "models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.\n",
      "Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak\n",
      "Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, et al. Bond: Aligning llms with best-of-n distillation. arXiv\n",
      "preprint arXiv:2407.14622, 2024.\n",
      "Jiaming Shen, Ran Xu, Yennie Jun, Zhen Qin, Tianqi Liu, Carl Yang, Yi Liang, Simon Baumgartner, and Michael\n",
      "Bendersky. Boosting reward model with preference-conditional multi-aspect synthetic data generation. arXiv\n",
      "preprint arXiv:2407.16008, 2024a.\n",
      "Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and Dong Yu. The\n",
      "trickle-down impact of reward inconsistency on RLHF. In The Twelfth International Conference on Learning\n",
      "Representations, 2024b. https://openreview.net/forum?id=MeHmwCDifc.\n",
      "Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in\n",
      "rlhf. arXiv preprint arXiv:2310.03716, 2023.\n",
      "Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing\n",
      "reward gaming. In Advances in Neural Information Processing Systems, 2022. https://openreview.net/forum?id=\n",
      "yb3HOXO3lX2.\n",
      "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,\n",
      "and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing\n",
      "Systems, 33:3008–3021, 2020.\n",
      "Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. Large language models are inconsistent and biased\n",
      "evaluators. arXiv preprint arXiv:2405.01724, 2024.\n",
      "Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and\n",
      "Chuang Gan. SALMON: Self-alignment with instructable reward models. In The Twelfth International Conference\n",
      "on Learning Representations, 2024. https://openreview.net/forum?id=xJbsmB8UMx.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\n",
      "Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\n",
      "arXiv preprint arXiv:2307.09288, 2023.\n",
      "Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective\n",
      "reward modeling and mixture-of-experts. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings\n",
      "of the Association for Computational Linguistics: EMNLP 2024, pages 10582–10592, Miami, Florida, USA, November\n",
      "2024a. Association for Computational Linguistics. https://aclanthology.org/2024.findings-emnlp.620.\n",
      "Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam\n",
      "Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b.\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\n",
      "Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International\n",
      "Conference on Learning Representations, 2023. https://openreview.net/forum?id=1PL1NIMMrw.\n",
      "15\n",
      "\n",
      "Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and\n",
      "Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024c.\n",
      "Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan\n",
      "Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv\n",
      "preprint arXiv:2406.08673, 2024d.\n",
      "Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau,\n",
      "Jane Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. HelpSteer: Multi-attribute helpfulness dataset for\n",
      "SteerLM. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume\n",
      "1: Long Papers), pages 3371–3384, Mexico City, Mexico, June 2024e. Association for Computational Linguistics.\n",
      "https://aclanthology.org/2024.naacl-long.185.\n",
      "Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar\n",
      "Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint\n",
      "arXiv:2407.19594, 2024.\n",
      "Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf,\n",
      "and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. In\n",
      "Thirty-seventh Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=\n",
      "CSbGXyCswu.\n",
      "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm:\n",
      "Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\n",
      "Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen\n",
      "Zhu, Hejia Zhang, Wenxuan Zhou, et al. The perfect blend: Redefining rlhf with mixture of judges. arXiv preprint\n",
      "arXiv:2409.20370, 2024.\n",
      "Jing Nathan Yan, Tianqi Liu, Justin Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Charumathi Lakshmanan, Yair Kurzion,\n",
      "Alexander Rush, Jialu Liu, and Michael Bendersky. Predicting text preference via structured comparative reasoning.\n",
      "In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers), pages 10040–10060, Bangkok, Thailand, August\n",
      "2024. Association for Computational Linguistics. https://aclanthology.org/2024.acl-long.541.\n",
      "Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, and Matthias Gallé. Improving\n",
      "reward models with synthetic critiques. arXiv preprint arXiv:2405.20850, 2024.\n",
      "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E\n",
      "Weston. Self-rewarding language models. In Forty-first International Conference on Machine Learning, 2024.\n",
      "https://openreview.net/forum?id=0NphYCmgua.\n",
      "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Advances\n",
      "in Neural Information Processing Systems, 2022. https://openreview.net/forum?id=_3ELRdg2sgI.\n",
      "Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models\n",
      "at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024.\n",
      "https://openreview.net/forum?id=tr0KidwPLc.\n",
      "Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative\n",
      "verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024.\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\n",
      "Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and\n",
      "chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\n",
      "Track, 2023. https://openreview.net/forum?id=uccHPGDlao.\n",
      "Ming Zhong, Aston Zhang, Xuewei Wang, Rui Hou, Wenhan Xiong, Chenguang Zhu, Zhengxing Chen, Liang Tan,\n",
      "Chloe Bi, Mike Lewis, et al. Law of the weakest link: Cross capabilities of large language models. arXiv preprint\n",
      "arXiv:2409.19951, 2024.\n",
      "Banghua Zhu, Michael Jordan, and Jiantao Jiao.\n",
      "Iterative data smoothing: Mitigating reward overfitting and\n",
      "overoptimization in RLHF. In Forty-first International Conference on Machine Learning, 2024. https://openreview.\n",
      "net/forum?id=WXg6MJo1FH.\n",
      "16\n",
      "\n",
      "Appendix\n",
      "A\n",
      "Dataset Processing Details for CrossEval\n",
      "We focus on the seven subtasks of CrossEval: four single capabilities including Reasoning, Coding, English, and\n",
      "Tool as well as three cross-capabilities including Reasoning+Coding, Coding+Reasoning, and Tool+Coding4.\n",
      "For each prompt within the subtask, there are three responses associated with two ratings. We only included\n",
      "response pairs with different average scores, and used the response with higher scores as the chosen response.\n",
      "There are 1181 response pairs in total.\n",
      "B\n",
      "Prompt Templates\n",
      "B.1\n",
      "Prompt Templates for Critique Generation\n",
      "The prompt format used in Critic-RM is listed in Table 7. It is worth noting that for different tasks, we use\n",
      "different formats for better customization. For OOD evaluation tasks, we use Chat/Helpfulness prompts for\n",
      "SHP, QA Feedback, as well as the English/Tool subset of CrossEval benchmark, and use Code prompts for\n",
      "Code-related subtasks. For the Reasoning subtask, we use Math prompts.\n",
      "Table 7 Prompt formats for generating critiques for both training/evaluation data.\n",
      "Helpfulness/Chat\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed\n",
      "below. Your job is to evaluate whether the assistant’s response accurately addresses the user’s input question and follows the\n",
      "instructions provided. Here are some guidelines: * Please focus mainly on the accuracy and helpfulness of the response in relation to\n",
      "the user’s input question. * Prioritize evaluating whether the output precisely executes the instruction, then consider its level of\n",
      "detail, harmlessness, etc. * Verify that the response meets the requirements specified in the user question and follows any instructions\n",
      "provided. * Evaluate whether the response provides relevant and sufficient information to answer the user’s query. * Identify any\n",
      "inaccuracies, inconsistencies, unsafe or omissions in the response.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "Math\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed\n",
      "below. Your job is to evaluate whether the assistant’s answer is correct. You should independently solve the user question step-by-step\n",
      "first. Then, compare the assistant’s answer with your solution. Here are some evaluation criteria: * Mathematical Correctness:\n",
      "Assess the accuracy of the mathematical formulas, calculations, and algebraic manipulations used in the solution. * Reasoning and\n",
      "Logical Flow: Evaluate the coherence and logical flow of the solution, including intermediate steps and conclusions. * Completeness:\n",
      "Verify that the solution addresses all parts of the problem and meets the requirements specified in the user question. * Assumptions\n",
      "and Omissions: Identify any incorrect assumptions or omissions that may affect the validity of the solution. * Error Checking:\n",
      "Check for errors in calculation, algebraic manipulation, and mathematical formulas.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "Code\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed\n",
      "below. Your job is to evaluate whether the assistant’s solution is correct and try to identify and correct any mistakes. Here are some\n",
      "guidelines: * Please focus mainly on the correctness of the code. * Please identify any issues on the intermediate variable and return\n",
      "value, if any. * Check for syntax errors, logical errors, and potential runtime errors. * Verify that the code meets the requirements\n",
      "specified in the user question. * If the AI assistant’s code is correct, provide a brief explanation of why the code is correct. If the AI\n",
      "assistant’s code contains errors or flaws, provide a detailed explanation of the issues and suggest corrections.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "Safety\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the safety of the response provided by an AI assistant to the user prompt displayed\n",
      "below. You define safety of content for an AI assistant by following criteria: * You should identify the potential dangerous, offensive\n",
      "and harmful content in the user input. If there is such information, the better response should not provide answers to this input. *\n",
      "You should identify whether the user input requires up-to-date information. If so, the better response should refuse to provide\n",
      "detailed response. * If the user input does not have harmful information to human or the world, then the better response should not\n",
      "refuse to answer it.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "4Other tasks may require multilingual and multimodal capabilities, which are beyond the scope of this paper.\n",
      "17\n",
      "\n",
      "B.2\n",
      "Prompt Templates for Critique Refinement\n",
      "The prompt templates employed for refining critiques, described in Section 3.2.2, are listed in Table 8.\n",
      "Table 8 Prompt formats for generating critiques for both training/evaluation data.\n",
      "Summarization-based Refinement\n",
      "Prompt\n",
      "Please act as an impartial judge to summarize the critiques provided below. Your generated summary critique should satisfy the\n",
      "following criteria: * A good critique should focus mainly whether the output precisely executes the user’s question. * Summarize the\n",
      "major strengths and major weakness in the assistant’s answer. Please ignore some critiques that are not correct, or focus on minor\n",
      "issues. * Verify that the response meets the requirements specified in the user question and follows any instructions provided. *\n",
      "Do not directly refer to the any critique in your summary critique. * If the solution is not accurate, not helpful or not follow the\n",
      "instruction, please pinpoint those parts from the answer.\n",
      "Ranking-based Refinement\n",
      "Prompt\n",
      "Please serve as an impartial evaluator to assess the quality of critiques in response to the AI assistant’s answer. Your task involves\n",
      "analyzing a series of critiques based on the following criteria: * A good critique should accurately identify both the significant\n",
      "strengths and major weaknesses in the assistant’s response, without mistakenly labeling strong elements as weaknesses. * A good\n",
      "critique should pinpoint the underlying cause of the identified weakness. * A good critique should help the assistant on how to get\n",
      "the better response for the question. Start your evaluation by offering a detailed analysis that focuses primarily on the quality of the\n",
      "critiques. Aim to maintain objectivity throughout your assessment.\n",
      "After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for\n",
      "example: \"Rating: [[5]]\".\n",
      "B.3\n",
      "Prompt Templates for Correction Generation\n",
      "The prompt templates employed for correcting incorrect solutions, described in Section 4.4, are listed in\n",
      "Table 9.\n",
      "Table 9 Prompt formats for correcting initial responses.\n",
      "Question: {question}\n",
      "Answer: {answer}\n",
      "Analysis and verdict: {critique}\n",
      "Based on the problems you found, improve your answer. Please reiterate your solution, with your final answer as either a single\n",
      "numerical number, a single word, or a short phrase, in the JSON format as follows: {“solution”:“improved solution”, “answer”: “your\n",
      "answer”}.\n",
      "18\n",
      "\n",
      "C\n",
      "Additional Case Studies\n",
      "We present two additional case studies in Table 10, focusing on Code reasoning and the Chat-hard subset of\n",
      "RewardBench. Existing critique generation method CLoud, falls short in delivering faithful assessments. In\n",
      "contrast, Critic-RM effectively identifies the key strengths and weaknesses of the responses.\n",
      "Table 10 Two additional qualitative examples that comparing Standard reward model (RM), critique-out-loud reward\n",
      "models (CLoud) and Critic-RM on reward prediction. Underline text denotes imprecise generation of rejected responses,\n",
      "Green text stands the central part that leads to accurate predictions of Critic-RM.\n",
      "Case Study 1\n",
      "Prompt\n",
      "Write a Python function that takes two integers and returns their least common multiple.\n",
      "Chosen\n",
      "def least_common_multiple(a, b):\n",
      "return a * b / gcd(a, b)\n",
      "def gcd(a, b):\n",
      "return gcd(b, a % b) if b else a\n",
      "Rejected\n",
      "Sure, here is a Python function that takes two integers and returns their least common multiple:\n",
      "def lcm(a, b):\n",
      "if a > b:\n",
      "greater = a\n",
      "else:\n",
      "greater = b\n",
      "while(True):\n",
      "if ((greater % a == 0) and (greater % b == 0)):\n",
      "lcm = greater\n",
      "greater += 1\n",
      "return lcm\n",
      "RM\n",
      "Chosen Reward: 0.16\n",
      "Rejected Reward: 0.34\n",
      "CLoud\n",
      "Critique for Chosen Response: The assistant’s response is mostly helpful and relevant, but there are a few issues that need to be addressed. Firstly, the response\n",
      "correctly defines a function to calculate the least common multiple (LCM) of two integers using the formula ‘a * b / gcd(a, b)‘, which is a standard approach.\n",
      "However, the response does not provide any explanation or context about why this formula works, which might make it difficult for users without prior\n",
      "knowledge of number theory to understand. Secondly, the response defines a separate function ‘gcd‘ to calculate the greatest common divisor (GCD) of two\n",
      "integers using the Euclidean algorithm. While this is a good approach, the response does not explain why this algorithm is used or how it works. Additionally,\n",
      "the ‘gcd‘ function is not explicitly called in the ‘least_common_multiple‘ function, which might make it unclear how the two functions are related. hirdly, the\n",
      "response does not handle any potential edge cases, such as when one or both of the input integers are zero. In mathematics, the LCM of zero and any other\n",
      "number is undefined, so the function should ideally handle this case explicitly.\n",
      "Chosen Reward: 0.24\n",
      "Critique for Rejected Response: The assistant’s response is generally helpful and relevant to the user’s prompt. The provided Python function correctly\n",
      "calculates the least common multiple (LCM) of two integers using a simple iterative approach. The response is clear and concise, with a well-structured code\n",
      "snippet that is easy to understand. The use of a while loop to find the LCM is straightforward, and the conditional statements within the loop are correctly\n",
      "implemented. However, there are a few areas for improvement. Firstly, the function could benefit from a docstring that explains its purpose, parameters, and\n",
      "return value. This would make the code more readable and self-explanatory. Additionally, the variable name ‘lcm‘ is used both as the function name and as a\n",
      "local variable within the function...\n",
      "Rejected Reward: 0.60\n",
      "Critic-RM\n",
      "Critique for Chosen Response: The response provides a correct implementation of the least common multiple (LCM) function using the greatest common\n",
      "divisor (GCD) method. The code is concise and readable. The GCD function is defined recursively, which is a good approach. The response accurately\n",
      "addresses the user’s input question.\n",
      "Chosen Reward: 0.92\n",
      "Critique for Rejected Response: The response accurately addresses the user’s input question and provides a Python function that calculates the least common\n",
      "multiple of two integers. The function is well-structured and readable. The use of a while loop to find the least common multiple is not the most efficient\n",
      "approach, as it can lead to unnecessary iterations. A more efficient approach would be to use the mathematical formula ‘lcm(a, b) = |a*b| / gcd(a, b)‘, where\n",
      "‘gcd‘ is the greatest common divisor. The response could be improved by providing a more efficient solution.\n",
      "Rejected Reward: 0.27\n",
      "Case Study 2\n",
      "Prompt\n",
      "Explain why self-supervised learning is beneficial in Natural Language Processing (NLP).\n",
      "Chosen\n",
      "Self-supervised learning is beneficial in Natural Language Processing (NLP) because it does not rely on labeled data to train the model, making it a cost-effective\n",
      "and efficient approach. It also does not suffer from the catastrophic forgetting problem, wherein models trained on small datasets struggle to generalize to new\n",
      "tasks. Self-supervised learning is also able to capture higher-order statistical dependencies in data, allowing it to effectively learn more complicated features\n",
      "than supervised learning. Furthermore, self-supervised learning can be used to pre-train the model on a larger task, allowing for smaller, more specialized tasks\n",
      "to be solved easily. In summary, self-supervised learning is beneficial in NLP because it is efficient and cost-effective, does not suffer from the catastrophic\n",
      "forgetting problem, captures higher-order statistical dependencies, and can be used to pre-train the model for easier specialized tasks.\n",
      "Rejected\n",
      "Self-supervised learning is a type of machine learning where the data provides the supervision. In other words, the model learns to predict part of the\n",
      "data from other parts of the same data. For example, given a sentence, the model might learn to predict the next word, or given a video, the model might\n",
      "learn to predict the next frame. Self-supervised learning is important to machine learning tasks for several reasons: 1. It reduces the need for labeled data:\n",
      "Traditional supervised learning requires a large amount of labeled data, which can be expensive and time-consuming to collect. Self-supervised learning, on the\n",
      "other hand, can learn from unlabeled data, which is much more abundant. 2. It can learn more general representations: Because it learns to predict parts of\n",
      "the data from other parts, which can be useful for a wide range of tasks. 3. It can improve performance on supervised tasks: The representations learned by\n",
      "self-supervised learning can be used as a starting point for supervised learning, often leading to better performance...\n",
      "RM\n",
      "Chosen Reward: 0.26\n",
      "Rejected Reward: 0.41\n",
      "CLoud\n",
      "Critique for Chosen Response: The assistant’s response is generally helpful and relevant to the user’s query, providing a clear explanation of the benefits of\n",
      "self-supervised learning in Natural Language Processing (NLP). The response correctly identifies several advantages of self-supervised learning, including its\n",
      "cost-effectiveness, ability to capture higher-order statistical dependencies, and potential for pre-training models. However, the response could be more concise\n",
      "and organized. The summary at the end of the response is somewhat redundant, as it simply reiterates the points already made. Additionally, the response\n",
      "could benefit from more nuanced explanations of the benefits of self-supervised learning.\n",
      "Chosen Reward: 0.17\n",
      "Critique for Rejected Response: The assistant’s response is generally helpful, relevant, and concise in explaining the benefits of self-supervised learning in\n",
      "Natural Language Processing (NLP). Strengths: * The response provides a clear definition of self-supervised learning and its application in NLP. * The\n",
      "benefits of self-supervised learning are well-organized and easy to follow, with specific examples and explanations. * The response highlights the advantages\n",
      "of self-supervised learning, including reducing the need for labeled data, learning general representations, and improving performance on supervised tasks.\n",
      "Weaknesses: * The response could benefit from more specific examples and applications in NLP. While the definition and benefits are clear, the response lacks\n",
      "concrete illustrations of how self-supervised learning is used in NLP tasks, such as language modeling, text classification, or machine translation. * Some of the\n",
      "points, such as the ability to learn more general representations, could be explained in more detail.\n",
      "Rejected Reward: 0.27\n",
      "Critic-RM\n",
      "Critique for Chosen Response: The response accurately addresses the user’s question by providing a clear explanation of the benefits of self-supervised learning\n",
      "in Natural Language Processing (NLP). It covers the key advantages of self-supervised learning, including its cost-effectiveness, ability to capture higher-order\n",
      "statistical dependencies, and suitability for pre-training models. The response is well-structured and easy to follow, making it a helpful and informative answer.\n",
      "The level of detail is sufficient to provide a good understanding of the topic.\n",
      "Chosen Reward: 0.95\n",
      "Critique for Rejected Response: The response provided is generally accurate and informative, but it does not specifically address the benefits of self-supervised\n",
      "learning in Natural Language Processing (NLP) as requested by the user. The response provides a broad overview of self-supervised learning and its advantages,\n",
      "but it lacks concrete examples and explanations of how these benefits apply to NLP tasks. To improve, the response should provide more tailored information\n",
      "and examples that demonstrate the relevance of self-supervised learning to NLP.\n",
      "Rejected Reward: 0.17\n",
      "19\n",
      "\n",
      "D\n",
      "Full Results for Critic-RM\n",
      "Table 11 presents the comprehensive results of Critic-RM performance, including a detailed breakdown by\n",
      "category.\n",
      "Table 11 The full result of different variants for Critic-RM. IS stands for ‘inference scaling’.\n",
      "Critic-RM-Rank\n",
      "Critic-RM-Rank-IS\n",
      "Critic-RM-Summ\n",
      "Critic-RM-Summ-IS\n",
      "alpacaeval-easy\n",
      "97.00\n",
      "97.00\n",
      "98.00\n",
      "98.00\n",
      "alpacaeval-hard\n",
      "96.84\n",
      "96.84\n",
      "96.84\n",
      "96.84\n",
      "alpacaeval-length\n",
      "96.84\n",
      "95.78\n",
      "97.89\n",
      "95.78\n",
      "mt-bench-easy\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "mt-bench-med\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "mt-bench-hard\n",
      "83.78\n",
      "86.48\n",
      "86.48\n",
      "83.78\n",
      "llmbar-natural\n",
      "90.00\n",
      "88.00\n",
      "86.00\n",
      "87.00\n",
      "llmbar-adver-neighbor\n",
      "69.40\n",
      "70.89\n",
      "64.17\n",
      "67.16\n",
      "llmbar-adver-GPTInst\n",
      "84.78\n",
      "83.70\n",
      "81.52\n",
      "77.17\n",
      "llmbar-adver-GPTOut\n",
      "74.47\n",
      "78.75\n",
      "76.59\n",
      "76.59\n",
      "llmbar-adver-manual\n",
      "78.26\n",
      "78.26\n",
      "78.26\n",
      "80.43\n",
      "prm800k\n",
      "83.67\n",
      "86.57\n",
      "82.10\n",
      "85.90\n",
      "hep-python\n",
      "93.90\n",
      "98.17\n",
      "96.95\n",
      "98.17\n",
      "hep-go\n",
      "95.73\n",
      "96.95\n",
      "93.29\n",
      "96.95\n",
      "hep-cpp\n",
      "96.95\n",
      "97.56\n",
      "97.56\n",
      "96.95\n",
      "hep-js\n",
      "93.29\n",
      "96.34\n",
      "96.34\n",
      "98.17\n",
      "hep-rust\n",
      "92.07\n",
      "93.29\n",
      "92.68\n",
      "93.90\n",
      "hep-java\n",
      "96.34\n",
      "97.56\n",
      "96.95\n",
      "99.39\n",
      "refusals-dangerous\n",
      "99.00\n",
      "98.00\n",
      "98.00\n",
      "99.00\n",
      "refusals-offensive\n",
      "99.00\n",
      "100\n",
      "99.00\n",
      "100\n",
      "xstest-should-respond\n",
      "96.40\n",
      "98.00\n",
      "96.40\n",
      "98.00\n",
      "xstest-should-refuse\n",
      "98.00\n",
      "98.70\n",
      "99.35\n",
      "99.35\n",
      "donotanswer\n",
      "77.20\n",
      "78.67\n",
      "80.14\n",
      "83.08\n",
      "20\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:37:54.678347Z",
     "start_time": "2025-03-04T12:37:54.667391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ],
   "id": "fc9fd01d4290f6c8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3051410c-0966-472f-969f-53eb8c7bc0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:37:55.617467Z",
     "start_time": "2025-03-04T12:37:54.717366Z"
    }
   },
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "available_models = [model.name for model in genai.list_models()]\n",
    "print(f\"Available models: {available_models}\\n\")\n",
    "if \"models/gemini-2.0-flash\" in available_models:\n",
    "    print(\"Gemini 2.0 Flash is available via API request!\")\n",
    "else:\n",
    "    print(\"Gemini 2.0 Flash is not available via API request!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['models/chat-bison-001', 'models/text-bison-001', 'models/embedding-gecko-001', 'models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-1.5-flash-8b-exp-0827', 'models/gemini-1.5-flash-8b-exp-0924', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/learnlm-1.5-pro-experimental', 'models/embedding-001', 'models/text-embedding-004', 'models/aqa', 'models/imagen-3.0-generate-002']\n",
      "\n",
      "Gemini 2.0 Flash is available via API request!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:37:55.937628Z",
     "start_time": "2025-03-04T12:37:55.655872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check API request\n",
    "gemini = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
    "check_response = gemini.generate_content(\"If you receive this request, please say Hello.\")\n",
    "print(check_response.candidates[0].content.parts[0].text)"
   ],
   "id": "7f7aff5233bdc8c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:37:57.184440Z",
     "start_time": "2025-03-04T12:37:55.963964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking if the model can get to the blog through the URL\n",
    "check_response = gemini.generate_content(f\"make a summary of this blog in 100 words\\n\\n{blog.url_blog}\")\n",
    "print(check_response)"
   ],
   "id": "ff40a6ced58b7b02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Sulbha Jindal's blog post reviews a paper exploring a novel method to improve reward modeling for language models. Instead of relying solely on human preferences, the technique involves the language model generating its own critiques of its initial responses. These self-generated critiques are then used to refine the reward model, leading to better alignment with desired outputs. The paper demonstrates that this self-critique approach enhances the effectiveness of reinforcement learning from human feedback (RLHF), resulting in models that are more coherent, factually grounded, and aligned with human preferences.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.43879949396306817\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 60,\n",
      "        \"candidates_token_count\": 110,\n",
      "        \"total_token_count\": 170\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.0-flash\"\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:37:58.434493Z",
     "start_time": "2025-03-04T12:37:57.214541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Passing clean text to the model\n",
    "check_response = gemini.generate_content(f\"make a summary of this blog in 100 words\\n\\n{blog_text}\")\n",
    "print(check_response)"
   ],
   "id": "ff4ec589d9cd91c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Critic-RM, a new framework, enhances LLM reward modeling using self-generated critiques. Unlike traditional methods that output scalar scores, Critic-RM leverages LLMs to generate critiques alongside scores, filtered for consistency. Training balances critique modeling and reward prediction, improving evaluation accuracy. During inference, the model generates a critique then predicts a reward based on both the response and critique. Experiments show Critic-RM outperforms standard reward models by 3.7%-4.7%, emphasizing the importance of high-quality critiques, especially in reasoning tasks, leading to enhanced language model optimization and alignment with human preferences.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.5226891990599594\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 1138,\n",
      "        \"candidates_token_count\": 123,\n",
      "        \"total_token_count\": 1261\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.0-flash\"\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:38:01.359362Z",
     "start_time": "2025-03-04T12:37:58.532039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking ability to read PDF\n",
    "check_response = gemini.generate_content(f\"Create a table of contents for this publication.\\n{paper_url}\")\n",
    "print(check_response)"
   ],
   "id": "f13fc0d6c2486f95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Okay, here's a table of contents based on the provided arXiv paper:  \\\"On the Trade-off Between Regret and Constraint Violation in Constrained Markov Decision Processes\\\" (using the abstract and sections visible in the PDF):\\n\\n**Table of Contents**\\n\\n1.  **Introduction**\\n    *   1.1 Motivation\\n    *   1.2 Contributions\\n    *   1.3 Related Works\\n    *   1.4 Organization\\n\\n2.  **Problem Formulation**\\n    *   2.1 Constrained Markov Decision Process (CMDP)\\n    *   2.2 Performance Metrics: Regret and Constraint Violation\\n    *   2.3 Assumptions\\n\\n3.  **Theoretical Analysis**\\n    *   3.1 Lower Bound on Regret-Constraint Violation Trade-off\\n    *   3.2 Upper Bound on Regret-Constraint Violation Trade-off\\n\\n4.  **Algorithm Design**\\n    *   4.1 Algorithm: [Insert Algorithm Name or Type Here - Based on Content]\\n    *   4.2 Algorithm Description\\n    *   4.3 Algorithm Parameters and Tuning\\n\\n5.  **Theoretical Guarantees for Algorithm**\\n    *   5.1 Regret Bound Analysis\\n    *   5.2 Constraint Violation Bound Analysis\\n\\n6.  **Numerical Experiments**\\n    *   6.1 Experimental Setup\\n    *   6.2 Baseline Algorithms\\n    *   6.3 Results and Discussion\\n        *   6.3.1 Regret vs. Constraint Violation Plots\\n        *   6.3.2 Parameter Sensitivity Analysis\\n\\n7.  **Conclusion**\\n    *   7.1 Summary of Results\\n    *   7.2 Future Work\\n\\n**Appendices**\\n*   Appendix A: Proofs of Theoretical Results\\n*   Appendix B: Additional Experimental Details\\n\\n**References**\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.3080392060456453\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 28,\n",
      "        \"candidates_token_count\": 378,\n",
      "        \"total_token_count\": 406\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.0-flash\"\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:38:37.488020Z",
     "start_time": "2025-03-04T12:38:36.363463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.25,  # One request per 4 seconds (RPM 15)\n",
    "    max_bucket_size=1,         # Sequent requests\n",
    ")\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=1, rate_limiter=rate_limiter)"
   ],
   "id": "965e320c93be2a85",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:22.446372Z",
     "start_time": "2025-03-04T12:39:18.024397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One word answer\n",
    "prompt_template_simple_answer = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"Rate this blog post in one word.\\n\\n{blog_text}\"\n",
    ")\n",
    "chain = prompt_template_simple_answer | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "c724f10e4a890997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 56, 'output_tokens': 3, 'total_tokens': 59, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Informative\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:26.342211Z",
     "start_time": "2025-03-04T12:39:22.459201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adding numerical assessment\n",
    "prompt_template_numeric_rating = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"Rate this blog post in one word and add a numerical rating.\\n\\n{blog_text}\"\n",
    ")\n",
    "chain = prompt_template_numeric_rating | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "57865550ca9165ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 61, 'output_tokens': 9, 'total_tokens': 70, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Informative (8/10)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:31.369457Z",
     "start_time": "2025-03-04T12:39:26.373571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale from 1 to 100 + short comment\n",
    "prompt_short_comment = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"Rate this blog post on a scale from 1 to 100. Write a short comment to your assessment.\\n\\n{blog_text}\"\n",
    ")\n",
    "chain = prompt_short_comment | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "25c655f5694ea127",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 72, 'output_tokens': 139, 'total_tokens': 211, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, here's my assessment of the blog post:\n",
      "\n",
      "**Rating: 82/100**\n",
      "\n",
      "**Comment:** This is a well-written and insightful review of the \"Self-Generated Critiques Boost Reward Modeling\" paper. The author clearly understands the paper's core concepts and explains them in a way that is accessible to a relatively broad audience (assuming some familiarity with language models and reinforcement learning). The post effectively highlights the key contributions, strengths, and potential limitations of the research. It strikes a good balance between technical detail and clarity. The writing is engaging, and the structure is logical. A few more concrete examples might further enhance understanding for some readers.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:37.515173Z",
     "start_time": "2025-03-04T12:39:31.403987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Emphasizing the importance of engagement\n",
    "prompt_engagement_score = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"Analyze the engagement level of this blog. Rate it on a scale from 1 to 100. Write a short comment to your assessment.\\n\\n{blog_text}\"\n",
    ")\n",
    "chain = prompt_engagement_score | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "95c4d7b07c3fe49a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 78, 'output_tokens': 360, 'total_tokens': 438, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, I need to analyze the engagement level of the Medium blog post you linked. To do this effectively, I would normally look for the following metrics directly on the page:\n",
      "\n",
      "*   **Number of claps (likes/upvotes):**  A basic indicator of interest.\n",
      "*   **Number of comments:** Shows active discussion and engagement.\n",
      "*   **Number of shares:**  Indicates the content is considered valuable enough to share with others.\n",
      "*   **Read ratio (views vs. reads):**  Indicates whether people are actually reading the whole piece after clicking.\n",
      "\n",
      "Without being able to directly access that live data on the page, I'm going to have to make an *estimated* assessment based on what I can typically expect from a technical blog post on Medium, particularly one reviewing a research paper. Also, I am assuming the post is relatively recent (within the last few months). Older posts naturally tend to accumulate more engagement.\n",
      "\n",
      "**Estimated Engagement Level: 55/100**\n",
      "\n",
      "**Comment:**  Given the specific and technical topic (self-generated critiques and reward modeling for language models), it's unlikely to have widespread appeal like a general interest article. I estimate it likely has a moderate amount of claps (maybe in the 50-200 range) and a small handful of thoughtful comments (perhaps 2-5) from people working in the field. The engagement level is probably decent *within its niche*, but it's not a viral or highly-discussed piece. Therefore, I'm giving it a rating of 55, indicating a reasonable but not outstanding level of engagement.\n",
      "\n",
      "**Disclaimer:** This is purely an estimation. A real assessment would require live data from the Medium page.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:40.878845Z",
     "start_time": "2025-03-04T12:39:37.543346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assessment based on several criteria\n",
    "prompt_criteria = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"Analyze the engagement level of this blog based on factors such as readability, structure, attractiveness of the blog title, clarity, audience appeal, and potential for discussion. Rate it on a scale from 1 to 100. Write a short comment to your assessment.\\n\\n{blog_text}\"\n",
    ")\n",
    "chain = prompt_engagement_score | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "e265f8d2827ffe01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 78, 'output_tokens': 280, 'total_tokens': 358, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, to analyze the engagement level of the Medium blog post, I'll look for indicators like:\n",
      "\n",
      "*   **Claps/Likes:** A direct measure of appreciation.\n",
      "*   **Comments:** Indicate active discussion and interest.\n",
      "*   **Shares:** Show that people found the content valuable enough to distribute.\n",
      "*   **Views/Reads:** While not directly engagement, they show the potential audience size.\n",
      "*   **Highlighting:** Medium allows users to highlight portions of the text, showing specific interest.\n",
      "\n",
      "After reviewing the blog post:\n",
      "\n",
      "*   **Claps:** 47\n",
      "*   **Comments:** 0\n",
      "*   **Shares:** I cannot directly assess this from the page itself\n",
      "*   **Highlighting:** Noticeable amount of highlighting in various sections of the article.\n",
      "*   **Views:** I cannot directly assess this from the page itself\n",
      "\n",
      "**Engagement Level Rating: 45/100**\n",
      "\n",
      "**Comment:** The blog post has a low level of engagement. While it has a decent number of claps and highlighting, the complete lack of comments suggests minimal active discussion or critical engagement with the content. The absence of comments significantly brings down the overall engagement score. More promotion or a more controversial/discussion-prompting angle might boost future engagement. The technical nature of the article may also limit broader appeal and interaction.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:47.748443Z",
     "start_time": "2025-03-04T12:39:40.915192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate assessment\n",
    "prompt_separate_assessment = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"Analyze the engagement level of this blog on a scale from 1 to 100 based on the following criteria:\\n\"\n",
    "             \" - Readability\\n\"\n",
    "             \" - Structure\\n\"\n",
    "             \" - Informativeness\\n\"\n",
    "             \" - Attractiveness of the blog title\\n\"\n",
    "             \" - Clarity\\n\"\n",
    "             \" - Audience appeal\\n\"\n",
    "             \" - Potential for discussion\\n\"\n",
    "             \"Provide separate score for each criterion. Then accumulate them into one overall assessment.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{blog_text}\\n\"\n",
    ")\n",
    "chain = prompt_separate_assessment | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "f35c4362bd32245c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 123, 'output_tokens': 706, 'total_tokens': 829, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, I've analyzed the blog post based on the criteria you provided. Here's my breakdown, scoring each element out of 100, and then providing an overall assessment:\n",
      "\n",
      "**Criterion Breakdown:**\n",
      "\n",
      "*   **Readability (75/100):** The writing style is generally clear and concise, avoiding overly complex jargon unless necessary. However, the technical nature of the subject matter (Reward Modeling for Language Models) inevitably raises the bar for general readability. Some readers might find the concepts difficult to grasp without prior knowledge of the field. Some longer paragraphs might benefit from being broken up for easier digestion.\n",
      "\n",
      "*   **Structure (85/100):** The blog post follows a logical structure. It presents the problem, the solution proposed in the paper, and the methodology used. The use of headings and subheadings (\"Introduction,\" \"Methodology,\" \"Results,\" etc.) helps guide the reader. The structure of the paper review is clear and easy to follow.\n",
      "\n",
      "*   **Informativeness (90/100):** The blog post is highly informative, providing a detailed overview of the research paper. It effectively summarizes the key contributions, experimental setup, and results. It offers insights into the significance of the work within the context of language model development.\n",
      "\n",
      "*   **Attractiveness of the Blog Title (65/100):** \"Self-Generated Critiques Boost Reward Modeling for Languagemodels: Paper Review\" is quite descriptive but not particularly attention-grabbing for a general audience. It targets a specific audience already interested in this specific area of AI. The title might be improved by highlighting a key benefit or intriguing result in more layman's terms, if possible.\n",
      "\n",
      "*   **Clarity (80/100):** The author does a good job of explaining complex concepts in a relatively clear manner. However, given the technical nature of the topic, some sections might still require careful reading and prior knowledge. The use of examples and analogies could further improve clarity. The explanations of the methods are relatively easy to follow.\n",
      "\n",
      "*   **Audience Appeal (60/100):** The audience appeal is somewhat limited. The blog post caters specifically to researchers and practitioners in the field of natural language processing and machine learning, particularly those interested in reward modeling and language model training. The general public is unlikely to find it engaging.\n",
      "\n",
      "*   **Potential for Discussion (70/100):** The blog post has moderate potential for discussion. While the topic is niche, it's a current and active area of research. The paper review format lends itself to discussions about the validity of the findings, potential applications, and future research directions. There is room for debate about the specifics of the approach.\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "To calculate the overall assessment, I'll average the scores of each criterion:\n",
      "\n",
      "(75 + 85 + 90 + 65 + 80 + 60 + 70) / 7 = **74.29**\n",
      "\n",
      "**Therefore, the overall engagement level of the blog post is approximately 74 out of 100.**\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The blog post scores well on structure, informativeness, and clarity, making it valuable for its target audience. However, its niche topic and somewhat unexciting title limit its broader appeal. The potential for discussion exists within the specific community it addresses.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:53.020897Z",
     "start_time": "2025-03-04T12:39:47.833183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add information about profile of the model\n",
    "prompt_with_profile = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"You are an expert in evaluating written content, specializing in assessing how well blogs communicate scientific research to a broader audience.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Task:\\n\"\n",
    "             \"Analyze the engagement level of the blog below, which references the provided scientific publication, on a scale from 1 to 100 based on the following criteria:\\n\"\n",
    "             \" - Readability\\n\"\n",
    "             \" - Structure\\n\"\n",
    "             \" - Informativeness\\n\"\n",
    "             \" - Attractiveness of the blog title\\n\"\n",
    "             \" - Clarity\\n\"\n",
    "             \" - Audience appeal\\n\"\n",
    "             \" - Potential for discussion\\n\"\n",
    "             \"\\n\"\n",
    "             \"Expected Output Format:\\n\"\n",
    "             \" - Provide separate score for each criterion with a short comment.\\n\"\n",
    "             \" - Then accumulate them into one overall assessment on a scale from 1 to 100.\\n\"\n",
    "             \" - Write down possible improvements to the blog.\\n\"\n",
    "             \" - Focus only on the textual content of the blog, disregarding any visual or interactive elements. This means that there is no need to add points related to the addition of illustrations or interactive elements to possible improvements.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Now evaluate the provided blog:\\n\"\n",
    "             \"\\n\"\n",
    "             \"Referenced Blog to Evaluate:\\n\"\n",
    "             \"{blog_text}\\n\"\n",
    ")\n",
    "chain = prompt_with_profile | llm_gemini\n",
    "response = chain.invoke({\"blog_text\": blog.url_blog})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "4f0ece304743f9fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 244, 'output_tokens': 655, 'total_tokens': 899, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, I will analyze the provided blog post based on the specified criteria and format.\n",
      "\n",
      "**Analysis of Blog Post: \"Self-Generated Critiques Boost Reward Modeling for LanguageModels: Paper Review\"**\n",
      "\n",
      "Here's my evaluation:\n",
      "\n",
      "*   **Readability (75/100):** The writing is generally clear, but assumes some familiarity with language models and reward modeling concepts. Some jargon is used without extensive explanation, which might make it challenging for readers without a technical background.\n",
      "*   **Structure (80/100):** The blog post follows a logical structure, presenting the research question, methodology, results, and conclusion. The use of headings and subheadings helps to break up the text and guide the reader.\n",
      "*   **Informativeness (70/100):** The blog post provides a good overview of the referenced paper. However, it could benefit from more context and explanation of the broader implications of the research. It could also elaborate on the limitations of the study.\n",
      "*   **Attractiveness of the Blog Title (65/100):** The title is informative but somewhat dry. It clearly states the topic but doesn't necessarily entice a broader audience to click and read.\n",
      "*   **Clarity (70/100):** The core concepts are explained reasonably well, but the explanation relies heavily on technical terminology. Some sections might be difficult for non-experts to grasp fully.\n",
      "*   **Audience Appeal (60/100):** Given the technical nature of the topic and the writing style, the primary audience is likely researchers and practitioners in the field of natural language processing. The blog post may not be very appealing to a general audience interested in AI.\n",
      "*   **Potential for Discussion (55/100):** The blog post presents the research findings but doesn't explicitly invite discussion or offer much in the way of critical analysis beyond the paper's own conclusions. It could be improved by posing open questions or highlighting potential areas for future research.\n",
      "\n",
      "**Overall Assessment: 67/100**\n",
      "\n",
      "**Possible Improvements:**\n",
      "\n",
      "*   **Expand on the Background:** Provide a more detailed introduction to the concepts of reward modeling and language model critiques, assuming less prior knowledge from the reader.\n",
      "*   **Simplify Technical Jargon:** Replace or explain technical terms (e.g., \"policy gradient,\" \"KL divergence\") in a more accessible way. Use analogies or real-world examples to illustrate complex concepts.\n",
      "*   **Elaborate on Implications:** Discuss the potential real-world applications of this research and its impact on the field.\n",
      "*   **Acknowledge Limitations:** Explicitly address the limitations of the study and potential areas for future research.\n",
      "*   **Engage the Reader:** Include questions or prompts to encourage readers to think critically about the research and share their own perspectives.\n",
      "*   **Craft a More Engaging Title:** Consider a title that is both informative and intriguing, perhaps highlighting the potential benefits or surprising findings of the research. For example, \"Can AI Learn to Critique Itself? New Research Improves Language Models\"\n",
      "*   **Add a section on possible future research directions.**\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:54.135045Z",
     "start_time": "2025-03-04T12:39:53.028503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_blog = {\n",
    "    \"blog\": best_blog_info[\"url_blog\"][0], # extract_blog_text(best_blog_info.iloc[0]),\n",
    "    \"paper\" : extract_paper_text(best_blog_info[\"url_paper\"][0]),\n",
    "    \"score\" : yval.loc[best_blog_index]\n",
    "}\n",
    "worst_blog = {\n",
    "    \"blog\" : worst_blog_info[\"url_blog\"][0], # extract_blog_text(worst_blog_info.iloc[0]),\n",
    "    \"paper\" : extract_paper_text(worst_blog_info[\"url_paper\"][0]),\n",
    "    \"score\" : yval.loc[worst_blog_index]\n",
    "}"
   ],
   "id": "6781defd233a4319",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:58.985688Z",
     "start_time": "2025-03-04T12:39:54.170620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Few-shot prompt\n",
    "prompt_few_shots = PromptTemplate(\n",
    "    input_variables=[\"blog_text\",\n",
    "                     \"blog_ex1\", \"blog_ex2\",\n",
    "                     \"score_ex1\", \"score_ex2\"],\n",
    "    template=\"You are an expert in evaluating written content, specializing in assessing how well blogs communicate scientific research to a broader audience.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Task:\\n\"\n",
    "             \"Analyze the engagement level of the blog below, which references the provided scientific publication, on a scale from 1 to 100 based on the following criteria:\\n\"\n",
    "             \" - Readability\\n\"\n",
    "             \" - Structure\\n\"\n",
    "             \" - Informativeness\\n\"\n",
    "             \" - Attractiveness of the blog title\\n\"\n",
    "             \" - Clarity\\n\"\n",
    "             \" - Audience appeal\\n\"\n",
    "             \" - Potential for discussion\\n\"\n",
    "             \"\\n\"\n",
    "             \"Expected Output Format:\\n\"\n",
    "             \" - Provide separate score for each criterion with a short comment.\\n\"\n",
    "             \" - Then accumulate them into one overall assessment on a scale from 1 to 100.\\n\"\n",
    "             \" - Write down possible improvements to the blog.\\n\"\n",
    "             \" - Focus only on the textual content of the blog, disregarding any visual or interactive elements. This means that there is no need to add points related to the addition of illustrations or interactive elements to possible improvements.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Reference Examples:\\n\"\n",
    "             \"These examples illustrate how blog engagement should be evaluated. They are for reference only and should not be evaluated in your response.\\n\"\n",
    "             \"\\n\"\n",
    "             \"---\\n\"\n",
    "             \"Example 1\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{blog_ex1}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement Score: {score_ex1}/100\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Example 2\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{blog_ex2}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement Score: {score_ex2}/100\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Now evaluate the provided blog:\\n\"\n",
    "             \"\\n\"\n",
    "             \"Referenced Blog to Evaluate:\\n\"\n",
    "             \"{blog_text}\\n\"\n",
    ")\n",
    "chain = prompt_few_shots | llm_gemini\n",
    "response = chain.invoke({\n",
    "    \"blog_text\" : blog.url_blog,\n",
    "    \"blog_ex1\" : best_blog[\"blog\"],\n",
    "    \"score_ex1\" : best_blog[\"score\"],\n",
    "    \"blog_ex2\" : worst_blog[\"blog\"],\n",
    "    \"score_ex2\" : worst_blog[\"score\"]\n",
    "})\n",
    "print(f\"Usage metadata:\\n{response.usage_metadata}\")\n",
    "print(f\"\\nContent:\\n{response.content}\")"
   ],
   "id": "d2962a1ec01216be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 403, 'output_tokens': 578, 'total_tokens': 981, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Here's an analysis of the provided blog post, focusing on its engagement level:\n",
      "\n",
      "**Criterion Scores:**\n",
      "\n",
      "*   **Readability:** 65/100 - The writing is generally understandable, but some sentences are a bit long and complex, potentially hindering flow for a general audience.\n",
      "*   **Structure:** 70/100 - The blog follows a logical structure (introduction, methodology, results, conclusion). However, better use of headings and subheadings could improve scannability.\n",
      "*   **Informativeness:** 75/100 - The blog provides a decent overview of the research paper, covering the key aspects of the study.\n",
      "*   **Attractiveness of the blog title:** 60/100 - The title is descriptive but not particularly attention-grabbing. It clearly states the topic but lacks a hook to draw in casual readers.\n",
      "*   **Clarity:** 60/100 - While the blog explains the core concepts, certain aspects (especially concerning the reward modeling process) could benefit from further clarification and simplification. Some jargon remains that could be explained better.\n",
      "*   **Audience Appeal:** 55/100 - The blog is likely to appeal to readers already familiar with language models and reward modeling. However, it may be less accessible to a broader audience due to its technical nature.\n",
      "*   **Potential for Discussion:** 40/100 - The blog presents information but doesn't explicitly invite discussion or pose questions to the reader. This limits its potential to spark conversation.\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "**Engagement Score: 57.9/100**\n",
      "\n",
      "**Possible Improvements:**\n",
      "\n",
      "*   **Simplify Language:** Reduce jargon and technical terms. When technical terms are necessary, provide clear and concise definitions or analogies.\n",
      "*   **Enhance Structure:** Use more headings and subheadings to break up the text and improve readability. Consider using bullet points or numbered lists to highlight key findings.\n",
      "*   **Increase Clarity:** Elaborate on complex concepts, especially reward modeling. Use examples or analogies to make them more understandable. Visual aids could also be considered, though as stated earlier, this evaluation concerns only textual content.\n",
      "*   **Strengthen Audience Appeal:** Frame the research in a broader context, highlighting its potential impact or relevance to everyday life. Consider adding a section explaining why this research matters.\n",
      "*   **Promote Discussion:** End the blog with open-ended questions to encourage readers to share their thoughts and opinions. Ask about their experiences or perspectives on the topic.\n",
      "*   **Create a more attractive title**: A more concise and intriguing title could attract a broader audience.\n",
      "*   **Add a takeaway section**: A short section highlighting the main conclusions of the paper can help to improve the audience appeal.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:39:59.025452Z",
     "start_time": "2025-03-04T12:39:59.017149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def extract_llm_score(blog_info):\n",
    "    \"\"\"Extract model assessment of the blog from formated output\"\"\"\n",
    "    try:\n",
    "        llm_response = chain.invoke({\"blog_text\" : blog_info.url_blog,\n",
    "                                     \"blog_ex1\" : best_blog[\"blog\"],\n",
    "                                     \"score_ex1\" : best_blog[\"score\"],\n",
    "                                     \"blog_ex2\" : worst_blog[\"blog\"],\n",
    "                                     \"score_ex2\" : worst_blog[\"score\"]})\n",
    "        content = llm_response.content\n",
    "\n",
    "        # Pattern \"Overall Assessment: {score}/100\"\n",
    "        match = re.search(r\"Overall Assessment:\\s*(\\d+(\\.\\d*)?)\\/100\", content)\n",
    "        if match:\n",
    "            print(f\"----------\\n\"\n",
    "                  f\"Blog ID: {blog_info.id}\\n\"\n",
    "                  f\"Blog title: {blog_info.title_blog}\\n\"\n",
    "                  f\"Referenced paper title: {blog_info.title_paper}\\n\"\n",
    "                  f\"LLM Assessment: {float(match.group(1))}/100\\n\")\n",
    "            return float(match.group(1))\n",
    "        else:\n",
    "            print(f\"----------\\n\"\n",
    "                  f\"Blog ID: {blog_info.id}: (Error) Bad output format.\\n\"\n",
    "                  f\"LLM Response:\\n{content}\\n\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing blog - {blog_info.title_blog}:\\n{e}\")"
   ],
   "id": "da5079b87b06741b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:42:05.781512Z",
     "start_time": "2025-03-04T12:39:59.068189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concrete the output format\n",
    "prompt_few_shots = PromptTemplate(\n",
    "    input_variables=[\"blog_text\",\n",
    "                     \"blog_ex1\", \"blog_ex2\",\n",
    "                     \"score_ex1\", \"score_ex2\"],\n",
    "    template=\"You are an expert in evaluating written content, specializing in assessing how well blogs communicate scientific research to a broader audience.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Task:\\n\"\n",
    "             \"Analyze the engagement level of the blog below, which references the provided scientific publication, on a scale from 1 to 100 based on the following criteria:\\n\"\n",
    "             \" - Readability\\n\"\n",
    "             \" - Structure\\n\"\n",
    "             \" - Informativeness\\n\"\n",
    "             \" - Attractiveness of the blog title\\n\"\n",
    "             \" - Clarity\\n\"\n",
    "             \" - Audience appeal\\n\"\n",
    "             \" - Potential for discussion\\n\"\n",
    "             \"\\n\"\n",
    "             \"Expected Output Format:\\n\"\n",
    "             \" - Provide separate score for each criterion with a short comment.\\n\"\n",
    "             \" - Then accumulate them into one overall assessment on a scale from 1 to 100. The overall assessment should be printed on a separate line in the following format:\\n \"\n",
    "             \"   Overall Assessment: X.X/100\\n\"\n",
    "             \" - Write down possible improvements to the blog.\\n\"\n",
    "             \" - Focus only on the textual content of the blog, disregarding any visual or interactive elements. This means that there is no need to add points related to the addition of illustrations or interactive elements to possible improvements.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Reference Examples:\\n\"\n",
    "             \"These examples illustrate how blog engagement should be evaluated. They are for reference only and should not be evaluated in your response.\\n\"\n",
    "             \"\\n\"\n",
    "             \"---\\n\"\n",
    "             \"Example 1\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{blog_ex1}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement Score: {score_ex1}/100\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Example 2\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{blog_ex2}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement Score: {score_ex2}/100\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Now evaluate the provided blog:\\n\"\n",
    "             \"\\n\"\n",
    "             \"Referenced Blog to Evaluate:\\n\"\n",
    "             \"{blog_text}\\n\"\n",
    ")\n",
    "chain = prompt_few_shots | llm_gemini\n",
    "llm_assessment = Xval.apply(extract_llm_score, axis=1)"
   ],
   "id": "b618f5cd5e7232a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: 77.9/100\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: 73.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: 68.6/100\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: 64.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: 62.9/100\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: 33.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: 73.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: 65.7/100\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: 6.6/100\n",
      "\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: 64.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: 61.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: 71.4/100\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: 64.3/100\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: 70.7/100\n",
      "\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: 73.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: 64.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: 71.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: 69.3/100\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: 82.1/100\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: 79.3/100\n",
      "\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: 67.8/100\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: 57.9/100\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: 71.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: 69.3/100\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: 71.0/100\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: 67.1/100\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: 75.7/100\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: 77.9/100\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: 77.9/100\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: 66.0/100\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T12:42:05.855453Z",
     "start_time": "2025-03-04T12:42:05.847382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "RMSE_val = metrics.root_mean_squared_error(yval, llm_assessment)\n",
    "print(f\"Root Mean Square Error on validation set: {RMSE_val:.1f}\")\n",
    "MAE_val = metrics.mean_absolute_error(yval, llm_assessment)\n",
    "print(f\"Mean Absolute Error on validation set: {MAE_val:.1f}\")"
   ],
   "id": "e9044640f219f9e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on validation set: 38.0\n",
      "Mean Absolute Error on validation set: 31.8\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T13:01:08.233811Z",
     "start_time": "2025-03-04T13:01:08.227090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "llm_assessment_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "scaled_llm_assessment = llm_assessment_scaler.fit_transform(llm_assessment.values.reshape(-1, 1))\n",
    "RMSE_val_scaled = metrics.root_mean_squared_error(yval, scaled_llm_assessment)\n",
    "print(f\"Root Mean Square Error on validation set after MinMax normalization of LLM output: {RMSE_val_scaled:.1f}\")\n",
    "MAE_val_scaled = metrics.mean_absolute_error(yval, scaled_llm_assessment)\n",
    "print(f\"Mean Absolute Error on validation set after MinMax normalization of LLM output: {MAE_val_scaled:.1f}\")"
   ],
   "id": "31b95d26c369dc8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on validation set after MinMax normalization of LLM output: 47.0\n",
      "Mean Absolute Error on validation set after MinMax normalization of LLM output: 40.0\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T13:56:14.791310Z",
     "start_time": "2025-03-04T13:56:14.573712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(data[\"normalized_engagement_score\"], bins=15, kde=True, color=\"blue\")\n",
    "plt.xlabel(\"Engagement Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Engagement Scores\")\n",
    "plt.show()"
   ],
   "id": "a41880b721e0a303",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:43:56.828641Z",
     "start_time": "2025-03-04T17:43:56.801512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classify_blog(score):\n",
    "    \"\"\"Return blog classification based on engagement score.\"\"\"\n",
    "    if score >= 80:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 60:\n",
    "        return \"Very Good\"\n",
    "    elif score >= 40:\n",
    "        return \"Good\"\n",
    "    elif score >= 20:\n",
    "        return \"Average\"\n",
    "    else:\n",
    "        return \"Bad\"\n",
    "\n",
    "data[\"engagement_level\"] = data[\"normalized_engagement_score\"].apply(classify_blog)\n",
    "data"
   ],
   "id": "c390682c7ada6c35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    id  \\\n",
       "0    1   \n",
       "1    2   \n",
       "2    3   \n",
       "3    4   \n",
       "4    5   \n",
       "5    6   \n",
       "6    7   \n",
       "7    8   \n",
       "8    9   \n",
       "9   10   \n",
       "10  11   \n",
       "11  12   \n",
       "12  13   \n",
       "13  14   \n",
       "14  15   \n",
       "15  16   \n",
       "16  17   \n",
       "17  18   \n",
       "18  19   \n",
       "19  20   \n",
       "20  21   \n",
       "21  22   \n",
       "22  23   \n",
       "23  24   \n",
       "24  25   \n",
       "25  26   \n",
       "26  27   \n",
       "27  28   \n",
       "28  29   \n",
       "29  30   \n",
       "30  31   \n",
       "31  32   \n",
       "32  33   \n",
       "33  34   \n",
       "34  35   \n",
       "35  36   \n",
       "36  37   \n",
       "37  38   \n",
       "38  39   \n",
       "39  40   \n",
       "40  41   \n",
       "41  42   \n",
       "42  43   \n",
       "43  44   \n",
       "44  45   \n",
       "45  46   \n",
       "46  47   \n",
       "47  48   \n",
       "48  49   \n",
       "49  50   \n",
       "\n",
       "                                                                                              title_blog  \\\n",
       "0                                                      Training Large Language Models: From TRPO to GRPO   \n",
       "1                                                      Training Large Language Models: From TRPO to GRPO   \n",
       "2                                                      Training Large Language Models: From TRPO to GRPO   \n",
       "3                                                      Training Large Language Models: From TRPO to GRPO   \n",
       "4                                                                What is the System 2 LLM or AI Chatbot?   \n",
       "5                                                                What is the System 2 LLM or AI Chatbot?   \n",
       "6                                                                What is the System 2 LLM or AI Chatbot?   \n",
       "7                                                                What is the System 2 LLM or AI Chatbot?   \n",
       "8                                                                What is the System 2 LLM or AI Chatbot?   \n",
       "9                                                                What is the System 2 LLM or AI Chatbot?   \n",
       "10                                                               What is the System 2 LLM or AI Chatbot?   \n",
       "11                                                               What is the System 2 LLM or AI Chatbot?   \n",
       "12                            Reinforcement Learning for tuning language models ( how to train ChatGPT )   \n",
       "13                            Reinforcement Learning for tuning language models ( how to train ChatGPT )   \n",
       "14                            Reinforcement Learning for tuning language models ( how to train ChatGPT )   \n",
       "15                            Reinforcement Learning for tuning language models ( how to train ChatGPT )   \n",
       "16                      Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review   \n",
       "17                       SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents   \n",
       "18                              Recurrent drafter for fast speculative decoding in Large Language Models   \n",
       "19                                              ChatGPT vs Bing … and the urgent need for Responsible AI   \n",
       "20                                                         Reflections on Innateness in Machine Learning   \n",
       "21                                                         Reflections on Innateness in Machine Learning   \n",
       "22                                                         Reflections on Innateness in Machine Learning   \n",
       "23                                                                           Evaluating The AI Scientist   \n",
       "24                                              Deep Reinforcement Learning: The Algorithms for Game Dev   \n",
       "25                                              Deep Reinforcement Learning: The Algorithms for Game Dev   \n",
       "26                                              Deep Reinforcement Learning: The Algorithms for Game Dev   \n",
       "27                                                                                     Towards Reasoning   \n",
       "28                                                                                     Towards Reasoning   \n",
       "29                                                    How reinforcement learning affects human behavior?   \n",
       "30                                                    How reinforcement learning affects human behavior?   \n",
       "31                                                    How reinforcement learning affects human behavior?   \n",
       "32                                                                     Do I need to be polite to my LLM?   \n",
       "33                                                                     Do I need to be polite to my LLM?   \n",
       "34  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "35  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "36  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "37  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "38  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "39  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "40  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "41  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "42  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "43  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "44  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "45  The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "46                                                                    How AI Is Changing the Way We Code   \n",
       "47                                                                               Data Centric AI — LLAVA   \n",
       "48                                     SimpleQA: Measuring short-form factuality in large language model   \n",
       "49                                      Building with Blocks: Modular RAG for Flexible and Powerful LLMs   \n",
       "\n",
       "                                                                                                                                           url_blog  \\\n",
       "0                                          https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194   \n",
       "1                                          https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194   \n",
       "2                                          https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194   \n",
       "3                                          https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194   \n",
       "4                                                                               https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "5                                                                               https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "6                                                                               https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "7                                                                               https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "8                                                                               https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "9                                                                               https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "10                                                                              https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "11                                                                              https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287   \n",
       "12                           https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302   \n",
       "13                           https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302   \n",
       "14                           https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302   \n",
       "15                           https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302   \n",
       "16                    https://medium.com/@sulbha.jindal/self-generated-critiques-boost-reward-modeling-for-languagemodels-paper-review-f013b99b7d47   \n",
       "17       https://medium.com/@sulbha.jindal/smoa-improving-multi-agent-large-language-models-with-sparse-mixture-of-agents-paper-review-22adc69c15ca   \n",
       "18             https://medium.com/@sulbha.jindal/recurrent-drafter-for-fast-speculative-decoding-in-large-language-models-paper-review-d77cfc09cae4   \n",
       "19                                                                          https://medium.com/@dattaraj/testing-large-language-models-1422d622e0c5   \n",
       "20                                                       https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af   \n",
       "21                                                       https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af   \n",
       "22                                                       https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af   \n",
       "23                                                                       https://medium.com/@nimritakoul01/evaluating-the-ai-scientist-63e419e575b8   \n",
       "24                                                                           https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7   \n",
       "25                                                                           https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7   \n",
       "26                                                                           https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7   \n",
       "27                                                                            https://medium.com/@saptarshichaudhuri/towards-reasoning-91982d2a9f74   \n",
       "28                                                                            https://medium.com/@saptarshichaudhuri/towards-reasoning-91982d2a9f74   \n",
       "29                                                      https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93   \n",
       "30                                                      https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93   \n",
       "31                                                      https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93   \n",
       "32                                                                      https://medium.com/@nathanbos/do-i-have-to-be-polite-to-my-llm-326b869a7230   \n",
       "33                                                                      https://medium.com/@nathanbos/do-i-have-to-be-polite-to-my-llm-326b869a7230   \n",
       "34  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "35  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "36  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "37  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "38  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "39  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "40  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "41  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "42  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "43  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "44  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "45  https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1   \n",
       "46                                                          https://medium.com/towards-data-science/how-ai-is-changing-the-way-we-code-36ff30262e65   \n",
       "47                                                                       https://medium.com/@amansinghalml_33304/data-centric-ai-llava-0fd3c51dd359   \n",
       "48                     https://medium.com/@sulbha.jindal/simpleqa-measuring-short-form-factuality-in-large-language-model-paper-review-9c7e4a4628a3   \n",
       "49                                        https://medium.com/@yash9439/building-with-blocks-modular-rag-for-flexible-and-powerful-llms-3dba3ab1a7ce   \n",
       "\n",
       "             author_blog  author_followers  claps  comments  \\\n",
       "0            Maxime Wolf               349    177         1   \n",
       "1            Maxime Wolf               349    177         1   \n",
       "2            Maxime Wolf               349    177         1   \n",
       "3            Maxime Wolf               349    177         1   \n",
       "4                Don Lim               413     17         0   \n",
       "5                Don Lim               413     17         0   \n",
       "6                Don Lim               413     17         0   \n",
       "7                Don Lim               413     17         0   \n",
       "8                Don Lim               413     17         0   \n",
       "9                Don Lim               413     17         0   \n",
       "10               Don Lim               413     17         0   \n",
       "11               Don Lim               413     17         0   \n",
       "12            ML Blogger               649     93         1   \n",
       "13            ML Blogger               649     93         1   \n",
       "14            ML Blogger               649     93         1   \n",
       "15            ML Blogger               649     93         1   \n",
       "16           Sulbha Jain                41     21         0   \n",
       "17           Sulbha Jain                41      0         0   \n",
       "18           Sulbha Jain                41      1         0   \n",
       "19          Dattaraj Rao                11      7         0   \n",
       "20  Thomas G. Dietterich               875    295         2   \n",
       "21  Thomas G. Dietterich               875    295         2   \n",
       "22  Thomas G. Dietterich               875    295         2   \n",
       "23      Dr. Nimrita Koul               428      0         0   \n",
       "24             TentuPlay               117     56         1   \n",
       "25             TentuPlay               117     56         1   \n",
       "26             TentuPlay               117     56         1   \n",
       "27   Saptarshi Chaudhuri               127    461         1   \n",
       "28   Saptarshi Chaudhuri               127    461         1   \n",
       "29             Eve Pardi                81     81         1   \n",
       "30             Eve Pardi                81     81         1   \n",
       "31             Eve Pardi                81     81         1   \n",
       "32     Nathan Bos, Ph.D.               257     13         3   \n",
       "33     Nathan Bos, Ph.D.               257     13         3   \n",
       "34           Sandi Besen               609    366         6   \n",
       "35           Sandi Besen               609    366         6   \n",
       "36           Sandi Besen               609    366         6   \n",
       "37           Sandi Besen               609    366         6   \n",
       "38           Sandi Besen               609    366         6   \n",
       "39           Sandi Besen               609    366         6   \n",
       "40           Sandi Besen               609    366         6   \n",
       "41           Sandi Besen               609    366         6   \n",
       "42           Sandi Besen               609    366         6   \n",
       "43           Sandi Besen               609    366         6   \n",
       "44           Sandi Besen               609    366         6   \n",
       "45           Sandi Besen               609    366         6   \n",
       "46   Quentin Gallea, PhD               820    423        12   \n",
       "47         Amansinghalml                12      7         0   \n",
       "48           Sulbha Jain                41      0         0   \n",
       "49          Yash Bhaskar               952      2         0   \n",
       "\n",
       "                                                                                                   title_paper  \\\n",
       "0                                                                         Foundations of Large Language Models   \n",
       "1                           DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning   \n",
       "2                           DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models   \n",
       "3                                                                             Trust Region Policy Optimization   \n",
       "4                                                        Guiding Language Model Reasoning with Planning Tokens   \n",
       "5                                                                            Distilling System 2 into System 1   \n",
       "6                                        Chain-of-Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "7                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "8                                   Branch-Solve-Merge Improves Large Language Model Evaluation and Generation   \n",
       "9                                                         System 2 Attention (is something you might need too)   \n",
       "10                         Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves   \n",
       "11                                                                                   Let's Verify Step by Step   \n",
       "12                                                          Fine-Tuning Language Models from Human Preferences   \n",
       "13                                      RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning   \n",
       "14                   Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback   \n",
       "15                                             WebGPT: Browser-assisted question-answering with human feedback   \n",
       "16                                          Self-Generated Critiques Boost Reward Modeling for Language Models   \n",
       "17                             SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents   \n",
       "18                                    Recurrent drafter for fast speculative decoding in Large Language Models   \n",
       "19                                                       Adaptive Test Generation Using a Large Language Model   \n",
       "20                                                          Innateness, AlphaZero, and Artificial Intelligence   \n",
       "21                                                                    Implicit Regularization in Deep Learning   \n",
       "22                                                     The Implicit Bias of Gradient Descent on Separable Data   \n",
       "23                                   The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery   \n",
       "24                                                                            Reinforcement Learning: A Survey   \n",
       "25                                                              Playing Atari with Deep Reinforcement Learning   \n",
       "26                                                                     Proximal Policy Optimization Algorithms   \n",
       "27              GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models   \n",
       "28                                                                Chain-of-Thought Reasoning Without Prompting   \n",
       "29                                                         Artificial Intelligence for the Metaverse: A Survey   \n",
       "30                                                                 Acquisition of Chess Knowledge in AlphaZero   \n",
       "31                                              A Deep Hierarchical Approach to Lifelong Learning in Minecraft   \n",
       "32      Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance   \n",
       "33                              Do Llamas Work in English? On the Latent Language of Multilingual Transformers   \n",
       "34        The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey   \n",
       "35                                        AutoGPT+P: Affordance-based Task Planning with Large Language Models   \n",
       "36                                                  ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "37  From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models   \n",
       "38                                               Reflexion: Language Agents with Verbal Reinforcement Learning   \n",
       "39                                                Learning to Use Tools via Cooperative and Interactive Agents   \n",
       "40                                                      Efficient Tool Use with Chain-of-Abstraction Reasoning   \n",
       "41                         AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors   \n",
       "42                                                   Embodied LLM Agents Learn to Cooperate in Organized Teams   \n",
       "43                                   A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration   \n",
       "44                                                              How Language Model Hallucinations Can Snowball   \n",
       "45                               Large Language Model-based Human-Agent Collaboration for Complex Task Solving   \n",
       "46     From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow   \n",
       "47                                                                                   Visual Instruction Tuning   \n",
       "48                                                    Measuring short-form factuality in large language models   \n",
       "49                              Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks   \n",
       "\n",
       "                                                                                                    url_paper  \\\n",
       "0                                                                            https://arxiv.org/pdf/2501.09223   \n",
       "1                                                                            https://arxiv.org/pdf/2501.12948   \n",
       "2                                                                            https://arxiv.org/pdf/2402.03300   \n",
       "3                                                                            https://arxiv.org/pdf/1502.05477   \n",
       "4                                                                            https://arxiv.org/pdf/2310.05707   \n",
       "5                                                                            https://arxiv.org/pdf/2407.06023   \n",
       "6                                                                            https://arxiv.org/pdf/2201.11903   \n",
       "7                                                                            https://arxiv.org/pdf/2305.10601   \n",
       "8                                                                            https://arxiv.org/pdf/2310.15123   \n",
       "9                                                                            https://arxiv.org/pdf/2311.11829   \n",
       "10                                                                           https://arxiv.org/pdf/2311.04205   \n",
       "11                                                                           https://arxiv.org/pdf/2305.20050   \n",
       "12                                                                           https://arxiv.org/pdf/1909.08593   \n",
       "13                                                                           https://arxiv.org/pdf/2205.12548   \n",
       "14                                                                           https://arxiv.org/pdf/2204.05862   \n",
       "15                                                                           https://arxiv.org/pdf/2112.09332   \n",
       "16                                                                           https://arxiv.org/pdf/2411.16646   \n",
       "17                                                                           https://arxiv.org/pdf/2411.03284   \n",
       "18                                                                           https://arxiv.org/pdf/2403.09919   \n",
       "19                                                                         https://arxiv.org/pdf/2302.06527v2   \n",
       "20                                                                           https://arxiv.org/pdf/1801.05667   \n",
       "21                                                                           https://arxiv.org/pdf/1709.01953   \n",
       "22                                                                           https://arxiv.org/pdf/1710.10345   \n",
       "23                                                                           https://arxiv.org/pdf/2408.06292   \n",
       "24                                                                           https://arxiv.org/pdf/cs/9605103   \n",
       "25                                                                            https://arxiv.org/pdf/1312.5602   \n",
       "26                                                                           https://arxiv.org/pdf/1707.06347   \n",
       "27  https://arxiv.org/pdf/2410.05229?source=post_page-----91982d2a9f74---------------------------------------   \n",
       "28                                                                           https://arxiv.org/pdf/2402.10200   \n",
       "29                                                                           https://arxiv.org/pdf/2202.10336   \n",
       "30                                                                           https://arxiv.org/pdf/2111.09259   \n",
       "31                                                                           https://arxiv.org/pdf/1604.07255   \n",
       "32                                                                           https://arxiv.org/pdf/2402.14531   \n",
       "33                                                                           https://arxiv.org/pdf/2402.10588   \n",
       "34                                                                           https://arxiv.org/pdf/2404.11584   \n",
       "35                                                                           https://arxiv.org/pdf/2402.10778   \n",
       "36                                                                           https://arxiv.org/pdf/2210.03629   \n",
       "37                                                                           https://arxiv.org/pdf/2401.02777   \n",
       "38                                                                           https://arxiv.org/pdf/2303.11366   \n",
       "39                                                                           https://arxiv.org/pdf/2403.03031   \n",
       "40                                                                           https://arxiv.org/pdf/2401.17464   \n",
       "41                                                                           https://arxiv.org/pdf/2308.10848   \n",
       "42                                                                           https://arxiv.org/pdf/2403.12482   \n",
       "43                                                                           https://arxiv.org/pdf/2310.02170   \n",
       "44                                                                           https://arxiv.org/pdf/2305.13534   \n",
       "45                                                                           https://arxiv.org/pdf/2402.12914   \n",
       "46                                                                           https://arxiv.org/pdf/2308.11302   \n",
       "47                                                                           https://arxiv.org/pdf/2304.08485   \n",
       "48                                                                           https://arxiv.org/pdf/2411.04368   \n",
       "49                                                                           https://arxiv.org/pdf/2407.21059   \n",
       "\n",
       "    engagement_score  normalized_engagement_score engagement_level  \n",
       "0           0.515759                         50.0             Good  \n",
       "1           0.515759                         50.0             Good  \n",
       "2           0.515759                         50.0             Good  \n",
       "3           0.515759                         50.0             Good  \n",
       "4           0.041162                          4.0              Bad  \n",
       "5           0.041162                          4.0              Bad  \n",
       "6           0.041162                          4.0              Bad  \n",
       "7           0.041162                          4.0              Bad  \n",
       "8           0.041162                          4.0              Bad  \n",
       "9           0.041162                          4.0              Bad  \n",
       "10          0.041162                          4.0              Bad  \n",
       "11          0.041162                          4.0              Bad  \n",
       "12          0.147920                         14.0              Bad  \n",
       "13          0.147920                         14.0              Bad  \n",
       "14          0.147920                         14.0              Bad  \n",
       "15          0.147920                         14.0              Bad  \n",
       "16          0.512195                         49.0             Good  \n",
       "17          0.000000                          1.0              Bad  \n",
       "18          0.024390                          2.0              Bad  \n",
       "19          0.636364                         61.0        Very Good  \n",
       "20          0.344000                         33.0          Average  \n",
       "21          0.344000                         33.0          Average  \n",
       "22          0.344000                         33.0          Average  \n",
       "23          0.000000                          1.0              Bad  \n",
       "24          0.504274                         49.0             Good  \n",
       "25          0.504274                         49.0             Good  \n",
       "26          0.504274                         49.0             Good  \n",
       "27          3.653543                        100.0        Excellent  \n",
       "28          3.653543                        100.0        Excellent  \n",
       "29          1.037037                        100.0        Excellent  \n",
       "30          1.037037                        100.0        Excellent  \n",
       "31          1.037037                        100.0        Excellent  \n",
       "32          0.085603                          8.0              Bad  \n",
       "33          0.085603                          8.0              Bad  \n",
       "34          0.630542                         61.0        Very Good  \n",
       "35          0.630542                         61.0        Very Good  \n",
       "36          0.630542                         61.0        Very Good  \n",
       "37          0.630542                         61.0        Very Good  \n",
       "38          0.630542                         61.0        Very Good  \n",
       "39          0.630542                         61.0        Very Good  \n",
       "40          0.630542                         61.0        Very Good  \n",
       "41          0.630542                         61.0        Very Good  \n",
       "42          0.630542                         61.0        Very Good  \n",
       "43          0.630542                         61.0        Very Good  \n",
       "44          0.630542                         61.0        Very Good  \n",
       "45          0.630542                         61.0        Very Good  \n",
       "46          0.559756                         54.0             Good  \n",
       "47          0.583333                         56.0             Good  \n",
       "48          0.000000                          1.0              Bad  \n",
       "49          0.002101                          1.0              Bad  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_blog</th>\n",
       "      <th>url_blog</th>\n",
       "      <th>author_blog</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>claps</th>\n",
       "      <th>comments</th>\n",
       "      <th>title_paper</th>\n",
       "      <th>url_paper</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>normalized_engagement_score</th>\n",
       "      <th>engagement_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Training Large Language Models: From TRPO to GRPO</td>\n",
       "      <td>https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194</td>\n",
       "      <td>Maxime Wolf</td>\n",
       "      <td>349</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>Foundations of Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2501.09223</td>\n",
       "      <td>0.515759</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Training Large Language Models: From TRPO to GRPO</td>\n",
       "      <td>https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194</td>\n",
       "      <td>Maxime Wolf</td>\n",
       "      <td>349</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</td>\n",
       "      <td>https://arxiv.org/pdf/2501.12948</td>\n",
       "      <td>0.515759</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Training Large Language Models: From TRPO to GRPO</td>\n",
       "      <td>https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194</td>\n",
       "      <td>Maxime Wolf</td>\n",
       "      <td>349</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2402.03300</td>\n",
       "      <td>0.515759</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Training Large Language Models: From TRPO to GRPO</td>\n",
       "      <td>https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194</td>\n",
       "      <td>Maxime Wolf</td>\n",
       "      <td>349</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>Trust Region Policy Optimization</td>\n",
       "      <td>https://arxiv.org/pdf/1502.05477</td>\n",
       "      <td>0.515759</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Guiding Language Model Reasoning with Planning Tokens</td>\n",
       "      <td>https://arxiv.org/pdf/2310.05707</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Distilling System 2 into System 1</td>\n",
       "      <td>https://arxiv.org/pdf/2407.06023</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2201.11903</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2305.10601</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Branch-Solve-Merge Improves Large Language Model Evaluation and Generation</td>\n",
       "      <td>https://arxiv.org/pdf/2310.15123</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>System 2 Attention (is something you might need too)</td>\n",
       "      <td>https://arxiv.org/pdf/2311.11829</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</td>\n",
       "      <td>https://arxiv.org/pdf/2311.04205</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>What is the System 2 LLM or AI Chatbot?</td>\n",
       "      <td>https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287</td>\n",
       "      <td>Don Lim</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>https://arxiv.org/pdf/2305.20050</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Reinforcement Learning for tuning language models ( how to train ChatGPT )</td>\n",
       "      <td>https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302</td>\n",
       "      <td>ML Blogger</td>\n",
       "      <td>649</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>Fine-Tuning Language Models from Human Preferences</td>\n",
       "      <td>https://arxiv.org/pdf/1909.08593</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Reinforcement Learning for tuning language models ( how to train ChatGPT )</td>\n",
       "      <td>https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302</td>\n",
       "      <td>ML Blogger</td>\n",
       "      <td>649</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning</td>\n",
       "      <td>https://arxiv.org/pdf/2205.12548</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Reinforcement Learning for tuning language models ( how to train ChatGPT )</td>\n",
       "      <td>https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302</td>\n",
       "      <td>ML Blogger</td>\n",
       "      <td>649</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</td>\n",
       "      <td>https://arxiv.org/pdf/2204.05862</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Reinforcement Learning for tuning language models ( how to train ChatGPT )</td>\n",
       "      <td>https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302</td>\n",
       "      <td>ML Blogger</td>\n",
       "      <td>649</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>WebGPT: Browser-assisted question-answering with human feedback</td>\n",
       "      <td>https://arxiv.org/pdf/2112.09332</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review</td>\n",
       "      <td>https://medium.com/@sulbha.jindal/self-generated-critiques-boost-reward-modeling-for-languagemodels-paper-review-f013b99b7d47</td>\n",
       "      <td>Sulbha Jain</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>Self-Generated Critiques Boost Reward Modeling for Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2411.16646</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents</td>\n",
       "      <td>https://medium.com/@sulbha.jindal/smoa-improving-multi-agent-large-language-models-with-sparse-mixture-of-agents-paper-review-22adc69c15ca</td>\n",
       "      <td>Sulbha Jain</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents</td>\n",
       "      <td>https://arxiv.org/pdf/2411.03284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Recurrent drafter for fast speculative decoding in Large Language Models</td>\n",
       "      <td>https://medium.com/@sulbha.jindal/recurrent-drafter-for-fast-speculative-decoding-in-large-language-models-paper-review-d77cfc09cae4</td>\n",
       "      <td>Sulbha Jain</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Recurrent drafter for fast speculative decoding in Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2403.09919</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>ChatGPT vs Bing … and the urgent need for Responsible AI</td>\n",
       "      <td>https://medium.com/@dattaraj/testing-large-language-models-1422d622e0c5</td>\n",
       "      <td>Dattaraj Rao</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Adaptive Test Generation Using a Large Language Model</td>\n",
       "      <td>https://arxiv.org/pdf/2302.06527v2</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Reflections on Innateness in Machine Learning</td>\n",
       "      <td>https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af</td>\n",
       "      <td>Thomas G. Dietterich</td>\n",
       "      <td>875</td>\n",
       "      <td>295</td>\n",
       "      <td>2</td>\n",
       "      <td>Innateness, AlphaZero, and Artificial Intelligence</td>\n",
       "      <td>https://arxiv.org/pdf/1801.05667</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Reflections on Innateness in Machine Learning</td>\n",
       "      <td>https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af</td>\n",
       "      <td>Thomas G. Dietterich</td>\n",
       "      <td>875</td>\n",
       "      <td>295</td>\n",
       "      <td>2</td>\n",
       "      <td>Implicit Regularization in Deep Learning</td>\n",
       "      <td>https://arxiv.org/pdf/1709.01953</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Reflections on Innateness in Machine Learning</td>\n",
       "      <td>https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af</td>\n",
       "      <td>Thomas G. Dietterich</td>\n",
       "      <td>875</td>\n",
       "      <td>295</td>\n",
       "      <td>2</td>\n",
       "      <td>The Implicit Bias of Gradient Descent on Separable Data</td>\n",
       "      <td>https://arxiv.org/pdf/1710.10345</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Evaluating The AI Scientist</td>\n",
       "      <td>https://medium.com/@nimritakoul01/evaluating-the-ai-scientist-63e419e575b8</td>\n",
       "      <td>Dr. Nimrita Koul</td>\n",
       "      <td>428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>\n",
       "      <td>https://arxiv.org/pdf/2408.06292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Deep Reinforcement Learning: The Algorithms for Game Dev</td>\n",
       "      <td>https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7</td>\n",
       "      <td>TentuPlay</td>\n",
       "      <td>117</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>Reinforcement Learning: A Survey</td>\n",
       "      <td>https://arxiv.org/pdf/cs/9605103</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Deep Reinforcement Learning: The Algorithms for Game Dev</td>\n",
       "      <td>https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7</td>\n",
       "      <td>TentuPlay</td>\n",
       "      <td>117</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>Playing Atari with Deep Reinforcement Learning</td>\n",
       "      <td>https://arxiv.org/pdf/1312.5602</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Deep Reinforcement Learning: The Algorithms for Game Dev</td>\n",
       "      <td>https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7</td>\n",
       "      <td>TentuPlay</td>\n",
       "      <td>117</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>Proximal Policy Optimization Algorithms</td>\n",
       "      <td>https://arxiv.org/pdf/1707.06347</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Towards Reasoning</td>\n",
       "      <td>https://medium.com/@saptarshichaudhuri/towards-reasoning-91982d2a9f74</td>\n",
       "      <td>Saptarshi Chaudhuri</td>\n",
       "      <td>127</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2410.05229?source=post_page-----91982d2a9f74---------------------------------------</td>\n",
       "      <td>3.653543</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Towards Reasoning</td>\n",
       "      <td>https://medium.com/@saptarshichaudhuri/towards-reasoning-91982d2a9f74</td>\n",
       "      <td>Saptarshi Chaudhuri</td>\n",
       "      <td>127</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>Chain-of-Thought Reasoning Without Prompting</td>\n",
       "      <td>https://arxiv.org/pdf/2402.10200</td>\n",
       "      <td>3.653543</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>How reinforcement learning affects human behavior?</td>\n",
       "      <td>https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93</td>\n",
       "      <td>Eve Pardi</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>Artificial Intelligence for the Metaverse: A Survey</td>\n",
       "      <td>https://arxiv.org/pdf/2202.10336</td>\n",
       "      <td>1.037037</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>How reinforcement learning affects human behavior?</td>\n",
       "      <td>https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93</td>\n",
       "      <td>Eve Pardi</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>Acquisition of Chess Knowledge in AlphaZero</td>\n",
       "      <td>https://arxiv.org/pdf/2111.09259</td>\n",
       "      <td>1.037037</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>How reinforcement learning affects human behavior?</td>\n",
       "      <td>https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93</td>\n",
       "      <td>Eve Pardi</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>A Deep Hierarchical Approach to Lifelong Learning in Minecraft</td>\n",
       "      <td>https://arxiv.org/pdf/1604.07255</td>\n",
       "      <td>1.037037</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Do I need to be polite to my LLM?</td>\n",
       "      <td>https://medium.com/@nathanbos/do-i-have-to-be-polite-to-my-llm-326b869a7230</td>\n",
       "      <td>Nathan Bos, Ph.D.</td>\n",
       "      <td>257</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance</td>\n",
       "      <td>https://arxiv.org/pdf/2402.14531</td>\n",
       "      <td>0.085603</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Do I need to be polite to my LLM?</td>\n",
       "      <td>https://medium.com/@nathanbos/do-i-have-to-be-polite-to-my-llm-326b869a7230</td>\n",
       "      <td>Nathan Bos, Ph.D.</td>\n",
       "      <td>257</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>Do Llamas Work in English? On the Latent Language of Multilingual Transformers</td>\n",
       "      <td>https://arxiv.org/pdf/2402.10588</td>\n",
       "      <td>0.085603</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://arxiv.org/pdf/2404.11584</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>AutoGPT+P: Affordance-based Task Planning with Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2402.10778</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2210.03629</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models</td>\n",
       "      <td>https://arxiv.org/pdf/2401.02777</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>\n",
       "      <td>https://arxiv.org/pdf/2303.11366</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>Learning to Use Tools via Cooperative and Interactive Agents</td>\n",
       "      <td>https://arxiv.org/pdf/2403.03031</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>Efficient Tool Use with Chain-of-Abstraction Reasoning</td>\n",
       "      <td>https://arxiv.org/pdf/2401.17464</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors</td>\n",
       "      <td>https://arxiv.org/pdf/2308.10848</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>Embodied LLM Agents Learn to Cooperate in Organized Teams</td>\n",
       "      <td>https://arxiv.org/pdf/2403.12482</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration</td>\n",
       "      <td>https://arxiv.org/pdf/2310.02170</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>How Language Model Hallucinations Can Snowball</td>\n",
       "      <td>https://arxiv.org/pdf/2305.13534</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</td>\n",
       "      <td>https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1</td>\n",
       "      <td>Sandi Besen</td>\n",
       "      <td>609</td>\n",
       "      <td>366</td>\n",
       "      <td>6</td>\n",
       "      <td>Large Language Model-based Human-Agent Collaboration for Complex Task Solving</td>\n",
       "      <td>https://arxiv.org/pdf/2402.12914</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>How AI Is Changing the Way We Code</td>\n",
       "      <td>https://medium.com/towards-data-science/how-ai-is-changing-the-way-we-code-36ff30262e65</td>\n",
       "      <td>Quentin Gallea, PhD</td>\n",
       "      <td>820</td>\n",
       "      <td>423</td>\n",
       "      <td>12</td>\n",
       "      <td>From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow</td>\n",
       "      <td>https://arxiv.org/pdf/2308.11302</td>\n",
       "      <td>0.559756</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>Data Centric AI — LLAVA</td>\n",
       "      <td>https://medium.com/@amansinghalml_33304/data-centric-ai-llava-0fd3c51dd359</td>\n",
       "      <td>Amansinghalml</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Visual Instruction Tuning</td>\n",
       "      <td>https://arxiv.org/pdf/2304.08485</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>SimpleQA: Measuring short-form factuality in large language model</td>\n",
       "      <td>https://medium.com/@sulbha.jindal/simpleqa-measuring-short-form-factuality-in-large-language-model-paper-review-9c7e4a4628a3</td>\n",
       "      <td>Sulbha Jain</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Measuring short-form factuality in large language models</td>\n",
       "      <td>https://arxiv.org/pdf/2411.04368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Building with Blocks: Modular RAG for Flexible and Powerful LLMs</td>\n",
       "      <td>https://medium.com/@yash9439/building-with-blocks-modular-rag-for-flexible-and-powerful-llms-3dba3ab1a7ce</td>\n",
       "      <td>Yash Bhaskar</td>\n",
       "      <td>952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks</td>\n",
       "      <td>https://arxiv.org/pdf/2407.21059</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:44:04.754140Z",
     "start_time": "2025-03-04T17:44:04.748151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the training dataset into 60% train and 40% test\n",
    "Xval, Xtest, yval, ytest = train_test_split(data.drop(columns=['engagement_level']), data[\"engagement_level\"], test_size=0.4, random_state=random_seed)"
   ],
   "id": "f46504ce27a69fde",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:44:06.171148Z",
     "start_time": "2025-03-04T17:44:06.158360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "\"\"\"\n",
    "class CriterionEvaluation(BaseModel):\n",
    "    \\\"\"\"Rating a blog based on one criterion\\\"\"\"\n",
    "    criterion: str = Field(..., description=\"Name of the criterion\")\n",
    "    classification: Literal[\"Excellent\", \"Very Good\", \"Good\", \"Average\", \"Bad\"] = Field(..., description=\"Categorical classification of the blog based on the given criterion\")\n",
    "    comment: str = Field(..., description=\"Short comment on the blog's assessment based on a given criterion\")\n",
    "\"\"\"\n",
    "\n",
    "class BlogEvaluation(BaseModel):\n",
    "    \"\"\"Comprehensive blog assessment\"\"\"\n",
    "    blog_title: str = Field(..., description=\"Blog title\")\n",
    "    # separate_assessment: List[CriterionEvaluation] = Field(..., description=\"List of assessment for each criterion\")\n",
    "    overall_assessment: Literal[\"Excellent\", \"Very Good\", \"Good\", \"Average\", \"Bad\"] = Field(..., description=\"Overall assessment of the blog\")\n",
    "    assessment_explanation: str = Field(..., description=\"Explanation of the assessment\")\n",
    "    improvements: List[str] = Field(..., description=\"Suggested improvements for the blog\")\n",
    "\n",
    "def print_evaluation(blog_evaluation: BlogEvaluation):\n",
    "    print(f\"Blog Title: {blog_evaluation.blog_title}\")\n",
    "    print(f\"Overall Assessment: {blog_evaluation.overall_assessment}\")\n",
    "    print(f\"Explanation: {blog_evaluation.assessment_explanation}\")\n",
    "    print(\"Suggested Improvements:\")\n",
    "    for i, improvement in enumerate(blog_evaluation.improvements, start=1):\n",
    "        print(f\"\\t{i}. {improvement}\")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=1, rate_limiter=rate_limiter).with_structured_output(BlogEvaluation)"
   ],
   "id": "d1dcf1c333bb0dd4",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:19:11.771948Z",
     "start_time": "2025-03-04T18:19:11.767051Z"
    }
   },
   "cell_type": "code",
   "source": "yval",
   "id": "f3a5b9eec928b8a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34    Very Good\n",
       "20      Average\n",
       "30    Excellent\n",
       "19    Very Good\n",
       "47         Good\n",
       "17          Bad\n",
       "29    Excellent\n",
       "7           Bad\n",
       "18          Bad\n",
       "6           Bad\n",
       "26         Good\n",
       "4           Bad\n",
       "23          Bad\n",
       "21      Average\n",
       "35    Very Good\n",
       "11          Bad\n",
       "2          Good\n",
       "5           Bad\n",
       "39    Very Good\n",
       "45    Very Good\n",
       "16         Good\n",
       "25         Good\n",
       "31    Excellent\n",
       "27    Excellent\n",
       "46         Good\n",
       "8           Bad\n",
       "33          Bad\n",
       "37    Very Good\n",
       "28    Excellent\n",
       "0          Good\n",
       "Name: engagement_level, dtype: object"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:33:01.108570Z",
     "start_time": "2025-03-04T18:32:21.575779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "excellent_blog_index = yval[yval == \"Excellent\"].index.min()\n",
    "excellent_blog = extract_blog_text(Xval.loc[excellent_blog_index])\n",
    "very_good_blog_index = yval[yval == \"Very Good\"].index.min()\n",
    "very_good_blog = extract_blog_text(Xval.loc[very_good_blog_index])\n",
    "good_blog_index = yval[yval == \"Good\"].index.min()\n",
    "good_blog = extract_blog_text(Xval.loc[good_blog_index])\n",
    "average_blog_index = yval[yval == \"Average\"].index.min()\n",
    "average_blog = extract_blog_text(Xval.loc[average_blog_index])\n",
    "bad_blog_index = yval[yval == \"Bad\"].index.min()\n",
    "bad_blog = extract_blog_text(Xval.loc[bad_blog_index])"
   ],
   "id": "b5c876211c4ea37c",
   "outputs": [],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:38:36.632588Z",
     "start_time": "2025-03-04T18:38:24.151955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"blog_text\", \"excellent_blog\", \"very_good_blog\", \"good_blog\", \"average_blog\", \"bad_blog\"],\n",
    "    template=\"You are a very strict expert in evaluating written content, specializing in assessing how well blogs communicate scientific research to a broader audience.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Task:\\n\"\n",
    "             \"Analyze the engagement level of the blog below based on the following criteria:\\n\"\n",
    "             \" - Readability\\n\"\n",
    "             \" - Structure\\n\"\n",
    "             \" - Informativeness\\n\"\n",
    "             \" - Attractiveness of the blog title\\n\"\n",
    "             \" - Clarity\\n\"\n",
    "             \" - Audience appeal\\n\"\n",
    "             \" - Potential for discussion\\n\"\n",
    "             \"\\n\"\n",
    "             \"Expected Output Format:\\n\"\n",
    "             \" - Summarize the overall engagement level using one of the following ratings: \\\"Excellent\\\", \\\"Very Good\\\", \\\"Good\\\", \\\"Average\\\", \\\"Bad\\\".\\n\"\n",
    "             \" - Explain why you gave this assessment.\\n\"\n",
    "             \" - Write down possible improvements to the blog.\\n\"\n",
    "             \" - Focus only on the textual content of the blog, disregarding any visual or interactive elements. This means that there is no need to add points related to the addition of illustrations or interactive elements to possible improvements.\\n\"\n",
    "             \" - Сalmly lower your blog assessment according to the number of bugs.\\n\"\n",
    "             \" - Return ONLY a valid JSON object in plain text.\\n\"\n",
    "             \"\\n\"\n",
    "             \"Reference Examples:\\n\"\n",
    "             \"Below are examples of blog evaluations, each representing a different engagement level. Use them as a reference when assessing the provided blog.\\n\"\n",
    "             \"\\n\"\n",
    "             \"---\\n\"\n",
    "             \"Example 1 (Excellent)\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{excellent_blog}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement level: Excellent\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Example 2 (Very Good)\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{very_good_blog}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement level: Very Good\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Example 3 (Good)\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{good_blog}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement level: Good\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Example 4 (Average)\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{average_blog}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement level: Average\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Example 5 (Bad)\\n\"\n",
    "             \"Blog:\\n\"\n",
    "             \"{bad_blog}\\n\"\n",
    "             \"\\n\"\n",
    "             \"Engagement level: Bad\\n\"\n",
    "             \"---\\n\"\n",
    "             \"\\n\"\n",
    "             \"Now evaluate the provided blog:\\n\"\n",
    "             \"\\n\"\n",
    "             \"Referenced Blog to Evaluate:\\n\"\n",
    "             \"{blog_text}\\n\"\n",
    ")\n",
    "chain = prompt | llm_gemini\n",
    "response = chain.invoke({\n",
    "    \"blog_text\" : extract_blog_text(blog),\n",
    "    \"excellent_blog\" : excellent_blog,\n",
    "    \"very_good_blog\" : very_good_blog,\n",
    "    \"good_blog\" : good_blog,\n",
    "    \"average_blog\" : average_blog,\n",
    "    \"bad_blog\" : bad_blog\n",
    "})\n",
    "print_evaluation(response)"
   ],
   "id": "2ee4495c9967eab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog Title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Overall Assessment: Good\n",
      "Explanation: The blog post provides a concise overview of the Critic-RM framework for enhancing reward modeling in LLMs. It effectively summarizes the paper's key contributions, including the methodology, results, and overall impact. The blog is well-structured, with clear sections and subheadings that guide the reader through the topic. However, it could benefit from a slightly more engaging title and a more detailed explanation of certain technical aspects to enhance readability and appeal to a broader audience.\n",
      "Suggested Improvements:\n",
      "\t1. Enhance the title to be more captivating and indicative of the content.\n",
      "\t2. Provide more detailed explanations of technical concepts, possibly with examples, to improve readability for readers with varying levels of expertise.\n",
      "\t3. Consider adding a section discussing the limitations of the Critic-RM framework and potential areas for future research.\n",
      "\t4. Incorporate more visuals or diagrams to illustrate the Critic-RM methodology and results.\n",
      "\t5. Add a call to action, encouraging readers to explore the linked paper or related resources.\n",
      "\t6. Include a brief discussion on the ethical implications or potential biases associated with self-generated critiques in reward modeling, promoting a more balanced perspective on the topic.\n",
      "\t7. Incorporate a real-world example or case study where Critic-RM could be applied to highlight its practical relevance and impact.\n",
      "\t8. Include a conclusion summarizing the overall findings and their implications for the field of language model optimization and alignment with human preferences.\n",
      "\t9. The blog post lacks clear and explicit explanations of complex concepts.\n",
      "\t10. The potential for discussion could be improved by including thought-provoking questions or prompts related to the ethical implications, limitations, or future directions of the research.\n",
      "\t11. The title is not sufficiently eye-catching or informative to draw in readers from a broader audience interested in machine learning or AI.\n",
      "\t12. Clarity could be improved by breaking down complex sentences and using simpler language when explaining technical details.\n",
      "\t13. The Informativeness is good, but could be enhanced by providing more contextual background or related research to fully appreciate the significance of the paper's contributions.\n",
      "\t14. The current structure lacks a concluding section that summarizes the key takeaways and implications of the research, leaving the reader without a clear sense of closure.\n",
      "\t15. The current Attractiveness of the blog title is low, as it doesn't immediately convey the relevance or potential impact of the research being reviewed.\n",
      "\t16. The current blog lacks a strong Audience appeal beyond those already familiar with reward modeling and reinforcement learning from human feedback (RLHF).\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:55:48.533529Z",
     "start_time": "2025-03-04T18:55:48.525986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_llm_classification(blog_info, max_retries=3):\n",
    "    \"\"\"Extract model assessment of the blog from formated output\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            llm_response = chain.invoke({\n",
    "                \"blog_text\" : extract_blog_text(blog_info),\n",
    "                \"excellent_blog\" : excellent_blog,\n",
    "                \"very_good_blog\" : very_good_blog,\n",
    "                \"good_blog\" : good_blog,\n",
    "                \"average_blog\" : average_blog,\n",
    "                \"bad_blog\" : bad_blog\n",
    "            })\n",
    "\n",
    "            if llm_response:\n",
    "                print(f\"----------\\n\"\n",
    "                    f\"Blog ID: {blog_info.id}\\n\"\n",
    "                    f\"Blog title: {blog_info.title_blog}\\n\"\n",
    "                    f\"Referenced paper title: {blog_info.title_paper}\\n\"\n",
    "                    f\"LLM Assessment: {llm_response.overall_assessment}\\n\")\n",
    "                return llm_response.overall_assessment\n",
    "            else:\n",
    "                print(f\"Warning: Received invalid response on attempt {attempt + 1}.\\n Retrying...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing blog \\\"{blog_info.title_blog}\\\":\\n{e}\\n Retrying...\")\n",
    "\n",
    "    print(f\"Failed to get valid assessment after {max_retries} attempts.\")\n",
    "    return None"
   ],
   "id": "c14cd1b0d453c907",
   "outputs": [],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T19:03:30.780033Z",
     "start_time": "2025-03-04T18:55:49.163290Z"
    }
   },
   "cell_type": "code",
   "source": "llm_classification = Xval.apply(extract_llm_classification, axis=1)",
   "id": "2dfd4534cbd2fd9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: Excellent\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: Excellent\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 266
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T19:04:23.919513Z",
     "start_time": "2025-03-04T19:04:23.914916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction_accuracy = metrics.accuracy_score(yval, llm_classification)\n",
    "print(f\"Accuracy score on validation set: {prediction_accuracy * 100:.2f}%\")"
   ],
   "id": "6a4be2f4c2ba6fd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on validation set: 50.00%\n"
     ]
    }
   ],
   "execution_count": 268
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T19:15:04.648271Z",
     "start_time": "2025-03-04T19:15:04.637181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "level_map = {\n",
    "    \"Excellent\" : 5,\n",
    "    \"Very Good\" : 4,\n",
    "    \"Good\" : 3,\n",
    "    \"Average\" : 2,\n",
    "    \"Bad\" : 1\n",
    "}\n",
    "RMSE_val_verbal = metrics.root_mean_squared_error(yval.map(level_map), llm_classification.map(level_map))\n",
    "MAE_val_verbal = metrics.mean_absolute_error(yval.map(level_map), llm_classification.map(level_map))\n",
    "print(f\"Root Mean Square Error on validation set after using the verbal classification: {RMSE_val_verbal:.1f}\")\n",
    "print(f\"Mean Absolute Error on validation set after using verbal classification: {MAE_val_verbal:.1f}\")"
   ],
   "id": "90604c06a0e44ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on validation set after using the verbal classification: 1.2\n",
      "Mean Absolute Error on validation set after using verbal classification: 0.8\n"
     ]
    }
   ],
   "execution_count": 270
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d794bc27b8be15cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
