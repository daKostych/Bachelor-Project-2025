{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:07:35.235325Z",
     "start_time": "2025-04-13T09:07:26.136053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if str(Path().resolve().parent) not in sys.path:\n",
    "    sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from src.config import PREPROCESSED_PAPER_DATASET_PATH, GENERATOR_ASSESSMENTS_PATH, GENERATOR_BEST_ASSESSMENTS_PATH\n",
    "from src.text_extraction import *\n",
    "from src.models_setup import embedding_model, gemini_2_flash\n",
    "from src.prompts import prompt_rag, prompt_zero_cot, prompt_retry_with_memory_usage\n",
    "from src.output_formats import BlogGeneration, BlogClassificationCoT\n",
    "from src.helpers import get_examples, load_or_create_vector_store\n",
    "from src.blog_generator import BlogGenerator\n",
    "from src.long_term_memory import LongTermMemory"
   ],
   "id": "265ae9990dfc97c3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1359c2b435d04127",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:07:37.446576Z",
     "start_time": "2025-04-13T09:07:37.377173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data\n",
    "papers = pd.read_csv(PREPROCESSED_PAPER_DATASET_PATH)\n",
    "papers = papers.reset_index(drop=True)\n",
    "\n",
    "# Split dataset into validation and test set (first half for validation)\n",
    "middle = papers.shape[0] // 2\n",
    "Xval = papers.iloc[:middle].reset_index(drop=True)\n",
    "Xtest = papers.iloc[middle:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Size of validation set, X: {Xval.shape}\")\n",
    "print(f\"Size of test set, X: {Xtest.shape}\")"
   ],
   "id": "ef227f791344e06d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation set, X: (37, 4)\n",
      "Size of test set, X: (37, 4)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generator manual test",
   "id": "b7c7064f2624bacd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:07:59.395715Z",
     "start_time": "2025-04-13T09:07:54.423458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = load_or_create_vector_store()\n",
    "\n",
    "def find_most_similar_article(query_text):\n",
    "    query_embedding = embedding_model.encode(query_text, clean_up_tokenization_spaces=True)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=2)\n",
    "\n",
    "    if results:\n",
    "        most_similar = results[1]\n",
    "        return most_similar.page_content, most_similar.metadata"
   ],
   "id": "587de66bbd3dff27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store does not exist. Creating new one...\n",
      "New vector store created successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:16.101425Z",
     "start_time": "2025-04-13T09:12:16.022905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paper_text = Xval.loc[0, \"paper_full_text\"]\n",
    "example_paper, example_blog_metadata = find_most_similar_article(paper_text)\n",
    "example_blog = example_blog_metadata[\"blog_full_text\"]"
   ],
   "id": "63d7a9945d8491d6",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:17.201913Z",
     "start_time": "2025-04-13T09:12:17.196901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG approach\n",
    "llm_generator = gemini_2_flash.with_structured_output(BlogGeneration, include_raw=True)\n",
    "generation_chain = prompt_rag | llm_generator\n",
    "#print(prompt_rag.format(paper_text=paper_text,\n",
    "#                        example_paper=example_paper,\n",
    "#                        example_blog=example_blog))"
   ],
   "id": "6b138943a12ce335",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:24.285754Z",
     "start_time": "2025-04-13T09:12:17.664467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator_response = generation_chain.invoke({\"paper_text\": paper_text,\n",
    "                                              \"example_paper\": example_paper,\n",
    "                                              \"example_blog\": example_blog})"
   ],
   "id": "96ced9f3ccd98531",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:24.302826Z",
     "start_time": "2025-04-13T09:12:24.299566Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator_response[\"parsed\"].text)",
   "id": "60e2552a08968a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Minimum Hyperspherical Energy (MHE) in Neural Networks\n",
      "\n",
      "## TL;DR\n",
      "\n",
      "This blog post discusses a novel regularization technique called Minimum Hyperspherical Energy (MHE) for neural networks. MHE draws inspiration from the Thomson problem in physics to improve the generalization ability of neural networks by encouraging diversity among neurons.\n",
      "\n",
      "## What's the Big Idea?\n",
      "\n",
      "Neural networks are powerful, but their tendency to have redundant, highly correlated neurons can hurt performance. The goal is to regularize the network, avoiding this redundancy. The inspiration comes from the Thomson problem: how to distribute electrons on a sphere to minimize potential energy. MHE applies this concept to neurons, diversifying them to improve the network's generalization ability.\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "*   **Over-parametrization:** Neural networks often have more parameters than necessary, leading to redundancy.\n",
      "*   **Generalization:** The ability of a neural network to perform well on unseen data.\n",
      "*   **Regularization:** Techniques to prevent overfitting and improve generalization.\n",
      "*   **Thomson Problem:** Finding the minimum electrostatic potential energy of N mutually repelling electrons on a sphere.\n",
      "*   **Hyperspherical Energy:** A measure of the diversity of neurons, with lower energy indicating more diversity.\n",
      "\n",
      "## How Does MHE Work?\n",
      "\n",
      "MHE aims to minimize the \"energy\" of neuron configurations, promoting even distribution on a hypersphere. Here's a breakdown:\n",
      "\n",
      "1.  **Hyperspherical Energy Formulation:**\n",
      "    *   Defines a mathematical function to quantify the \"energy\" of a set of neurons.\n",
      "    *   Lower energy means neurons are more diverse and uniformly spaced.\n",
      "2.  **MHE Regularization:**\n",
      "    *   Adds a term to the neural network's loss function that penalizes high hyperspherical energy.\n",
      "    *   Encourages neurons to be as different as possible.\n",
      "3.  **Variants of MHE:**\n",
      "    *   **Half-Space MHE:** Addresses potential redundancy by creating virtual neurons and minimizing energy across both real and virtual neurons. Primarily for hidden layers.\n",
      "    *   **Angular-MHE (A-MHE):** Uses angular distance instead of Euclidean distance to calculate hyperspherical energy.\n",
      "\n",
      "## MHE for Hidden and Output Layers\n",
      "\n",
      "MHE is applied differently to hidden and output layers:\n",
      "\n",
      "*   **Hidden Layers:** MHE encourages neurons to be uniformly distributed on a unit hypersphere, maximizing the average angular difference between neurons.\n",
      "*   **Output Layers:** MHE enhances inter-class feature separability, leading to better generalization. It complements softmax cross-entropy loss by focusing on separating classes.\n",
      "\n",
      "## Theoretical Insights\n",
      "\n",
      "*   **Asymptotic Behavior:** As the number of neurons increases, MHE drives their distribution towards a uniform spread on the hypersphere.\n",
      "*   **Generalization and Optimality:** MHE improves generalization by eliminating spurious local minima and increasing the hyperspherical diversity of neurons.\n",
      "\n",
      "## Experiments and Results\n",
      "\n",
      "MHE was tested on various tasks, including:\n",
      "\n",
      "*   **Generic Object Recognition (CIFAR-10, CIFAR-100, ImageNet):** MHE consistently improved performance across different network architectures.\n",
      "*   **Class-Imbalance Learning (MNIST, CIFAR-10):** MHE effectively handled imbalanced datasets, maintaining accuracy even with significantly fewer samples in some classes.\n",
      "*   **Face Recognition (LFW, MegaFace):** MHE, when incorporated into SphereFace (creating SphereFace+), achieved state-of-the-art results by improving inter-class feature separability.\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "*   MHE is architecture-agnostic and can be applied to various neural network structures.\n",
      "*   Half-space MHE often outperforms standard MHE.\n",
      "*   MHE is not very sensitive to hyperparameter settings.\n",
      "*   MHE improves generalization and prevents overfitting.\n",
      "\n",
      "## Why is This Important?\n",
      "\n",
      "MHE provides a principled way to regularize neural networks, leading to better generalization and performance. By drawing inspiration from physics, it offers a novel perspective on how to optimize the distribution of neurons within a network.\n",
      "\n",
      "## Want to Learn More?\n",
      "\n",
      "*   Read the full paper for detailed mathematical formulations and experimental setups.\n",
      "*   Explore the code (if available) to implement MHE in your own projects.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "MHE is a promising regularization technique that addresses the problem of redundancy in neural networks. Its ability to improve generalization across diverse tasks makes it a valuable tool for deep learning practitioners.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:24.417950Z",
     "start_time": "2025-04-13T09:12:24.410544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = get_examples()\n",
    "blog_text = generator_response[\"parsed\"].text\n",
    "\n",
    "llm_evaluator = gemini_2_flash.with_structured_output(BlogClassificationCoT, include_raw=True)\n",
    "evaluation_chain = prompt_zero_cot | llm_evaluator\n",
    "print(prompt_zero_cot.format(a=\"test for redundant additional parameters\" ,blog_text=blog_text))"
   ],
   "id": "31801f0a1b12cf55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a very strict expert in evaluating written content, specializing in assessing how well blogs communicate scientific research to a broader audience.\n",
      "\n",
      "Task:\n",
      "Analyze the engagement level of the blog below based on the following criteria:\n",
      "    - Readability\n",
      "    - Structure\n",
      "    - Informativeness\n",
      "    - Attractiveness of the blog title\n",
      "    - Clarity\n",
      "    - Audience appeal\n",
      "    - Potential for discussion\n",
      "        \n",
      "Clarifications:\n",
      "    - Focus only on the textual content of the blog, disregarding any visual or interactive elements. This means that there is no need to add points related to the addition of illustrations or interactive elements to possible improvements.\n",
      "    - Calmly lower your blog assessment according to the number of bugs.\n",
      "    - Return ONLY a valid JSON object in plain text.\n",
      "\n",
      "Expected Output Format:\n",
      "    - Step by step, explain your analysis for each of the criteria listed above. Start by evaluating readability, then move on to structure, informativeness, and so on. Make sure to detail why you gave the specific score for each criterion and provide reasoning.\n",
      "    - After completing the analysis for each criterion, summarize the overall engagement level using one of the following ratings: \"Excellent\", \"Very Good\", \"Good\", \"Average\", \"Bad\".\n",
      "    - Write down a few possible improvements that will improve the engagement level, if necessary (if the overall assessment is worse than \"Excellent\").\n",
      "\n",
      "Now evaluate the provided blog.\n",
      "\n",
      "Referenced Blog to Evaluate:\n",
      "\"\"\"# Minimum Hyperspherical Energy (MHE) in Neural Networks\n",
      "\n",
      "## TL;DR\n",
      "\n",
      "This blog post discusses a novel regularization technique called Minimum Hyperspherical Energy (MHE) for neural networks. MHE draws inspiration from the Thomson problem in physics to improve the generalization ability of neural networks by encouraging diversity among neurons.\n",
      "\n",
      "## What's the Big Idea?\n",
      "\n",
      "Neural networks are powerful, but their tendency to have redundant, highly correlated neurons can hurt performance. The goal is to regularize the network, avoiding this redundancy. The inspiration comes from the Thomson problem: how to distribute electrons on a sphere to minimize potential energy. MHE applies this concept to neurons, diversifying them to improve the network's generalization ability.\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "*   **Over-parametrization:** Neural networks often have more parameters than necessary, leading to redundancy.\n",
      "*   **Generalization:** The ability of a neural network to perform well on unseen data.\n",
      "*   **Regularization:** Techniques to prevent overfitting and improve generalization.\n",
      "*   **Thomson Problem:** Finding the minimum electrostatic potential energy of N mutually repelling electrons on a sphere.\n",
      "*   **Hyperspherical Energy:** A measure of the diversity of neurons, with lower energy indicating more diversity.\n",
      "\n",
      "## How Does MHE Work?\n",
      "\n",
      "MHE aims to minimize the \"energy\" of neuron configurations, promoting even distribution on a hypersphere. Here's a breakdown:\n",
      "\n",
      "1.  **Hyperspherical Energy Formulation:**\n",
      "    *   Defines a mathematical function to quantify the \"energy\" of a set of neurons.\n",
      "    *   Lower energy means neurons are more diverse and uniformly spaced.\n",
      "2.  **MHE Regularization:**\n",
      "    *   Adds a term to the neural network's loss function that penalizes high hyperspherical energy.\n",
      "    *   Encourages neurons to be as different as possible.\n",
      "3.  **Variants of MHE:**\n",
      "    *   **Half-Space MHE:** Addresses potential redundancy by creating virtual neurons and minimizing energy across both real and virtual neurons. Primarily for hidden layers.\n",
      "    *   **Angular-MHE (A-MHE):** Uses angular distance instead of Euclidean distance to calculate hyperspherical energy.\n",
      "\n",
      "## MHE for Hidden and Output Layers\n",
      "\n",
      "MHE is applied differently to hidden and output layers:\n",
      "\n",
      "*   **Hidden Layers:** MHE encourages neurons to be uniformly distributed on a unit hypersphere, maximizing the average angular difference between neurons.\n",
      "*   **Output Layers:** MHE enhances inter-class feature separability, leading to better generalization. It complements softmax cross-entropy loss by focusing on separating classes.\n",
      "\n",
      "## Theoretical Insights\n",
      "\n",
      "*   **Asymptotic Behavior:** As the number of neurons increases, MHE drives their distribution towards a uniform spread on the hypersphere.\n",
      "*   **Generalization and Optimality:** MHE improves generalization by eliminating spurious local minima and increasing the hyperspherical diversity of neurons.\n",
      "\n",
      "## Experiments and Results\n",
      "\n",
      "MHE was tested on various tasks, including:\n",
      "\n",
      "*   **Generic Object Recognition (CIFAR-10, CIFAR-100, ImageNet):** MHE consistently improved performance across different network architectures.\n",
      "*   **Class-Imbalance Learning (MNIST, CIFAR-10):** MHE effectively handled imbalanced datasets, maintaining accuracy even with significantly fewer samples in some classes.\n",
      "*   **Face Recognition (LFW, MegaFace):** MHE, when incorporated into SphereFace (creating SphereFace+), achieved state-of-the-art results by improving inter-class feature separability.\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "*   MHE is architecture-agnostic and can be applied to various neural network structures.\n",
      "*   Half-space MHE often outperforms standard MHE.\n",
      "*   MHE is not very sensitive to hyperparameter settings.\n",
      "*   MHE improves generalization and prevents overfitting.\n",
      "\n",
      "## Why is This Important?\n",
      "\n",
      "MHE provides a principled way to regularize neural networks, leading to better generalization and performance. By drawing inspiration from physics, it offers a novel perspective on how to optimize the distribution of neurons within a network.\n",
      "\n",
      "## Want to Learn More?\n",
      "\n",
      "*   Read the full paper for detailed mathematical formulations and experimental setups.\n",
      "*   Explore the code (if available) to implement MHE in your own projects.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "MHE is a promising regularization technique that addresses the problem of redundancy in neural networks. Its ability to improve generalization across diverse tasks makes it a valuable tool for deep learning practitioners.\n",
      "\n",
      "\"\"\"\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:29.404910Z",
     "start_time": "2025-04-13T09:12:25.839507Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})",
   "id": "8b97342ab63b79f9",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:29.416778Z",
     "start_time": "2025-04-13T09:12:29.413511Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")",
   "id": "110822fc4da1a1a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Very Good\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:31.086647Z",
     "start_time": "2025-04-13T09:12:31.082811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improvements = evaluator_response[\"parsed\"].improvements\n",
    "print(f\"Possible improvements:\")\n",
    "for i, improvement in enumerate(improvements):\n",
    "    print(f\"{i+1}. {improvement}\")"
   ],
   "id": "dc7f787c0ae4c674",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible improvements:\n",
      "1. Consider adding a section on the limitations of MHE and potential areas for future research.\n",
      "2. Include a link to the official implementation, if available, could boost the blog attractiveness and potential for discussion.\n",
      "3. Expand the explanation of the Thomson Problem to make it more accessible to readers without a physics background. This can be done by adding more intuitive examples or analogies, simplifying the explanation and adding a simple visual representation of the Thomson problem concept to improve understanding\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "c3beb73142bd99e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:31.721545Z",
     "start_time": "2025-04-13T09:12:31.698792Z"
    }
   },
   "source": [
    "possible_improvements = \"\\n\".join([f\"{i+1}. {improvement}\" for i, improvement in enumerate(improvements)])\n",
    "memory = LongTermMemory()\n",
    "blog = generator_response[\"parsed\"].text\n",
    "similar_blog, metadata = memory.retrieve_memory(blog)\n",
    "\n",
    "# Reflexion\n",
    "generation_chain = prompt_retry_with_memory_usage | llm_generator"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading long term memory module from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/long_term_memory\n",
      "Long term memory module loaded successfully.\n",
      "Found relevant memory.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:37.793349Z",
     "start_time": "2025-04-13T09:12:32.581763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator_response = generation_chain.invoke({\n",
    "    \"generated_blog\": blog,\n",
    "    \"possible_improvements\": possible_improvements,\n",
    "    \"similar_blog\": similar_blog,\n",
    "    \"similar_blog_score\":\n",
    "        metadata[\"overall_assessment\"],\n",
    "    \"similar_blog_improvements\":\n",
    "        metadata[\"improvements\"]\n",
    "})"
   ],
   "id": "7a593dd2daf85cf4",
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "29cde0c08e508057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:12:37.808733Z",
     "start_time": "2025-04-13T09:12:37.804874Z"
    }
   },
   "source": [
    "print(generator_response[\"parsed\"].text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Minimum Hyperspherical Energy (MHE) in Neural Networks\n",
      "\n",
      "## TL;DR\n",
      "\n",
      "This blog post discusses a novel regularization technique called Minimum Hyperspherical Energy (MHE) for neural networks. MHE draws inspiration from the Thomson problem in physics to improve the generalization ability of neural networks by encouraging diversity among neurons.\n",
      "\n",
      "## What's the Big Idea?\n",
      "\n",
      "Neural networks are powerful, but their tendency to have redundant, highly correlated neurons can hurt performance. The goal is to regularize the network, avoiding this redundancy. The inspiration comes from the Thomson problem: how to distribute electrons on a sphere to minimize potential energy. MHE applies this concept to neurons, diversifying them to improve the network's generalization ability.\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "*   **Over-parametrization:** Neural networks often have more parameters than necessary, leading to redundancy.\n",
      "*   **Generalization:** The ability of a neural network to perform well on unseen data.\n",
      "*   **Regularization:** Techniques to prevent overfitting and improve generalization.\n",
      "*   **Thomson Problem:** Finding the minimum electrostatic potential energy of N mutually repelling electrons on a sphere. Imagine trying to place balloons on a perfectly round ball so that they are as far apart from each other as possible - this is essentially the Thomson Problem.\n",
      "*   **Hyperspherical Energy:** A measure of the diversity of neurons, with lower energy indicating more diversity.\n",
      "\n",
      "## How Does MHE Work?\n",
      "\n",
      "MHE aims to minimize the \"energy\" of neuron configurations, promoting even distribution on a hypersphere. Here's a breakdown:\n",
      "\n",
      "1.  **Hyperspherical Energy Formulation:**\n",
      "    *   Defines a mathematical function to quantify the \"energy\" of a set of neurons.\n",
      "    *   Lower energy means neurons are more diverse and uniformly spaced.\n",
      "2.  **MHE Regularization:**\n",
      "    *   Adds a term to the neural network's loss function that penalizes high hyperspherical energy.\n",
      "    *   Encourages neurons to be as different as possible.\n",
      "3.  **Variants of MHE:**\n",
      "    *   **Half-Space MHE:** Addresses potential redundancy by creating virtual neurons and minimizing energy across both real and virtual neurons. Primarily for hidden layers.\n",
      "    *   **Angular-MHE (A-MHE):** Uses angular distance instead of Euclidean distance to calculate hyperspherical energy.\n",
      "\n",
      "## MHE for Hidden and Output Layers\n",
      "\n",
      "MHE is applied differently to hidden and output layers:\n",
      "\n",
      "*   **Hidden Layers:** MHE encourages neurons to be uniformly distributed on a unit hypersphere, maximizing the average angular difference between neurons.\n",
      "*   **Output Layers:** MHE enhances inter-class feature separability, leading to better generalization. It complements softmax cross-entropy loss by focusing on separating classes.\n",
      "\n",
      "## Theoretical Insights\n",
      "\n",
      "*   **Asymptotic Behavior:** As the number of neurons increases, MHE drives their distribution towards a uniform spread on the hypersphere.\n",
      "*   **Generalization and Optimality:** MHE improves generalization by eliminating spurious local minima and increasing the hyperspherical diversity of neurons.\n",
      "\n",
      "## Experiments and Results\n",
      "\n",
      "MHE was tested on various tasks, including:\n",
      "\n",
      "*   **Generic Object Recognition (CIFAR-10, CIFAR-100, ImageNet):** MHE consistently improved performance across different network architectures.\n",
      "*   **Class-Imbalance Learning (MNIST, CIFAR-10):** MHE effectively handled imbalanced datasets, maintaining accuracy even with significantly fewer samples in some classes.\n",
      "*   **Face Recognition (LFW, MegaFace):** MHE, when incorporated into SphereFace (creating SphereFace+), achieved state-of-the-art results by improving inter-class feature separability.\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "*   MHE is architecture-agnostic and can be applied to various neural network structures.\n",
      "*   Half-space MHE often outperforms standard MHE.\n",
      "*   MHE is not very sensitive to hyperparameter settings.\n",
      "*   MHE improves generalization and prevents overfitting.\n",
      "\n",
      "## Why is This Important?\n",
      "\n",
      "MHE provides a principled way to regularize neural networks, leading to better generalization and performance. By drawing inspiration from physics, it offers a novel perspective on how to optimize the distribution of neurons within a network.\n",
      "\n",
      "## Limitations and Future Research\n",
      "\n",
      "While MHE offers significant benefits, it's important to acknowledge its limitations. One potential drawback is the computational cost associated with calculating hyperspherical energy, especially for very large networks. Future research could explore methods to approximate this energy more efficiently. Additionally, investigating the applicability of MHE to other types of neural network architectures, such as transformers, could be a promising avenue for future work.\n",
      "\n",
      "## Want to Learn More?\n",
      "\n",
      "*   Read the full paper for detailed mathematical formulations and experimental setups.\n",
      "*   Explore the code (if available) to implement MHE in your own projects. [If available, add link to official implementation here]\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "MHE is a promising regularization technique that addresses the problem of redundancy in neural networks. Its ability to improve generalization across diverse tasks makes it a valuable tool for deep learning practitioners.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "5206f48504f5deef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:13:00.833164Z",
     "start_time": "2025-04-13T09:12:58.185665Z"
    }
   },
   "source": [
    "blog_text = generator_response[\"parsed\"].text\n",
    "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "ec4b23503b229f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:13:00.843717Z",
     "start_time": "2025-04-13T09:13:00.840636Z"
    }
   },
   "source": [
    "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Excellent\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "a9ac6d3e8d32b56f",
   "metadata": {},
   "source": [
    "# End-to-end generation test"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb1b8f5fd61f0acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:13:13.641714Z",
     "start_time": "2025-04-13T09:13:13.629977Z"
    }
   },
   "source": "generator = BlogGenerator(use_memory=True, use_reflexion=True)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/vector_store\n",
      "Vector store loaded successfully.\n",
      "Loading long term memory module from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/long_term_memory\n",
      "Long term memory module loaded successfully.\n",
      "BlogGenerator initialized with vector store.\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "b85733eac47fe27c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:13:28.235721Z",
     "start_time": "2025-04-13T09:13:18.377128Z"
    }
   },
   "source": "blog = generator.generate_blog(paper_text=Xval.loc[0, \"paper_full_text\"])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Found most similar article.\n",
      "Attempt number 1: Generating blog...\n",
      "Using RAG prompt...\n",
      "Checking request limits before invoking the model...\n",
      "Invoking the model...\n",
      "Model invoked successfully. Total requests today: 1, RPM: 1, TPM: 30323\n",
      "Blog generated successfully.\n",
      "Checking request limits before invoking the model...\n",
      "Invoking the model...\n",
      "Model invoked successfully. Total requests today: 2, RPM: 2, TPM: 32050\n",
      "Blog evaluated successfully. Evaluation: Very Good.\n",
      "No similar blogs found, new blog is added to memory.\n",
      "Saving blog in file: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/generation_results/blog\n",
      "Long term memory module saved successfully.\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "e20ec1fbb312fcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:13:43.441670Z",
     "start_time": "2025-04-13T09:13:43.438279Z"
    }
   },
   "source": [
    "print(blog)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Regularizing Neural Networks with Hyperspherical Energy: A Deep Dive\n",
      "\n",
      "Have you ever wondered why some neural networks generalize better than others? The secret might lie in how well the neurons are organized within the network. In a recent paper, researchers explored a fascinating connection between neural network regularization and a classic physics problem: the Thomson problem.\n",
      "\n",
      "## The Thomson Problem: Evenly Distributing Electrons on a Sphere\n",
      "\n",
      "The Thomson problem asks: how do you arrange N electrons on the surface of a sphere so that their electrostatic potential energy is minimized? In simpler terms, how do you spread out the electrons as evenly as possible, given that they all repel each other?\n",
      "\n",
      "## Minimum Hyperspherical Energy (MHE): Applying the Thomson Problem to Neural Networks\n",
      "\n",
      "The researchers drew inspiration from this problem to tackle a common issue in neural networks: redundancy. Over-parameterized networks often have highly correlated neurons, which can hurt their ability to generalize to new data. To combat this, they proposed a novel regularization technique called Minimum Hyperspherical Energy (MHE).\n",
      "\n",
      "MHE aims to distribute neurons evenly on a hypersphere, a higher-dimensional analog of a sphere. By minimizing the \"hyperspherical energy,\" the neurons are encouraged to be as diverse and uniformly spaced as possible. Think of it like forcing the neurons to repel each other, just like the electrons in the Thomson problem.\n",
      "\n",
      "### Key Concepts:\n",
      "\n",
      "*   **Neurons as Points on a Hypersphere:** Each neuron's weight vector is projected onto a unit hypersphere.\n",
      "*   **Hyperspherical Energy:** A measure of how evenly the neurons are distributed on the hypersphere. Lower energy means more even distribution.\n",
      "*   **Regularization:** Adding MHE to the neural network's loss function to encourage diversity and improve generalization.\n",
      "\n",
      "## MHE Variants: Tailoring Regularization to Different Layers\n",
      "\n",
      "The researchers introduced a few clever variations of MHE to handle different scenarios within the neural network:\n",
      "\n",
      "*   **Half-Space MHE:** This variant addresses a potential issue where neurons might align in opposite directions. It creates virtual neurons with opposite directions and minimizes the energy of both real and virtual neurons.\n",
      "*   **Angular-MHE (A-MHE):** Instead of using Euclidean distance to measure the distance between neurons, A-MHE uses the angular distance (the angle between the neurons).\n",
      "\n",
      "## Theoretical Insights: Why MHE Works\n",
      "\n",
      "The paper also provides some theoretical justification for why MHE is effective:\n",
      "\n",
      "*   **Asymptotic Behavior:** As the number of neurons increases, the optimal configuration tends towards a uniform distribution on the hypersphere.\n",
      "*   **Generalization and Optimality:** MHE encourages diversity, which helps eliminate spurious local minima and promotes better generalization.\n",
      "\n",
      "## Experimental Results: MHE in Action\n",
      "\n",
      "The researchers tested MHE on a variety of challenging tasks, including:\n",
      "\n",
      "*   **Generic Object Recognition (CIFAR-10, CIFAR-100, ImageNet):** MHE consistently improved the generalization performance of various network architectures.\n",
      "*   **Class-Imbalance Learning (MNIST, CIFAR-10):** MHE helped the network learn more effectively when the number of training samples was unevenly distributed across classes.\n",
      "*   **Face Recognition (LFW, MegaFace):** MHE, when applied to SphereFace (a face recognition method), led to state-of-the-art results.\n",
      "\n",
      "### Key Findings:\n",
      "\n",
      "*   MHE is architecture-agnostic and can be applied to various neural network architectures.\n",
      "*   Half-space MHE often outperforms the original MHE.\n",
      "*   MHE is not very sensitive to hyperparameter settings, making it easy to use.\n",
      "\n",
      "## MHE for GANs\n",
      "MHE can also significantly improve the image generation quality of GANs\n",
      "\n",
      "## Conclusion: A Novel Approach to Regularization\n",
      "\n",
      "This research offers a fresh perspective on regularizing neural networks by drawing inspiration from a physics problem. MHE encourages diversity among neurons, leading to improved generalization and performance on a range of tasks. By promoting a more uniform distribution of neurons on a hypersphere, MHE helps neural networks learn more robust and reliable representations.\n",
      "\n",
      "**Want to learn more?** Check out the full paper for all the details!\n",
      "\n",
      "**What do you think about this approach? Share your thoughts in the comments below!**\n",
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "id": "ca760e82f6293302",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:16:17.107939Z",
     "start_time": "2025-04-13T09:16:17.091750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results(column, assessments, best_assessments):\n",
    "    def create_new_json(error, res_type):\n",
    "        print(f\"{error}\\n\"\n",
    "              f\"Creating new json file for {res_type}...\")\n",
    "        json_file = {\n",
    "            \"RAG only\": [],\n",
    "            \"Reflexion\": [],\n",
    "            \"Reflexion + LTM\": []\n",
    "        }\n",
    "        return json_file\n",
    "\n",
    "    try:\n",
    "        with open(GENERATOR_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            results_generator_assessments = json.load(f)\n",
    "    except Exception as err:\n",
    "        results_generator_assessments = create_new_json(err, \"Generator assessments\")\n",
    "\n",
    "    try:\n",
    "        with open(GENERATOR_BEST_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            results_generator_best_assessments = json.load(f)\n",
    "    except Exception as err:\n",
    "        results_generator_best_assessments = create_new_json(err, \"Generator best assessments\")\n",
    "\n",
    "    results_generator_assessments[column] = assessments\n",
    "    results_generator_best_assessments[column] = best_assessments\n",
    "\n",
    "    with open(GENERATOR_ASSESSMENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_generator_assessments, f)\n",
    "    with open(GENERATOR_BEST_ASSESSMENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_generator_best_assessments, f)\n",
    "\n",
    "def run_experiment(history, best_history, dataset):\n",
    "    redo = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        assessment, best_assessment = generator.generate_blog(paper_text=row[\"paper_full_text\"])\n",
    "        if assessment and best_assessment:\n",
    "            history[index] = assessment\n",
    "            best_history[index] = best_assessment\n",
    "        else:\n",
    "            redo.append(index)\n",
    "    return redo\n",
    "\n",
    "def rerun_experiment_if_need(redo_list, history, best_history, dataset):\n",
    "    while redo_list:\n",
    "        redo_index = 0\n",
    "        for index in redo_list:\n",
    "            assessment, best_assessment = generator.generate_blog(paper_text=dataset.loc[index, \"paper_full_text\"])\n",
    "            if assessment and best_assessment:\n",
    "                history[index] = assessment\n",
    "                best_history[index] = best_assessment\n",
    "                del redo_list[redo_index]\n",
    "                continue\n",
    "            redo_index += 1\n",
    "\n",
    "MAX_ATTEMPTS = 5\n",
    "DATASET_SIZE = Xval.shape[0]\n",
    "generator = BlogGenerator()"
   ],
   "id": "5260266041bec0bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/vector_store\n",
      "Vector store loaded successfully.\n",
      "Loading long term memory module from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/long_term_memory\n",
      "Long term memory module loaded successfully.\n",
      "BlogGenerator initialized with vector store.\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG only approach",
   "id": "c77bed12327234c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:55:20.370206Z",
     "start_time": "2025-04-12T19:55:20.366995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator.max_attempts = MAX_ATTEMPTS\n",
    "generator.experiment_mode = True\n",
    "generator.set_usage_of_reflexion(False)\n",
    "generator.set_usage_of_memory(False)"
   ],
   "id": "e89febdf9b5d4b9d",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assessment_history_only_RAG = [None] * DATASET_SIZE\n",
    "best_assessment_history_only_RAG = [None] * DATASET_SIZE\n",
    "redo_only_RAG = run_experiment(assessment_history_only_RAG, best_assessment_history_only_RAG, Xval)"
   ],
   "id": "66f70e68429e6065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Experiment should be conduct again for blogs with ids: {redo_only_RAG}\")",
   "id": "824cd3c1d495d540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rerun_experiment_if_need(redo_only_RAG, assessment_history_only_RAG, best_assessment_history_only_RAG, Xval)",
   "id": "458cf48903cd009a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T17:55:11.579822Z",
     "start_time": "2025-04-12T17:55:11.575374Z"
    }
   },
   "cell_type": "code",
   "source": "save_results(\"RAG only\", assessment_history_only_RAG, best_assessment_history_only_RAG)",
   "id": "1391aeeea4afd119",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reflexion",
   "id": "75d70b144b72e0ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T18:26:38.912657Z",
     "start_time": "2025-04-12T18:26:38.909945Z"
    }
   },
   "cell_type": "code",
   "source": "generator.set_usage_of_reflexion(True)",
   "id": "2fccaa31040aaa97",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assessment_history_ref = [None] * DATASET_SIZE\n",
    "best_assessment_history_ref = [None] * DATASET_SIZE\n",
    "redo_ref = run_experiment(assessment_history_ref, best_assessment_history_ref, Xval)"
   ],
   "id": "93f0ced475f854f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:24:56.060750Z",
     "start_time": "2025-04-12T19:24:56.056392Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Experiment should be conduct again for blogs with ids: {redo_ref}\")",
   "id": "5b37a502e60df3ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment should be conduct again for blogs with ids: [34, 45]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rerun_experiment_if_need(redo_ref, assessment_history_ref, best_assessment_history_ref, Xval)",
   "id": "d82a057b27132779",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:53:05.927171Z",
     "start_time": "2025-04-12T19:53:05.921400Z"
    }
   },
   "cell_type": "code",
   "source": "save_results(\"Reflexion\", assessment_history_ref, best_assessment_history_ref)",
   "id": "9baac8b13d29ac84",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reflexion + Long Term Memory",
   "id": "a1278a92032fbafe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "generator.set_usage_of_memory(True)",
   "id": "e0740ac9ffb375b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "assessment_history_with_memory_usage = [None] * DATASET_SIZE\n",
    "best_assessment_history_with_memory_usage = [None] * DATASET_SIZE\n",
    "redo_with_memory_usage = run_experiment(assessment_history_with_memory_usage, best_assessment_history_with_memory_usage, Xtest)"
   ],
   "id": "cc9f13b75d01aa12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(f\"Experiment should be conduct again for blogs with ids: {redo_with_memory_usage}\")",
   "id": "b4931d22ca8195ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "rerun_experiment_if_need(redo_with_memory_usage, assessment_history_with_memory_usage, best_assessment_history_with_memory_usage, Xtest)",
   "id": "213581e0e09a928"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_results(\"Reflexion + LTM\", assessment_history_with_memory_usage, best_assessment_history_with_memory_usage)",
   "id": "7fbcdca994aacd54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results",
   "id": "c82e898f90f4965"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:40:32.762056Z",
     "start_time": "2025-04-12T19:40:32.755982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot(results_rag, results_ref, results_with_memory_usage, results_type):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, 6), results_rag, marker=\"s\", label=\"RAG only\")\n",
    "    plt.plot(range(1, 6), results_ref, marker=\"o\", label=\"Reflexion\")\n",
    "    plt.plot(range(1, 6), results_with_memory_usage, marker=\"d\", label=\"Reflexion + LTM\")\n",
    "\n",
    "    distance_from_edge = 0.2\n",
    "    min_value = np.min([results_rag.min(), results_ref.min(), results_with_memory_usage.min()])\n",
    "    plt.yticks([1, 2, 3, 4, 5], [\"Bad\", \"Average\", \"Good\", \"Very Good\", \"Excellent\"])\n",
    "    plt.ylim(math.floor(min_value) - distance_from_edge, 5 + distance_from_edge)\n",
    "    plt.xticks(range(1, 6))\n",
    "\n",
    "    plt.xlabel(\"Number of Attempts\")\n",
    "    plt.ylabel(f\"Average Engagement Level\")\n",
    "    plt.title(f\"{results_type}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c2c325e625c85e0f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    with open(GENERATOR_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "        assessment_history = json.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\\n\"\n",
    "          f\"Results not founded.\")"
   ],
   "id": "4ba6718381026f22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:32:58.158061Z",
     "start_time": "2025-04-11T14:32:58.154334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag = np.array(assessment_history[\"RAG only\"])\n",
    "average_rag = np.mean(rag, axis=0)\n",
    "\n",
    "reflexion = np.array(assessment_history[\"Reflexion\"])\n",
    "average_reflexion = np.mean(reflexion, axis=0)\n",
    "\n",
    "ltm = np.array(assessment_history[\"Reflexion + LTM\"])\n",
    "average_ltm = np.mean(ltm, axis=0)\n",
    "\n",
    "plot(average_rag, average_reflexion, average_ltm, \"The Average Blog Engagement Level in This Attempt\")"
   ],
   "id": "9f9d62893bb19e40",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:35:31.281189Z",
     "start_time": "2025-04-12T19:35:31.277520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    with open(GENERATOR_BEST_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "        best_assessment_history = json.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\\n\"\n",
    "          f\"Results not founded.\")"
   ],
   "id": "c3e9a7b37e570349",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rag_best = np.array(best_assessment_history[\"RAG only\"])\n",
    "average_rag_best = np.mean(rag_best, axis=0)\n",
    "\n",
    "reflexion_best = np.array(best_assessment_history[\"Reflexion\"])\n",
    "average_reflexion_best = np.mean(reflexion_best, axis=0)\n",
    "\n",
    "ltm_best = np.array(best_assessment_history[\"Reflexion + LTM\"])\n",
    "average_ltm_best = np.mean(ltm_best, axis=0)\n",
    "\n",
    "plot(average_rag_best, average_reflexion_best, average_ltm_best, \"The Average Engagement Level of The Best Blog Generated Up To and Including This Attempt\")"
   ],
   "id": "a2452043568d3ad9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
