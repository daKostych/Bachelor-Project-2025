{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T21:23:25.357170Z",
     "start_time": "2025-04-12T21:23:14.045354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if str(Path().resolve().parent) not in sys.path:\n",
    "    sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from src.config import PREPROCESSED_BLOG_DATASET_PATH, GENERATOR_ASSESSMENTS_PATH, GENERATOR_BEST_ASSESSMENTS_PATH, random_seed\n",
    "from src.text_extraction import *\n",
    "from src.models_setup import embedding_model, gemini_2_flash\n",
    "from src.prompts import prompt_rag, prompt_zero_cot, prompt_retry_with_memory_usage\n",
    "from src.output_formats import BlogGeneration, BlogClassificationCoT\n",
    "from src.helpers import get_examples, load_or_create_vector_store\n",
    "from src.blog_generator import BlogGenerator\n",
    "from src.long_term_memory import LongTermMemory"
   ],
   "id": "265ae9990dfc97c3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "1359c2b435d04127",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T21:23:25.398712Z",
     "start_time": "2025-04-12T21:23:25.375178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data\n",
    "blogs = pd.read_csv(PREPROCESSED_BLOG_DATASET_PATH)\n",
    "blogs = blogs.reset_index(drop=True)\n",
    "\n",
    "# Split dataset into validation and test set\n",
    "Xval, Xtest, yval_score, ytest_score = train_test_split(\n",
    "    blogs.drop(columns=['normalized_engagement_score']), blogs[\"normalized_engagement_score\"],\n",
    "    test_size=0.5, random_state=random_seed)\n",
    "\n",
    "# Same Xval, Xtest; new explained variable \"engagement_level\"\n",
    "Xval, Xtest, yval_level, ytest_level = train_test_split(\n",
    "    blogs.drop(columns=['engagement_level', 'normalized_engagement_score']), blogs[\"engagement_level\"],\n",
    "    test_size=0.5, random_state=random_seed)\n",
    "\n",
    "Xval.reset_index(drop=True, inplace=True)\n",
    "Xtest.reset_index(drop=True, inplace=True)\n",
    "yval_score.reset_index(drop=True, inplace=True)\n",
    "ytest_score.reset_index(drop=True, inplace=True)\n",
    "yval_level.reset_index(drop=True, inplace=True)\n",
    "ytest_level.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Size of validation set, X: {Xval.shape}, y: {yval_score.shape}\")\n",
    "print(f\"Size of test set, X: {Xtest.shape}, y: {ytest_score.shape}\")"
   ],
   "id": "ef227f791344e06d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation set, X: (25, 13), y: (25,)\n",
      "Size of test set, X: (25, 13), y: (25,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generator manual test",
   "id": "b7c7064f2624bacd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:11:47.294595Z",
     "start_time": "2025-04-11T19:11:47.287788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = load_or_create_vector_store()\n",
    "\n",
    "def find_most_similar_article(query_text):\n",
    "    query_embedding = embedding_model.encode(query_text, clean_up_tokenization_spaces=True)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=2)\n",
    "\n",
    "    if results:\n",
    "        most_similar = results[1]\n",
    "        return most_similar.page_content, most_similar.metadata"
   ],
   "id": "587de66bbd3dff27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/vector_store\n",
      "Vector store loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:11:48.261784Z",
     "start_time": "2025-04-11T19:11:47.524188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paper_text = extract_paper_text(Xval.loc[0, \"url_paper\"])\n",
    "example_paper, example_blog_metadata = find_most_similar_article(paper_text)\n",
    "example_blog = example_blog_metadata[\"blog_full_text\"]"
   ],
   "id": "63d7a9945d8491d6",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:11:48.272597Z",
     "start_time": "2025-04-11T19:11:48.268152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG approach\n",
    "llm_generator = gemini_2_flash.with_structured_output(BlogGeneration, include_raw=True)\n",
    "generation_chain = prompt_rag | llm_generator\n",
    "#print(prompt_rag.format(paper_text=paper_text,\n",
    "#                        example_paper=example_paper,\n",
    "#                        example_blog=example_blog))"
   ],
   "id": "6b138943a12ce335",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:12:14.326503Z",
     "start_time": "2025-04-11T19:12:08.423466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator_response = generation_chain.invoke({\"paper_text\": paper_text,\n",
    "                                              \"example_paper\": example_paper,\n",
    "                                              \"example_blog\": example_blog})"
   ],
   "id": "96ced9f3ccd98531",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:12:14.345226Z",
     "start_time": "2025-04-11T19:12:14.341852Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator_response[\"parsed\"].text)",
   "id": "60e2552a08968a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Level Up Your Language Model with Self-Generated Critiques\n",
      "\n",
      "By Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Imagine your language model constantly learning and improving, not just from external data, but from its own insightful self-critiques. That's the power of **Critic-RM**, a new framework that boosts reward modeling for language models using self-generated critiques. This innovative approach enhances language models' ability to align with human preferences, leading to more helpful, honest, and harmless responses.\n",
      "\n",
      "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce unexplainable scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that generating both critiques and scalar rewards would improve reward models’ capability on preference ranking.\n",
      "\n",
      "## What is Critic-RM?\n",
      "\n",
      "Critic-RM is a framework that utilizes self-generated, high-quality critiques to train reward models for scalar reward-based preference prediction, with explicit rationales serving as supporting evidence. It's like giving your language model a built-in editor that helps it refine its responses and better understand what makes a good answer.\n",
      "\n",
      "Traditional reward models often fall short because they only produce a single, hard-to-interpret score. Critic-RM changes the game by incorporating detailed critiques, offering a more nuanced and effective way to provide feedback during the training process.\n",
      "\n",
      "## How Does Critic-RM Work?\n",
      "\n",
      "Critic-RM employs a clever two-stage process:\n",
      "\n",
      "### 1. Generating and Filtering High-Quality Critiques\n",
      "\n",
      "The model starts by generating multiple candidate critiques for each response. To ensure quality, it uses a consistency-guided filtering technique, retaining only critiques that align with human-annotated preference labels. Think of it as sifting through feedback to find the most constructive and accurate insights.\n",
      "\n",
      "### 2. Joint Fine-Tuning on Reward Prediction and Critique Generation Objectives\n",
      "\n",
      "Next, the model undergoes joint fine-tuning, learning to predict rewards and generate critiques simultaneously. This balanced approach allows the model to excel at both high-quality critique generation and accurate reward prediction.\n",
      "\n",
      "## Key Benefits of Critic-RM\n",
      "\n",
      "*   **Improved Reward Modeling Accuracy:** Critic-RM improves reward modeling accuracy by 3.7%–7.3% compared to standard reward models and LLM judges.\n",
      "*   **Enhanced Reasoning Accuracy:** The generated critiques help rectify flawed reasoning steps, improving reasoning accuracy by 2.5%-3.2%.\n",
      "*   **Data Efficiency:** Critic-RM demonstrates strong performance even with limited data.\n",
      "\n",
      "## Critic-RM in Action: Experiments and Results\n",
      "\n",
      "Extensive experiments on preference ranking benchmarks, including RewardBench and CrossEval, demonstrate Critic-RM's superior performance compared to traditional reward models and LLM judges. These results highlight the effectiveness of self-generated critiques in enhancing language model capabilities.\n",
      "\n",
      "## The Power of Self-Critique\n",
      "\n",
      "Critic-RM draws inspiration from recent advances in self-improving language models, where models are iteratively refined using data generated by themselves. By injecting critique generation ability into the reward modeling process, Critic-RM unlocks a new level of self-improvement for language models.\n",
      "\n",
      "## Addressing the Challenges\n",
      "\n",
      "Incorporating critiques into reward modeling presents two major challenges:\n",
      "\n",
      "1.  **Conflicting Objectives:** Critique generation requires language modeling, while reward models provide scalar outputs, complicating its integration into language modeling.\n",
      "2.  **Evaluator Limitations:** Off-the-shelf LMs are often not good evaluators, while additional fine-tuning requires costly human-generated or annotated critiques.\n",
      "\n",
      "Critic-RM tackles these challenges head-on with its innovative framework and training strategies.\n",
      "\n",
      "## Related Work\n",
      "\n",
      "Recent work has explored incorporating critiques into reward modeling, but these methods typically rely on strong teacher LLMs to generate high-quality critiques, which can be costly and inefficient. Critic-RM distinguishes itself by enhancing reward models using synthetic critiques, without relying on strong LLM teachers.\n",
      "\n",
      "## Methodology\n",
      "\n",
      "Critic-RM leverages an instruction-finetuned LLM as the backbone, which generates multiple candidate critiques, each with a discrete score for individual responses. The framework consists of three main steps: (1) Qualitative problem analysis, (2) Synthetic data generation, and (3) Model training.\n",
      "\n",
      "## Critic-RM Inference\n",
      "\n",
      "Compared to standard reward model training, Critic-RM involves an additional step for each (prompt, response) pair during inference. Specifically, given the (prompt, response) pair (x, y), the model will first generate a critique z, then predict the reward for the response as r.\n",
      "\n",
      "## Experiment Setup\n",
      "\n",
      "To ensure the representativeness of the preference pairs used in this study, we leverage both public and synthetic datasets for reward model training.\n",
      "\n",
      "### Public Preference Datasets:\n",
      "\n",
      "*   General Chat Domain: We include datasets from ChatArena and AlpacaFarm-Human-Pref.\n",
      "*   Helpfulness Data: We leverage HelpSteer2 to create preference data.\n",
      "*   Reasoning: We mainly use Evol-instruct which contains preference pairs for complex instruction following, coding-related tasks.\n",
      "*   Safety: We employ PKU-SafeRLHF, which includes safety-related prompts paired with both safe and unsafe responses to form preference pairs.\n",
      "\n",
      "### Synthetic Preference Datasets:\n",
      "\n",
      "To incorporate additional preference supervision from different domains, we further include synthetic data using Llama-3.1 models. Specifically, for the math domain, we consider questions in GSM8K and the MATH dataset. In the safety domain, we generate synthetic prompts following the safety principles outlined in SafeRLHF\n",
      "\n",
      "## Evaluation Benchmarks\n",
      "\n",
      "In our experiments, we mainly evaluate on RewardBench, which contains a collection of prompt-chosen-rejected triplets across chat, reasoning, and safety domains, including 2985 examples in total. Beyond RewardBench, we also aim to test the out-of-distribution generalization ability of reward models. Specifically, we consider CrossEval, a recently proposed benchmark to evaluate the LLM’s capability in real-world interactions.\n",
      "\n",
      "## Ablation Studies\n",
      "\n",
      "Our data filtering strategy demonstrates that using the entire dataset without filtering leads to poor performance, particularly in the Chat-hard domain, which requires stronger reasoning capabilities for LLMs to assess response preferences accurately. Additionally, removing noisy preference pairs improves standard reward modeling. Moreover, incorporating summarization and ranking proves to be an effective approach for boosting overall performance.\n",
      "\n",
      "## Limitations and Future Directions\n",
      "\n",
      "While Critic-RM offers significant advancements, there are still areas for improvement:\n",
      "\n",
      "*   **Single Model Focus:** Testing Critic-RM across different LLM architectures could provide broader insights into its effectiveness.\n",
      "*   **Longer Inference Time:** Generating critiques during inference adds computational overhead.\n",
      "*   **No Iterative Training:** Adding iterative training steps could further improve reward modeling performance.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Critic-RM represents a significant step forward in reward modeling for language models. By harnessing the power of self-generated critiques, this framework enhances language models' ability to align with human preferences, leading to more helpful, honest, and harmless responses. As language models continue to evolve, self-critiquing techniques like Critic-RM will play a crucial role in ensuring their responsible and effective deployment.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:12:14.484124Z",
     "start_time": "2025-04-11T19:12:14.462122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = get_examples()\n",
    "blog_text = generator_response[\"parsed\"].text\n",
    "\n",
    "llm_evaluator = gemini_2_flash.with_structured_output(BlogClassificationCoT, include_raw=True)\n",
    "evaluation_chain = prompt_zero_cot | llm_evaluator\n",
    "print(prompt_zero_cot.format(a=\"test for redundant additional parameters\" ,blog_text=blog_text))"
   ],
   "id": "31801f0a1b12cf55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a very strict expert in evaluating written content, specializing in assessing how well blogs communicate scientific research to a broader audience.\n",
      "\n",
      "Task:\n",
      "Analyze the engagement level of the blog below based on the following criteria:\n",
      "    - Readability\n",
      "    - Structure\n",
      "    - Informativeness\n",
      "    - Attractiveness of the blog title\n",
      "    - Clarity\n",
      "    - Audience appeal\n",
      "    - Potential for discussion\n",
      "        \n",
      "Clarifications:\n",
      "    - Focus only on the textual content of the blog, disregarding any visual or interactive elements. This means that there is no need to add points related to the addition of illustrations or interactive elements to possible improvements.\n",
      "    - Calmly lower your blog assessment according to the number of bugs.\n",
      "    - Return ONLY a valid JSON object in plain text.\n",
      "\n",
      "Expected Output Format:\n",
      "    - Step by step, explain your analysis for each of the criteria listed above. Start by evaluating readability, then move on to structure, informativeness, and so on. Make sure to detail why you gave the specific score for each criterion and provide reasoning.\n",
      "    - After completing the analysis for each criterion, summarize the overall engagement level using one of the following ratings: \"Excellent\", \"Very Good\", \"Good\", \"Average\", \"Bad\".\n",
      "    - Write down a few possible improvements that will improve the engagement level, if necessary (if the overall assessment is worse than \"Excellent\").\n",
      "\n",
      "Now evaluate the provided blog.\n",
      "\n",
      "Referenced Blog to Evaluate:\n",
      "\"\"\"# Level Up Your Language Model with Self-Generated Critiques\n",
      "\n",
      "By Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Imagine your language model constantly learning and improving, not just from external data, but from its own insightful self-critiques. That's the power of **Critic-RM**, a new framework that boosts reward modeling for language models using self-generated critiques. This innovative approach enhances language models' ability to align with human preferences, leading to more helpful, honest, and harmless responses.\n",
      "\n",
      "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce unexplainable scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that generating both critiques and scalar rewards would improve reward models’ capability on preference ranking.\n",
      "\n",
      "## What is Critic-RM?\n",
      "\n",
      "Critic-RM is a framework that utilizes self-generated, high-quality critiques to train reward models for scalar reward-based preference prediction, with explicit rationales serving as supporting evidence. It's like giving your language model a built-in editor that helps it refine its responses and better understand what makes a good answer.\n",
      "\n",
      "Traditional reward models often fall short because they only produce a single, hard-to-interpret score. Critic-RM changes the game by incorporating detailed critiques, offering a more nuanced and effective way to provide feedback during the training process.\n",
      "\n",
      "## How Does Critic-RM Work?\n",
      "\n",
      "Critic-RM employs a clever two-stage process:\n",
      "\n",
      "### 1. Generating and Filtering High-Quality Critiques\n",
      "\n",
      "The model starts by generating multiple candidate critiques for each response. To ensure quality, it uses a consistency-guided filtering technique, retaining only critiques that align with human-annotated preference labels. Think of it as sifting through feedback to find the most constructive and accurate insights.\n",
      "\n",
      "### 2. Joint Fine-Tuning on Reward Prediction and Critique Generation Objectives\n",
      "\n",
      "Next, the model undergoes joint fine-tuning, learning to predict rewards and generate critiques simultaneously. This balanced approach allows the model to excel at both high-quality critique generation and accurate reward prediction.\n",
      "\n",
      "## Key Benefits of Critic-RM\n",
      "\n",
      "*   **Improved Reward Modeling Accuracy:** Critic-RM improves reward modeling accuracy by 3.7%–7.3% compared to standard reward models and LLM judges.\n",
      "*   **Enhanced Reasoning Accuracy:** The generated critiques help rectify flawed reasoning steps, improving reasoning accuracy by 2.5%-3.2%.\n",
      "*   **Data Efficiency:** Critic-RM demonstrates strong performance even with limited data.\n",
      "\n",
      "## Critic-RM in Action: Experiments and Results\n",
      "\n",
      "Extensive experiments on preference ranking benchmarks, including RewardBench and CrossEval, demonstrate Critic-RM's superior performance compared to traditional reward models and LLM judges. These results highlight the effectiveness of self-generated critiques in enhancing language model capabilities.\n",
      "\n",
      "## The Power of Self-Critique\n",
      "\n",
      "Critic-RM draws inspiration from recent advances in self-improving language models, where models are iteratively refined using data generated by themselves. By injecting critique generation ability into the reward modeling process, Critic-RM unlocks a new level of self-improvement for language models.\n",
      "\n",
      "## Addressing the Challenges\n",
      "\n",
      "Incorporating critiques into reward modeling presents two major challenges:\n",
      "\n",
      "1.  **Conflicting Objectives:** Critique generation requires language modeling, while reward models provide scalar outputs, complicating its integration into language modeling.\n",
      "2.  **Evaluator Limitations:** Off-the-shelf LMs are often not good evaluators, while additional fine-tuning requires costly human-generated or annotated critiques.\n",
      "\n",
      "Critic-RM tackles these challenges head-on with its innovative framework and training strategies.\n",
      "\n",
      "## Related Work\n",
      "\n",
      "Recent work has explored incorporating critiques into reward modeling, but these methods typically rely on strong teacher LLMs to generate high-quality critiques, which can be costly and inefficient. Critic-RM distinguishes itself by enhancing reward models using synthetic critiques, without relying on strong LLM teachers.\n",
      "\n",
      "## Methodology\n",
      "\n",
      "Critic-RM leverages an instruction-finetuned LLM as the backbone, which generates multiple candidate critiques, each with a discrete score for individual responses. The framework consists of three main steps: (1) Qualitative problem analysis, (2) Synthetic data generation, and (3) Model training.\n",
      "\n",
      "## Critic-RM Inference\n",
      "\n",
      "Compared to standard reward model training, Critic-RM involves an additional step for each (prompt, response) pair during inference. Specifically, given the (prompt, response) pair (x, y), the model will first generate a critique z, then predict the reward for the response as r.\n",
      "\n",
      "## Experiment Setup\n",
      "\n",
      "To ensure the representativeness of the preference pairs used in this study, we leverage both public and synthetic datasets for reward model training.\n",
      "\n",
      "### Public Preference Datasets:\n",
      "\n",
      "*   General Chat Domain: We include datasets from ChatArena and AlpacaFarm-Human-Pref.\n",
      "*   Helpfulness Data: We leverage HelpSteer2 to create preference data.\n",
      "*   Reasoning: We mainly use Evol-instruct which contains preference pairs for complex instruction following, coding-related tasks.\n",
      "*   Safety: We employ PKU-SafeRLHF, which includes safety-related prompts paired with both safe and unsafe responses to form preference pairs.\n",
      "\n",
      "### Synthetic Preference Datasets:\n",
      "\n",
      "To incorporate additional preference supervision from different domains, we further include synthetic data using Llama-3.1 models. Specifically, for the math domain, we consider questions in GSM8K and the MATH dataset. In the safety domain, we generate synthetic prompts following the safety principles outlined in SafeRLHF\n",
      "\n",
      "## Evaluation Benchmarks\n",
      "\n",
      "In our experiments, we mainly evaluate on RewardBench, which contains a collection of prompt-chosen-rejected triplets across chat, reasoning, and safety domains, including 2985 examples in total. Beyond RewardBench, we also aim to test the out-of-distribution generalization ability of reward models. Specifically, we consider CrossEval, a recently proposed benchmark to evaluate the LLM’s capability in real-world interactions.\n",
      "\n",
      "## Ablation Studies\n",
      "\n",
      "Our data filtering strategy demonstrates that using the entire dataset without filtering leads to poor performance, particularly in the Chat-hard domain, which requires stronger reasoning capabilities for LLMs to assess response preferences accurately. Additionally, removing noisy preference pairs improves standard reward modeling. Moreover, incorporating summarization and ranking proves to be an effective approach for boosting overall performance.\n",
      "\n",
      "## Limitations and Future Directions\n",
      "\n",
      "While Critic-RM offers significant advancements, there are still areas for improvement:\n",
      "\n",
      "*   **Single Model Focus:** Testing Critic-RM across different LLM architectures could provide broader insights into its effectiveness.\n",
      "*   **Longer Inference Time:** Generating critiques during inference adds computational overhead.\n",
      "*   **No Iterative Training:** Adding iterative training steps could further improve reward modeling performance.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Critic-RM represents a significant step forward in reward modeling for language models. By harnessing the power of self-generated critiques, this framework enhances language models' ability to align with human preferences, leading to more helpful, honest, and harmless responses. As language models continue to evolve, self-critiquing techniques like Critic-RM will play a crucial role in ensuring their responsible and effective deployment.\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:12:17.909891Z",
     "start_time": "2025-04-11T19:12:14.627671Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})",
   "id": "8b97342ab63b79f9",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:12:17.936424Z",
     "start_time": "2025-04-11T19:12:17.932956Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")",
   "id": "110822fc4da1a1a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Good\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:10.856652Z",
     "start_time": "2025-04-11T19:13:10.852787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improvements = evaluator_response[\"parsed\"].improvements\n",
    "print(f\"Possible improvements:\")\n",
    "for i, improvement in enumerate(improvements):\n",
    "    print(f\"{i+1}. {improvement}\")"
   ],
   "id": "dc7f787c0ae4c674",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible improvements:\n",
      "1. Add more specific implementation details or technical aspects to increase informativeness.\n",
      "2. Define all technical AI/ML terms for better readability for a broader audience. \n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "c3beb73142bd99e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:12.481669Z",
     "start_time": "2025-04-11T19:13:12.459093Z"
    }
   },
   "source": [
    "possible_improvements = \"\\n\".join([f\"{i+1}. {improvement}\" for i, improvement in enumerate(improvements)])\n",
    "memory = LongTermMemory()\n",
    "blog = generator_response[\"parsed\"].text\n",
    "similar_blog, metadata = memory.retrieve_memory(blog)\n",
    "\n",
    "# Reflexion\n",
    "generation_chain = prompt_retry_with_memory_usage | llm_generator"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading long term memory module from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/long_term_memory\n",
      "Long term memory module loaded successfully.\n",
      "Found relevant memory.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:17.097070Z",
     "start_time": "2025-04-11T19:13:12.687411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator_response = generation_chain.invoke({\n",
    "    \"generated_blog\": blog,\n",
    "    \"possible_improvements\": possible_improvements,\n",
    "    \"similar_blog\": similar_blog,\n",
    "    \"similar_blog_score\":\n",
    "        metadata[\"overall_assessment\"],\n",
    "    \"similar_blog_improvements\":\n",
    "        metadata[\"improvements\"]\n",
    "})"
   ],
   "id": "7a593dd2daf85cf4",
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "29cde0c08e508057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:17.115395Z",
     "start_time": "2025-04-11T19:13:17.111535Z"
    }
   },
   "source": [
    "print(generator_response[\"parsed\"].text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Level Up Your Language Model with Self-Generated Critiques\n",
      "\n",
      "By Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Imagine a world where AI can not only generate text but also critique and improve its own work. That's the core idea behind **Critic-RM**, a novel approach that enhances language models by enabling them to learn from self-generated critiques. This leads to AI that better understands and aligns with what humans want, ultimately creating more helpful, honest, and harmless interactions. Think of a customer service chatbot that doesn't just answer your questions, but also identifies and corrects any misleading or unhelpful information in its response, ensuring a truly satisfying experience. This is especially useful when using Reinforcement Learning from Human Feedback (RLHF), a process where a model learns from feedback to better align with human preferences.\n",
      "\n",
      "Reward modeling plays a crucial role in aligning Large Language Models (LLMs) with human values. LLMs are sophisticated AI systems trained on vast amounts of text data, enabling them to generate human-like text, translate languages, and answer questions. Reward modeling is like teaching the AI what we like so it can generate better responses. Traditional methods often produce simple numerical scores, lacking detailed explanations or the ability to incorporate feedback in natural language. Critic-RM addresses this by allowing language models to generate both critiques and scalar rewards, significantly improving their ability to understand and rank preferences. Scalar rewards are single numerical values assigned to a response, indicating its quality.\n",
      "\n",
      "## What is Critic-RM?\n",
      "\n",
      "Critic-RM is a novel framework that trains reward models using self-generated, high-quality critiques, which in turn are used for scalar reward-based preference prediction. These critiques provide explicit rationales, offering supporting evidence for the model's decisions. It's like giving your language model a built-in editor that meticulously refines its responses and gains a deeper understanding of what makes a great answer. \n",
      "\n",
      "For example, imagine a language model summarizing a news article. With Critic-RM, it can self-assess whether the summary accurately captures the main points without bias. If not, it adjusts the summary accordingly. This constant self-evaluation leads to more reliable and accurate outputs. \n",
      "\n",
      "Specifically, Critic-RM leverages an instruction-finetuned LLM as the backbone. This LLM generates multiple candidate critiques, each with a discrete score, for individual responses. To ensure the quality of these critiques, a consistency-guided filtering technique is applied, retaining only critiques whose scores align with human-annotated preference labels. Furthermore, summarization and ranking strategies are employed to refine the critiques used in training the reward model. Summarization involves the LLM acting as a summarizer, identifying the most common feedback from different critiques, while ranking involves the LLM acting as a meta-judge, assigning evaluation scores to critiques and retaining the top-ranked ones.\n",
      "\n",
      "Want to learn more about how Critic-RM can improve language models? Read the full paper [here](link-to-paper).\n",
      "\n",
      "## Limitations\n",
      "\n",
      "While Critic-RM offers many advantages, it's important to acknowledge its limitations:\n",
      "\n",
      "*   **Dependency on Base LLM Quality:** Critic-RM's effectiveness relies on the underlying LLM's ability to generate insightful critiques. If the base LLM is not sufficiently trained or lacks the capacity to provide meaningful feedback, Critic-RM's performance may be limited. \n",
      "*   **Computational Overhead:** Generating critiques adds computational cost during inference. The increase in accuracy and alignment may justify the added computation, but this trade-off is important to consider, especially in real-time applications.\n",
      "\n",
      "## Let's Discuss!\n",
      "\n",
      "What are your thoughts on AI models critiquing their own work? How else can we improve language models to be more helpful and aligned with human values? Share your thoughts in the comments below! #NLP #AI #LanguageModels #RewardModeling #SelfSupervisedLearning\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "5206f48504f5deef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:20.068729Z",
     "start_time": "2025-04-11T19:13:17.226403Z"
    }
   },
   "source": [
    "blog_text = generator_response[\"parsed\"].text\n",
    "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "ec4b23503b229f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:20.115650Z",
     "start_time": "2025-04-11T19:13:20.111660Z"
    }
   },
   "source": [
    "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Very Good\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "a9ac6d3e8d32b56f",
   "metadata": {},
   "source": [
    "# End-to-end generation test"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb1b8f5fd61f0acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:13:39.086924Z",
     "start_time": "2025-04-11T19:13:39.075393Z"
    }
   },
   "source": "generator = BlogGenerator(use_memory=True, use_reflexion=True)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/vector_store\n",
      "Vector store loaded successfully.\n",
      "Loading long term memory module from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/long_term_memory\n",
      "Long term memory module loaded successfully.\n",
      "BlogGenerator initialized with vector store.\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "b85733eac47fe27c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:14:00.753174Z",
     "start_time": "2025-04-11T19:13:40.513781Z"
    }
   },
   "source": "blog = generator.generate_blog(Xval.loc[0, \"url_paper\"])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Extracting paper text from URL: https://arxiv.org/pdf/2411.16646\n",
      "Found most similar article.\n",
      "Attempt number 1: Generating blog...\n",
      "Using RAG prompt...\n",
      "Checking request limits before invoking the model...\n",
      "Invoking the model...\n",
      "Model invoked successfully. Total requests today: 1, RPM: 1, TPM: 36169\n",
      "Blog generated successfully.\n",
      "Checking request limits before invoking the model...\n",
      "Invoking the model...\n",
      "Model invoked successfully. Total requests today: 2, RPM: 2, TPM: 37996\n",
      "Blog evaluated successfully. Evaluation: Good.\n",
      "Found 2 similar blogs. Upsert blog to memory...\n",
      "Deleting blogs with ids: ['c52bfd62-5395-4ca7-90fb-b18a11b5061b']\n",
      "Blog generation attempt 1 unsuccessful. Retrying...\n",
      "Attempt number 2: Generating blog...\n",
      "Using retry prompt with Reflexion and memory module...\n",
      "Found relevant memory.\n",
      "Checking request limits before invoking the model...\n",
      "Invoking the model...\n",
      "Model invoked successfully. Total requests today: 3, RPM: 3, TPM: 42556\n",
      "Blog generated successfully.\n",
      "Checking request limits before invoking the model...\n",
      "Invoking the model...\n",
      "Model invoked successfully. Total requests today: 4, RPM: 4, TPM: 44698\n",
      "Blog evaluated successfully. Evaluation: Very Good.\n",
      "No similar blogs found, new blog is added to memory.\n",
      "Saving blog in file: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/generation_results/blog\n",
      "Long term memory module saved successfully.\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "e20ec1fbb312fcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T19:14:00.766787Z",
     "start_time": "2025-04-11T19:14:00.762744Z"
    }
   },
   "source": [
    "print(blog)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Self-Improving Language Models: How Synthetic Critiques Boost Reward Modeling\n",
      "\n",
      "By Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, and Rui Hou\n",
      "\n",
      "Imagine teaching a computer to not only generate text but also to critique its own work. That's the core idea behind our new research, which focuses on improving how we align large language models (LLMs) with human preferences. We introduce Critic-RM, a framework that uses self-generated critiques to train reward models. This leads to notable improvements in how well these models understand and rank preferences, as well as in their reasoning abilities.\n",
      "\n",
      "## The Challenge of Reward Modeling: Why It Matters\n",
      "\n",
      "Reward models are crucial for something called reinforcement learning from human feedback (RLHF). Think of RLHF as a way to guide LLMs—like the ones powering chatbots—to produce helpful, honest, and harmless responses. Reward models act like judges, providing scores that reflect how well an LLM's output aligns with what humans want. However, traditional reward models often give scores that are hard to understand, making it difficult to tap into the LLM's own language skills. This can lead to inefficiencies and make the models less reliable.\n",
      "\n",
      "## Introducing Critic-RM: Teaching Models to Critique Themselves\n",
      "\n",
      "Critic-RM offers a fresh approach. Instead of relying on separate, 'expert' LLMs to provide feedback, Critic-RM uses synthetic critiques—essentially, self-generated feedback—to enhance reward models. This happens without needing strong external guidance. Inspired by recent advancements in self-improving language models, Critic-RM integrates the ability to critique into the reward modeling process. Here’s a simplified breakdown of how it works:\n",
      "\n",
      "1.  **Critique Generation:** An instruction-finetuned LLM (a language model specifically trained to follow instructions) generates several potential critiques for a given response. Each critique includes a score.\n",
      "2.  **Consistency-Guided Filtering:** To ensure quality, we only keep the critiques where the scores align with human preferences.\n",
      "3.  **Quality-Aware Refinement:** We then refine these critiques using techniques called summarization and ranking, making them even more insightful.\n",
      "4.  **Joint Fine-Tuning:** Finally, the model is trained to predict rewards and generate critiques simultaneously, carefully balancing these two tasks.\n",
      "\n",
      "### How Critic-RM Works: A Closer Look\n",
      "\n",
      "*   **Generating Critiques:** We ask the LLM to create different critiques, evaluating the quality of each response it generates.\n",
      "*   **Filtering Noisy Critiques:** We filter out critiques that don't match human preferences, reducing the impact of incorrect reasoning.\n",
      "*   **Refining Critiques:** We use summarization and ranking methods to improve the quality of the remaining critiques.\n",
      "*   **Joint Training:** We train the model to both generate critiques and predict reward scores, allowing it to excel at both.\n",
      "\n",
      "## Key Benefits of Critic-RM: What Makes It Special\n",
      "\n",
      "*   **Improved Accuracy:** Our experiments show that Critic-RM improves reward modeling accuracy by 3.7%–7.3% compared to standard reward models and even LLMs acting as judges.\n",
      "*   **Enhanced Reasoning:** The critiques generated by Critic-RM help correct flawed reasoning, improving accuracy by 2.5%-3.2%.\n",
      "*   **Data Efficiency:** Critic-RM performs strongly and efficiently, even without needing a lot of training data.\n",
      "\n",
      "## Experimental Results: Proof in the Pudding\n",
      "\n",
      "We tested Critic-RM extensively on RewardBench and CrossEval, which are benchmarks used to evaluate reward models. The results showed that Critic-RM consistently outperformed other models, both in familiar scenarios and in new, unseen situations. This confirms that Critic-RM can effectively identify and use high-quality, self-generated critiques to generalize better.\n",
      "\n",
      "### Key Findings: What We Learned\n",
      "\n",
      "*   Critic-RM surpasses other models in both typical and novel evaluations.\n",
      "*   The model creates more accurate critiques compared to strong existing models.\n",
      "*   These critiques help the language model fix flawed reasoning, leading to improved accuracy in the refined responses.\n",
      "\n",
      "## Ablation Studies: Dissecting Critic-RM\n",
      "\n",
      "We also conducted ablation studies, which are like controlled experiments where we remove parts of Critic-RM to see how each component contributes:\n",
      "\n",
      "*   **Two-Stage Training:** A dynamic weighting system helps balance critique generation and reward learning, preventing the model from focusing too much on one aspect.\n",
      "*   **Data Filtering:** Removing preference pairs that are inconsistent improves performance, especially in difficult scenarios.\n",
      "*   **Critique Refinement:** Summarization and ranking enhance the quality of the critiques, leading to better overall performance.\n",
      "\n",
      "## Data Efficiency: Getting More from Less\n",
      "\n",
      "Critic-RM consistently outperforms other models, even when using limited amounts of training data. In fact, it only needs 10% of the labeled data to outperform a standard reward model.\n",
      "\n",
      "## Real-World Applications and Future Directions: Where Can This Go?\n",
      "\n",
      "Critic-RM has the potential to significantly improve how well LLMs align with human preferences in various real-world applications. Imagine chatbots that are more helpful and less prone to errors. Future research could explore:\n",
      "\n",
      "*   Testing Critic-RM with different LLM architectures.\n",
      "*   Reducing the time it takes to generate critiques.\n",
      "*   Using iterative training, where the model repeatedly critiques and improves itself.\n",
      "\n",
      "## Conclusion: A Step Towards Better Language Models\n",
      "\n",
      "Critic-RM offers a promising way to improve reward modeling by using self-generated critiques. By enhancing both the quality of critiques and the accuracy of reward prediction, Critic-RM paves the way for more aligned and reliable language models. We believe that self-critiquing techniques offer a promising future direction for advancing reward modeling and improving the alignment between LLMs and human preferences.\n",
      "\n",
      "To dive deeper into the details of Critic-RM, explore the complete paper [here](insert ArXiv link here once available).\n",
      "\n",
      "**In simple terms:** Critic-RM teaches AI models to critique their own work, leading to more accurate and reliable language generation. This approach improves the alignment of these models with human preferences, making them more helpful and safer for real-world applications.\n",
      "\n",
      "Stay tuned for more updates! #NLP #AI #LanguageModels #RewardModeling #SelfSupervisedLearning\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "id": "ca760e82f6293302",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T21:23:28.490375Z",
     "start_time": "2025-04-12T21:23:28.440588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results(column, assessments, best_assessments):\n",
    "    def create_new_json(error, res_type):\n",
    "        print(f\"{error}\\n\"\n",
    "              f\"Creating new json file for {res_type}...\")\n",
    "        json_file = {\n",
    "            \"RAG only\": [],\n",
    "            \"Reflexion\": [],\n",
    "            \"Reflexion + LTM\": []\n",
    "        }\n",
    "        return json_file\n",
    "\n",
    "    try:\n",
    "        with open(GENERATOR_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            results_generator_assessments = json.load(f)\n",
    "    except Exception as err:\n",
    "        results_generator_assessments = create_new_json(err, \"Generator assessments\")\n",
    "\n",
    "    try:\n",
    "        with open(GENERATOR_BEST_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            results_generator_best_assessments = json.load(f)\n",
    "    except Exception as err:\n",
    "        results_generator_best_assessments = create_new_json(err, \"Generator best assessments\")\n",
    "\n",
    "    results_generator_assessments[column] = assessments\n",
    "    results_generator_best_assessments[column] = best_assessments\n",
    "\n",
    "    with open(GENERATOR_ASSESSMENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_generator_assessments, f)\n",
    "    with open(GENERATOR_BEST_ASSESSMENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_generator_best_assessments, f)\n",
    "\n",
    "def run_experiment(history, best_history, dataset):\n",
    "    redo = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        assessment, best_assessment = generator.generate_blog(paper_url=row[\"url_paper\"])\n",
    "        if assessment and best_assessment:\n",
    "            history[index] = assessment\n",
    "            best_history[index] = best_assessment\n",
    "        else:\n",
    "            redo.append(index)\n",
    "    return redo\n",
    "\n",
    "def rerun_experiment_if_need(redo_list, history, best_history, dataset):\n",
    "    while redo_list:\n",
    "        redo_index = 0\n",
    "        for index in redo_list:\n",
    "            assessment, best_assessment = generator.generate_blog(paper_url=dataset.loc[index, \"url_paper\"])\n",
    "            if assessment and best_assessment:\n",
    "                history[index] = assessment\n",
    "                best_history[index] = best_assessment\n",
    "                del redo_list[redo_index]\n",
    "                continue\n",
    "            redo_index += 1\n",
    "\n",
    "MAX_ATTEMPTS = 5\n",
    "DATASET_SIZE = Xval.shape[0]\n",
    "generator = BlogGenerator()"
   ],
   "id": "5260266041bec0bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/vector_store\n",
      "Vector store loaded successfully.\n",
      "Loading long term memory module from: /home/kanstantsin-downar/PycharmProjects/bachalor-project/llm_blog_generator/data/long_term_memory\n",
      "Long term memory module loaded successfully.\n",
      "BlogGenerator initialized with vector store.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG only approach",
   "id": "c77bed12327234c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:55:20.370206Z",
     "start_time": "2025-04-12T19:55:20.366995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator.max_attempts = MAX_ATTEMPTS\n",
    "generator.experiment_mode = True\n",
    "generator.set_usage_of_reflexion(False)\n",
    "generator.set_usage_of_memory(False)"
   ],
   "id": "e89febdf9b5d4b9d",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assessment_history_only_RAG = [None] * DATASET_SIZE\n",
    "best_assessment_history_only_RAG = [None] * DATASET_SIZE\n",
    "redo_only_RAG = run_experiment(assessment_history_only_RAG, best_assessment_history_only_RAG, Xval)"
   ],
   "id": "66f70e68429e6065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Experiment should be conduct again for blogs with ids: {redo_only_RAG}\")",
   "id": "824cd3c1d495d540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rerun_experiment_if_need(redo_only_RAG, assessment_history_only_RAG, best_assessment_history_only_RAG, Xval)",
   "id": "458cf48903cd009a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T17:55:11.579822Z",
     "start_time": "2025-04-12T17:55:11.575374Z"
    }
   },
   "cell_type": "code",
   "source": "save_results(\"RAG only\", assessment_history_only_RAG, best_assessment_history_only_RAG)",
   "id": "1391aeeea4afd119",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reflexion",
   "id": "75d70b144b72e0ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T18:26:38.912657Z",
     "start_time": "2025-04-12T18:26:38.909945Z"
    }
   },
   "cell_type": "code",
   "source": "generator.set_usage_of_reflexion(True)",
   "id": "2fccaa31040aaa97",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assessment_history_ref = [None] * DATASET_SIZE\n",
    "best_assessment_history_ref = [None] * DATASET_SIZE\n",
    "redo_ref = run_experiment(assessment_history_ref, best_assessment_history_ref, Xval)"
   ],
   "id": "93f0ced475f854f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:24:56.060750Z",
     "start_time": "2025-04-12T19:24:56.056392Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Experiment should be conduct again for blogs with ids: {redo_ref}\")",
   "id": "5b37a502e60df3ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment should be conduct again for blogs with ids: [34, 45]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rerun_experiment_if_need(redo_ref, assessment_history_ref, best_assessment_history_ref, Xval)",
   "id": "d82a057b27132779",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:53:05.927171Z",
     "start_time": "2025-04-12T19:53:05.921400Z"
    }
   },
   "cell_type": "code",
   "source": "save_results(\"Reflexion\", assessment_history_ref, best_assessment_history_ref)",
   "id": "9baac8b13d29ac84",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reflexion + Long Term Memory",
   "id": "a1278a92032fbafe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "generator.set_usage_of_memory(True)",
   "id": "e0740ac9ffb375b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "assessment_history_with_memory_usage = [None] * DATASET_SIZE\n",
    "best_assessment_history_with_memory_usage = [None] * DATASET_SIZE\n",
    "redo_with_memory_usage = run_experiment(assessment_history_with_memory_usage, best_assessment_history_with_memory_usage, Xtest)"
   ],
   "id": "cc9f13b75d01aa12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(f\"Experiment should be conduct again for blogs with ids: {redo_with_memory_usage}\")",
   "id": "b4931d22ca8195ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "rerun_experiment_if_need(redo_with_memory_usage, assessment_history_with_memory_usage, best_assessment_history_with_memory_usage, Xtest)",
   "id": "213581e0e09a928"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_results(\"Reflexion + LTM\", assessment_history_with_memory_usage, best_assessment_history_with_memory_usage)",
   "id": "7fbcdca994aacd54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results",
   "id": "c82e898f90f4965"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:40:32.762056Z",
     "start_time": "2025-04-12T19:40:32.755982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot(results_rag, results_ref, results_with_memory_usage, results_type):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, 6), results_rag, marker=\"s\", label=\"RAG only\")\n",
    "    plt.plot(range(1, 6), results_ref, marker=\"o\", label=\"Reflexion\")\n",
    "    plt.plot(range(1, 6), results_with_memory_usage, marker=\"d\", label=\"Reflexion + LTM\")\n",
    "\n",
    "    distance_from_edge = 0.2\n",
    "    min_value = np.min([results_rag.min(), results_ref.min(), results_with_memory_usage.min()])\n",
    "    plt.yticks([1, 2, 3, 4, 5], [\"Bad\", \"Average\", \"Good\", \"Very Good\", \"Excellent\"])\n",
    "    plt.ylim(math.floor(min_value) - distance_from_edge, 5 + distance_from_edge)\n",
    "    plt.xticks(range(1, 6))\n",
    "\n",
    "    plt.xlabel(\"Number of Attempts\")\n",
    "    plt.ylabel(f\"Average Engagement Level\")\n",
    "    plt.title(f\"{results_type}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c2c325e625c85e0f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    with open(GENERATOR_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "        assessment_history = json.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\\n\"\n",
    "          f\"Results not founded.\")"
   ],
   "id": "4ba6718381026f22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:32:58.158061Z",
     "start_time": "2025-04-11T14:32:58.154334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag = np.array(assessment_history[\"RAG only\"])\n",
    "average_rag = np.mean(rag, axis=0)\n",
    "\n",
    "reflexion = np.array(assessment_history[\"Reflexion\"])\n",
    "average_reflexion = np.mean(reflexion, axis=0)\n",
    "\n",
    "ltm = np.array(assessment_history[\"Reflexion + LTM\"])\n",
    "average_ltm = np.mean(ltm, axis=0)\n",
    "\n",
    "plot(average_rag, average_reflexion, average_ltm, \"The Average Blog Engagement Level in This Attempt\")"
   ],
   "id": "9f9d62893bb19e40",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T19:35:31.281189Z",
     "start_time": "2025-04-12T19:35:31.277520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    with open(GENERATOR_BEST_ASSESSMENTS_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "        best_assessment_history = json.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\\n\"\n",
    "          f\"Results not founded.\")"
   ],
   "id": "c3e9a7b37e570349",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rag_best = np.array(best_assessment_history[\"RAG only\"])\n",
    "average_rag_best = np.mean(rag_best, axis=0)\n",
    "\n",
    "reflexion_best = np.array(best_assessment_history[\"Reflexion\"])\n",
    "average_reflexion_best = np.mean(reflexion_best, axis=0)\n",
    "\n",
    "ltm_best = np.array(best_assessment_history[\"Reflexion + LTM\"])\n",
    "average_ltm_best = np.mean(ltm_best, axis=0)\n",
    "\n",
    "plot(average_rag_best, average_reflexion_best, average_ltm_best, \"The Average Engagement Level of The Best Blog Generated Up To and Including This Attempt\")"
   ],
   "id": "a2452043568d3ad9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
