{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:40:30.078173Z",
     "start_time": "2025-03-28T20:40:30.073485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path().resolve().parent) not in sys.path:\n",
    "    sys.path.append(str(Path().resolve().parent))"
   ],
   "id": "96d4c1b9bfd31448",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Processing",
   "id": "1359c2b435d04127"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:40:31.571165Z",
     "start_time": "2025-03-28T20:40:30.305340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.config import random_seed, PREPROCESSED_BLOG_DATASET_PATH\n",
    "\n",
    "# Import data\n",
    "blogs = pd.read_csv(PREPROCESSED_BLOG_DATASET_PATH)\n",
    "\n",
    "# Split dataset into validation and test set\n",
    "Xval, Xtest, yval_score, ytest_score = train_test_split(\n",
    "    blogs.drop(columns=['normalized_engagement_score']), blogs[\"normalized_engagement_score\"],\n",
    "    test_size=0.4, random_state=random_seed)\n",
    "\n",
    "# Same Xval, Xtest; new explained variable \"engagement_level\"\n",
    "Xval, Xtest, yval_level, ytest_level = train_test_split(\n",
    "    blogs.drop(columns=['engagement_level', 'normalized_engagement_score']), blogs[\"engagement_level\"],\n",
    "    test_size=0.4, random_state=random_seed)\n",
    "\n",
    "print(f\"Size of validation set, X: {Xval.shape}, y: {yval_score.shape}\")\n",
    "print(f\"Size of test set, X: {Xtest.shape}, y: {ytest_score.shape}\")"
   ],
   "id": "3ca8de9f51404a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation set, X: (30, 10), y: (30,)\n",
      "Size of test set, X: (20, 10), y: (20,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:40:31.646537Z",
     "start_time": "2025-03-28T20:40:31.629081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "valid_blogs = blogs[blogs[\"engagement_level\"].isin([\"Good\", \"Very Good\", \"Excellent\"])].copy()\n",
    "valid_blogs.info()"
   ],
   "id": "5a639a1c9d723e17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28 entries, 0 to 47\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           28 non-null     int64  \n",
      " 1   title_blog                   28 non-null     object \n",
      " 2   url_blog                     28 non-null     object \n",
      " 3   author_blog                  28 non-null     object \n",
      " 4   author_followers             28 non-null     int64  \n",
      " 5   claps                        28 non-null     int64  \n",
      " 6   comments                     28 non-null     int64  \n",
      " 7   title_paper                  28 non-null     object \n",
      " 8   url_paper                    28 non-null     object \n",
      " 9   engagement_score             28 non-null     float64\n",
      " 10  normalized_engagement_score  28 non-null     float64\n",
      " 11  engagement_level             28 non-null     object \n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:40:43.453410Z",
     "start_time": "2025-03-28T20:40:31.686358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.text_extraction import *\n",
    "\n",
    "valid_blogs[\"full_paper\"] = valid_blogs[\"url_paper\"].apply(extract_paper_text)"
   ],
   "id": "e477307db5c1f4ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vector storage",
   "id": "f69939f38ef89093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:40:59.441940Z",
     "start_time": "2025-03-28T20:40:43.485794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "VECTOR_STORE_PATH = \"../data/vector_store\"\n",
    "\n",
    "texts = valid_blogs['full_paper'].tolist()\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "elements = []\n",
    "for text, blog_url, author, claps, comments in zip(valid_blogs[\"full_paper\"],\n",
    "                                                   valid_blogs[\"url_blog\"],\n",
    "                                                   valid_blogs[\"author_blog\"],\n",
    "                                                   valid_blogs[\"claps\"],\n",
    "                                                   valid_blogs[\"comments\"]):\n",
    "    embedding = model.encode(text, clean_up_tokenization_spaces=True)\n",
    "    metadata = {\n",
    "        \"full_text\": text,\n",
    "        \"blog_url\": blog_url,\n",
    "        \"author\": author,\n",
    "        \"claps\": claps,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "    elements.append((embedding, metadata))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "vector_store = FAISS.from_texts(\n",
    "    texts=[element[1][\"full_text\"] for element in elements],\n",
    "    embedding=embedding_model,\n",
    "    metadatas=[element[1] for element in elements]\n",
    ")\n",
    "\n",
    "vector_store.save_local(VECTOR_STORE_PATH)"
   ],
   "id": "fd1d625972aaf063",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:40:59.497974Z",
     "start_time": "2025-03-28T20:40:59.486606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "def find_most_similar_article(query_text):\n",
    "    query_embedding = model.encode(query_text, clean_up_tokenization_spaces=True)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=2)\n",
    "\n",
    "    if results:\n",
    "        most_similar = results[1]\n",
    "        return most_similar.metadata"
   ],
   "id": "e32bee0d3b513143",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generator test",
   "id": "aef7c3d1f21f0f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:00.291159Z",
     "start_time": "2025-03-28T20:40:59.538290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ],
   "id": "b5dcdb1ae70e10f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:28.985506Z",
     "start_time": "2025-03-28T20:41:00.331332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.prompts import prompt_rag\n",
    "from src.models_setup import gemini_2_flash\n",
    "from src.output_formats import BlogGeneration\n",
    "\n",
    "paper_text = extract_paper_text(blogs.loc[0, \"url_paper\"])\n",
    "most_similar_article = find_most_similar_article(paper_text)\n",
    "example_blog = extract_blog_text(url_blog=most_similar_article[\"blog_url\"],\n",
    "                                 author_blog=most_similar_article[\"author\"],\n",
    "                                 claps=most_similar_article[\"claps\"],\n",
    "                                 comments=most_similar_article[\"comments\"])\n",
    "\n",
    "# RAG approach\n",
    "llm_generator = gemini_2_flash.with_structured_output(BlogGeneration, include_raw=True)\n",
    "generation_chain = prompt_rag | llm_generator\n",
    "generator_response = generation_chain.invoke({\"paper_text\": paper_text,\n",
    "                                              \"example_paper\": most_similar_article[\"full_text\"],\n",
    "                                              \"example_blog\": example_blog})"
   ],
   "id": "63392ea1c21d422c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:29.034307Z",
     "start_time": "2025-03-28T20:41:29.029987Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator_response[\"parsed\"].text)",
   "id": "ec3569e9a5374a69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "## Preface\n",
      "Large language models have revolutionized the field of artificial intelligence, offering a universal model capable of handling diverse problems through large-scale language modeling tasks. This blog post outlines the basic concepts and techniques related to these models, focusing on foundational aspects rather than cutting-edge methods.\n",
      "\n",
      "## Key Concepts\n",
      "- **Pre-training:** The foundation of large language models, involving common pre-training methods and model architectures.\n",
      "- **Generative Models:** The large language models we commonly use today, exploring their construction, scaling, and handling of long texts.\n",
      "- **Prompting Methods:** Strategies for effective prompting, including chain-of-thought reasoning and automatic prompt design.\n",
      "- **Alignment Methods:** Techniques for instruction fine-tuning and alignment based on human feedback.\n",
      "\n",
      "## Pre-training\n",
      "Pre-training involves optimizing a neural network before applying it to specific tasks, enabling generalization across various problems. Key aspects include:\n",
      "\n",
      "- **Unsupervised, Supervised, and Self-supervised Pre-training:**\n",
      "  - Unsupervised learning optimizes the neural network using criteria not directly related to specific tasks.\n",
      "  - Supervised pre-training trains the model on supervised learning tasks before adapting it to downstream tasks.\n",
      "  - Self-supervised learning trains the model using supervision signals generated by itself, creating training tasks from unlabeled data.\n",
      "\n",
      "- **Adapting Pre-trained Models:**\n",
      "  - Fine-tuning pre-trained models using labeled data to adjust the parameters for specific tasks.\n",
      "  - Prompting pre-trained models to transform NLP problems into simple text generation tasks.\n",
      "\n",
      "## Generative Models\n",
      "Large language models are generative models trained on vast amounts of data. Key components include:\n",
      "\n",
      "- **Decoder-only Transformers:** Models that predict the distribution of tokens at a position given preceding tokens.\n",
      "- **Training LLMs:** Optimizing model parameters to minimize loss on a set of token sequences.\n",
      "- **Fine-tuning LLMs:** Adjusting the model to the task by explicit labeling in downstream tasks.\n",
      "- **Aligning LLMs with the World:** Guaranteeing that tool-learning models provide responses that align with human preferences.\n",
      "- **Prompting LLMs:** Transforming language understanding and generation capabilities to solve NLP problems.\n",
      "\n",
      "## Training at Scale\n",
      "Training large language models at scale requires careful data preparation and model modifications.\n",
      "\n",
      "- **Data Preparation:**\n",
      "  - Ensuring data quality through filtering and cleaning techniques.\n",
      "  - Enhancing data diversity by including various scenarios and contexts.\n",
      "- **Model Modifications:**\n",
      "  - Optimizing models by using low-precision data types and distributed training strategies.\n",
      "  - Improving models by dynamic adjustments of the model’s architecture.\n",
      "\n",
      "## Prompting\n",
      "Prompting is critical for guiding large language models to generate desired outputs. Key strategies include:\n",
      "\n",
      "- **General Prompt Design:**\n",
      "  - Basics: Use clear and concise instructions.\n",
      "  - In-context Learning: Add examples demonstrating how an input corresponds to an output.\n",
      "  - Prompt Engineering Strategies: Apply various techniques to improve the task.\n",
      "- **Advanced Prompting Methods:**\n",
      "  - Chain of Thought: Decompose complex problems into smaller steps.\n",
      "  - Problem Decomposition: Break down complex problems into smaller, manageable tasks.\n",
      "  - Self-refinement: Allow the model to re-evaluate and refine its outputs iteratively.\n",
      "  - Ensembling: Combine multiple models to improve output reliability.\n",
      "  - RAG and Tool Use: Use external resources to enhance the accuracy of model responses.\n",
      "- **Learning to Prompt:**\n",
      "  - Prompt Optimization: Use algorithms to automatically design and refine prompts.\n",
      "  - Soft Prompts: Use non-textual prompts that can be learned as parameters.\n",
      "  - Prompt Length Reduction: Compress prompts to reduce computational costs.\n",
      "\n",
      "## Alignment\n",
      "LLM alignment ensures that the model behaves in ways that align with human values and preferences. Key methods include:\n",
      "\n",
      "- **Instruction Alignment:** Fine-tuning models with instruction-following data.\n",
      "- **Human Preference Alignment:** Using human feedback to guide the learning process.\n",
      "\n",
      "## Conclusion\n",
      "Large language models have demonstrated significant progress in natural language processing. By understanding the foundational aspects and related techniques, we can continue to improve these models and unlock their full potential.\n",
      "\n",
      "If you found this blog post helpful, feel free to share it and explore the full research paper for more in-depth information!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:31.872416Z",
     "start_time": "2025-03-28T20:41:29.080211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.helpers import get_examples\n",
    "from src.output_formats import BlogClassification\n",
    "from src.prompts import prompt_five_shots\n",
    "\n",
    "examples = get_examples()\n",
    "blog_text = generator_response[\"parsed\"].text\n",
    "\n",
    "llm_evaluator = gemini_2_flash.with_structured_output(BlogClassification, include_raw=True)\n",
    "evaluation_chain = prompt_five_shots | llm_evaluator\n",
    "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})"
   ],
   "id": "3dddc49b0f625b8c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:31.929760Z",
     "start_time": "2025-03-28T20:41:31.923210Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")",
   "id": "ca2fead73c6dfbd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Average\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:31.982840Z",
     "start_time": "2025-03-28T20:41:31.976241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improvements = evaluator_response[\"parsed\"].improvements\n",
    "print(f\"Possible improvements:\")\n",
    "for i, improvement in enumerate(improvements):\n",
    "    print(f\"{i+1}. {improvement}\")"
   ],
   "id": "f65a3ea64157fed0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible improvements:\n",
      "1. Add specific examples and case studies to illustrate the concepts discussed.\n",
      "2. Provide more in-depth explanations of the techniques mentioned.\n",
      "3. Incorporate visuals or diagrams to enhance understanding.\n",
      "4. Make the title more engaging and specific to attract a wider audience.\n",
      "5. Include a call to action to encourage discussion and feedback from readers.\n",
      "6. Add a section on the limitations and challenges of current AI agent architectures.\n",
      "7. Elaborate on the ethical considerations associated with AI agent development and deployment.\n",
      "8. Include a section discussing future trends and potential research directions in the field.\n",
      "9. Consider tailoring the content to a more specific audience (e.g., researchers, practitioners, or general enthusiasts).\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:41.685942Z",
     "start_time": "2025-03-28T20:41:32.060193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.prompts import prompt_retry\n",
    "\n",
    "possible_improvements = \"\\n\".join([f\"{i+1}. {improvement}\" for i, improvement in enumerate(improvements)])\n",
    "\n",
    "# Reflexion\n",
    "generation_chain = prompt_retry | llm_generator\n",
    "generator_response = generation_chain.invoke({\"generated_blog\": blog_text,\n",
    "                                              \"possible_improvements\": possible_improvements})"
   ],
   "id": "7a593dd2daf85cf4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:41.790294Z",
     "start_time": "2025-03-28T20:41:41.783537Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator_response[\"parsed\"].text)",
   "id": "29cde0c08e508057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Navigating the AI Agent Landscape: Reasoning, Planning, and Tool Calling\n",
      "\n",
      "### Preface\n",
      "Large language models (LLMs) have revolutionized artificial intelligence, offering a versatile approach to diverse problems through language modeling. This blog post explores the core concepts and techniques behind these models, focusing on fundamental aspects relevant to researchers, practitioners, and enthusiasts alike.\n",
      "\n",
      "### Key Concepts\n",
      "- **Pre-training:** The bedrock of LLMs, covering common methods and architectures.\n",
      "- **Generative Models:** Exploring the construction, scaling, and long-text handling capabilities of modern LLMs.\n",
      "- **Prompting Methods:** Strategies for effective prompting, including chain-of-thought reasoning and automated prompt design.\n",
      "- **Alignment Methods:** Techniques for instruction fine-tuning and human feedback alignment.\n",
      "\n",
      "### Pre-training\n",
      "Pre-training optimizes a neural network before task-specific applications, enabling broad generalization. Key aspects include:\n",
      "\n",
      "- **Unsupervised, Supervised, and Self-supervised Pre-training:**\n",
      "  - **Unsupervised Learning:** Optimizes networks using criteria unrelated to specific tasks. For example, autoencoders learn to compress and reconstruct input data.\n",
      "  - **Supervised Pre-training:** Trains models on labeled data before adapting them to downstream tasks. A classic example is training on ImageNet before fine-tuning for object detection.\n",
      "  - **Self-supervised Learning:** Generates supervision signals from unlabeled data. BERT, for instance, uses masked word prediction.\n",
      "\n",
      "- **Adapting Pre-trained Models:**\n",
      "  - **Fine-tuning:** Adjusts model parameters using labeled data for specific tasks, such as sentiment analysis or machine translation.\n",
      "  - **Prompting:** Transforms NLP problems into text generation tasks. For example, a question-answering task can be framed as \"Answer the following question: ...\"\n",
      "\n",
      "### Generative Models\n",
      "LLMs are generative models trained on massive datasets. Key components include:\n",
      "\n",
      "- **Decoder-only Transformers:** Models predicting token distributions based on preceding tokens, like GPT-3.\n",
      "- **Training LLMs:** Optimizing model parameters to minimize loss on token sequences. This involves techniques like backpropagation and gradient descent.\n",
      "- **Fine-tuning LLMs:** Adapting models to specific tasks via explicit labeling. For instance, fine-tuning a model for medical text summarization.\n",
      "- **Aligning LLMs with the World:** Ensuring tool-learning models produce responses aligned with human preferences. This often involves reinforcement learning from human feedback (RLHF).\n",
      "- **Prompting LLMs:** Leveraging language understanding and generation to solve NLP problems. Examples include using prompts for text completion or creative writing.\n",
      "\n",
      "### Training at Scale\n",
      "Training LLMs at scale demands careful data preparation and model modifications.\n",
      "\n",
      "- **Data Preparation:**\n",
      "  - **Ensuring Data Quality:** Filtering and cleaning techniques to remove noise and errors.\n",
      "  - **Enhancing Data Diversity:** Including various scenarios and contexts to improve generalization.\n",
      "\n",
      "- **Model Modifications:**\n",
      "  - **Optimizing Models:** Using low-precision data types (e.g., FP16) and distributed training strategies (e.g., data parallelism) to reduce memory footprint and accelerate training.\n",
      "  - **Improving Models:** Dynamically adjusting the model's architecture during training, such as conditional computation.\n",
      "\n",
      "### Prompting\n",
      "Prompting is crucial for guiding LLMs to generate desired outputs. Key strategies include:\n",
      "\n",
      "- **General Prompt Design:**\n",
      "  - **Basics:** Using clear and concise instructions. For example, \"Summarize the following text.\"\n",
      "  - **In-context Learning:** Providing examples demonstrating input-output correspondence. For instance, showing a few question-answer pairs before asking a new question.\n",
      "  - **Prompt Engineering Strategies:** Applying techniques like role-playing or using specific keywords to improve task performance.\n",
      "\n",
      "- **Advanced Prompting Methods:**\n",
      "  - **Chain of Thought (CoT):** Decomposing complex problems into smaller steps. For example, showing the model how to solve a math problem step-by-step.\n",
      "  - **Problem Decomposition:** Breaking down complex problems into manageable tasks. For example, splitting a document summarization task into sentence extraction and synthesis.\n",
      "  - **Self-refinement:** Allowing the model to iteratively re-evaluate and refine its outputs. This involves the model critiquing its own work and suggesting improvements.\n",
      "  - **Ensembling:** Combining multiple models to improve output reliability and reduce variance.\n",
      "  - **RAG (Retrieval-Augmented Generation) and Tool Use:** Using external resources to enhance accuracy. For instance, retrieving relevant documents to answer a question.\n",
      "\n",
      "- **Learning to Prompt:**\n",
      "  - **Prompt Optimization:** Using algorithms to automatically design and refine prompts. Techniques include gradient-based optimization and reinforcement learning.\n",
      "  - **Soft Prompts:** Using non-textual prompts learned as parameters. These prompts are typically vectors in the embedding space.\n",
      "  - **Prompt Length Reduction:** Compressing prompts to reduce computational costs. Techniques include summarization and knowledge distillation.\n",
      "\n",
      "### Alignment\n",
      "LLM alignment ensures that models behave in ways aligned with human values. Key methods include:\n",
      "\n",
      "- **Instruction Alignment:** Fine-tuning models with instruction-following data. This involves training models to follow a wide range of instructions.\n",
      "- **Human Preference Alignment:** Using human feedback to guide the learning process, often via reinforcement learning (RLHF).\n",
      "\n",
      "### Limitations and Challenges\n",
      "Despite their advancements, LLMs face limitations:\n",
      "\n",
      "- **Bias and Fairness:** LLMs can perpetuate biases present in their training data.\n",
      "- **Factuality:** LLMs may generate incorrect or misleading information.\n",
      "- **Computational Cost:** Training and deploying large models can be expensive.\n",
      "\n",
      "### Ethical Considerations\n",
      "Ethical considerations are paramount:\n",
      "\n",
      "- **Responsible Use:** Preventing misuse of LLMs for malicious purposes.\n",
      "- **Transparency:** Understanding how LLMs make decisions.\n",
      "- **Privacy:** Protecting user data and ensuring data security.\n",
      "\n",
      "### Future Trends\n",
      "Future research directions include:\n",
      "\n",
      "- **Improving Efficiency:** Developing more efficient model architectures.\n",
      "- **Enhancing Reasoning:** Improving the reasoning capabilities of LLMs.\n",
      "- **Expanding Context:** Enabling LLMs to process longer contexts.\n",
      "\n",
      "### Conclusion\n",
      "LLMs have demonstrated significant progress in NLP. By understanding the foundational aspects and related techniques, we can continue to improve these models and unlock their full potential.\n",
      "\n",
      "We encourage you to share this blog post and explore the full research paper for more information. We welcome your thoughts and feedback in the comments below! What are the most promising applications of LLMs you see on the horizon? Let's discuss!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:44.546882Z",
     "start_time": "2025-03-28T20:41:41.865330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blog_text = generator_response[\"parsed\"].text\n",
    "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})"
   ],
   "id": "5206f48504f5deef",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:41:44.566208Z",
     "start_time": "2025-03-28T20:41:44.561162Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")",
   "id": "ec4b23503b229f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Good\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
