{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:45:38.364428Z",
     "start_time": "2025-03-29T18:45:38.360433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path().resolve().parent) not in sys.path:\n",
    "    sys.path.append(str(Path().resolve().parent))"
   ],
   "id": "96d4c1b9bfd31448",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Processing",
   "id": "1359c2b435d04127"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T19:43:51.378986Z",
     "start_time": "2025-03-29T19:43:50.394106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.config import random_seed, PREPROCESSED_BLOG_DATASET_PATH\n",
    "\n",
    "# Import data\n",
    "blogs = pd.read_csv(PREPROCESSED_BLOG_DATASET_PATH)\n",
    "\n",
    "# Split dataset into validation and test set\n",
    "Xval, Xtest, yval_score, ytest_score = train_test_split(\n",
    "    blogs.drop(columns=['normalized_engagement_score']), blogs[\"normalized_engagement_score\"],\n",
    "    test_size=0.4, random_state=random_seed)\n",
    "\n",
    "# Same Xval, Xtest; new explained variable \"engagement_level\"\n",
    "Xval, Xtest, yval_level, ytest_level = train_test_split(\n",
    "    blogs.drop(columns=['engagement_level', 'normalized_engagement_score']), blogs[\"engagement_level\"],\n",
    "    test_size=0.4, random_state=random_seed)\n",
    "\n",
    "print(f\"Size of validation set, X: {Xval.shape}, y: {yval_score.shape}\")\n",
    "print(f\"Size of test set, X: {Xtest.shape}, y: {ytest_score.shape}\")"
   ],
   "id": "3ca8de9f51404a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation set, X: (30, 10), y: (30,)\n",
      "Size of test set, X: (20, 10), y: (20,)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:45:40.913612Z",
     "start_time": "2025-03-29T18:45:40.902802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "valid_blogs = blogs[blogs[\"engagement_level\"].isin([\"Good\", \"Very Good\", \"Excellent\"])].copy()\n",
    "valid_blogs.info()"
   ],
   "id": "5a639a1c9d723e17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28 entries, 0 to 47\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           28 non-null     int64  \n",
      " 1   title_blog                   28 non-null     object \n",
      " 2   url_blog                     28 non-null     object \n",
      " 3   author_blog                  28 non-null     object \n",
      " 4   author_followers             28 non-null     int64  \n",
      " 5   claps                        28 non-null     int64  \n",
      " 6   comments                     28 non-null     int64  \n",
      " 7   title_paper                  28 non-null     object \n",
      " 8   url_paper                    28 non-null     object \n",
      " 9   engagement_score             28 non-null     float64\n",
      " 10  normalized_engagement_score  28 non-null     float64\n",
      " 11  engagement_level             28 non-null     object \n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:45:51.810242Z",
     "start_time": "2025-03-29T18:45:41.410339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.text_extraction import *\n",
    "\n",
    "valid_blogs[\"full_paper\"] = valid_blogs[\"url_paper\"].apply(extract_paper_text)"
   ],
   "id": "e477307db5c1f4ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vector storage",
   "id": "f69939f38ef89093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:46:10.350234Z",
     "start_time": "2025-03-29T18:45:57.052639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from langchain.vectorstores import FAISS\n",
    "from src.models_setup import embedding_model, langchain_embedding_model\n",
    "from src.config import VECTOR_STORE_PATH\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "elements = []\n",
    "for text, blog_url, author, claps, comments in zip(valid_blogs[\"full_paper\"],\n",
    "                                                   valid_blogs[\"url_blog\"],\n",
    "                                                   valid_blogs[\"author_blog\"],\n",
    "                                                   valid_blogs[\"claps\"],\n",
    "                                                   valid_blogs[\"comments\"]):\n",
    "    embedding = embedding_model.encode(text, clean_up_tokenization_spaces=True)\n",
    "    metadata = {\n",
    "        \"full_text\": text,\n",
    "        \"blog_url\": blog_url,\n",
    "        \"author\": author,\n",
    "        \"claps\": claps,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "    elements.append((embedding, metadata))\n",
    "\n",
    "vector_store = FAISS.from_texts(\n",
    "    texts=[element[1][\"full_text\"] for element in elements],\n",
    "    embedding=langchain_embedding_model,\n",
    "    metadatas=[element[1] for element in elements]\n",
    ")\n",
    "\n",
    "vector_store.save_local(VECTOR_STORE_PATH)"
   ],
   "id": "fd1d625972aaf063",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 19:46:01,583 - INFO - Successfully loaded Google API key from environment variables.\n",
      "2025-03-29 19:46:01,584 - INFO - Google API successfully configured with the API key.\n",
      "2025-03-29 19:46:01,671 - INFO - Use pytorch device_name: cuda\n",
      "2025-03-29 19:46:01,671 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/home/kanstantsin-downar/virtualPython/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2025-03-29 19:46:03,600 - INFO - Use pytorch device_name: cuda\n",
      "2025-03-29 19:46:03,600 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc6366331a01435f8deb03acd8a8ce4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ef516caa1a744a59f3c9392d547c5f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fd23a7b263044d68d394931a683b30a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af663de1794d4438a9dcc4f51211ecfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "688995fd5d35449ba4e6ed40ac0b25da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b93550765714886b5a7e87654788102"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eef745d5078d4384b429ef926bc2c00f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c82e58f0e6d943b0a8ca9611cebf5793"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f26a10de7c5246da9fc950bb48ef7860"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75dba0b7f283496f96b9bed547586a0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e57ae211eb114beeafd4d9636cecd275"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57460f693c144f9cbbaabb2a32afe195"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "889f01adf2c144ca88672b24dad0c376"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e351fc316ad473fb7656bf07e9dd97e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b74fa8bfa6344aba31e76e1a3c12a00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d71b6af25a3a48548ddf3028309aa41e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ddfa3053aac4999ba978435d9cf7997"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ce4a60a703c44168c519d79435bec9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8b9b5d4f8ec444187c037fb6afc67b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3677b1b12c10487697ed10e00f7e2e66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "963a285d79734f8a8a42325a0f24db32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efb82730427b41c485bda29bb92ef0fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f36d6c06da494a9795b14b61836d9d0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cabde18fcdb74bad80b8767ae59885f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07ccf6f0dc0e41118c4390771306081e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25ffe7330a814257934626bdb065bdc4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c3068eca5374ae49240b60448080f98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb973ab5f2ac42b6978a328cc542bf54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 19:46:10,273 - INFO - Loading faiss with AVX2 support.\n",
      "2025-03-29 19:46:10,323 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-03-29 19:46:10,331 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:48:07.466770Z",
     "start_time": "2025-03-29T18:48:07.457171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = FAISS.load_local(VECTOR_STORE_PATH,\n",
    "                                embeddings=langchain_embedding_model,\n",
    "                                allow_dangerous_deserialization=True)\n",
    "\n",
    "def find_most_similar_article(query_text):\n",
    "    query_embedding = embedding_model.encode(query_text, clean_up_tokenization_spaces=True)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=2)\n",
    "\n",
    "    if results:\n",
    "        most_similar = results[1]\n",
    "        return most_similar.metadata"
   ],
   "id": "e32bee0d3b513143",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generator manual test",
   "id": "aef7c3d1f21f0f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:48:10.597326Z",
     "start_time": "2025-03-29T18:48:10.591984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ],
   "id": "b5dcdb1ae70e10f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:50:12.426226Z",
     "start_time": "2025-03-29T18:49:55.816935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.prompts import prompt_rag\n",
    "from src.models_setup import gemini_2_flash\n",
    "from src.output_formats import BlogGeneration\n",
    "\n",
    "paper_text = extract_paper_text(blogs.loc[0, \"url_paper\"])\n",
    "most_similar_article = find_most_similar_article(paper_text)\n",
    "example_blog = extract_blog_text(url_blog=most_similar_article[\"blog_url\"],\n",
    "                                 author_blog=most_similar_article[\"author\"],\n",
    "                                 claps=most_similar_article[\"claps\"],\n",
    "                                 comments=most_similar_article[\"comments\"])\n",
    "\n",
    "# RAG approach\n",
    "llm_generator = gemini_2_flash.with_structured_output(BlogGeneration, include_raw=True)\n",
    "generation_chain = prompt_rag | llm_generator\n",
    "generator_response = generation_chain.invoke({\"paper_text\": paper_text,\n",
    "                                              \"example_paper\": most_similar_article[\"full_text\"],\n",
    "                                              \"example_blog\": example_blog})"
   ],
   "id": "63392ea1c21d422c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07d94329519145aabf4eedea54511236"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:50:14.810548Z",
     "start_time": "2025-03-29T18:50:14.807196Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator_response[\"parsed\"].text)",
   "id": "ec3569e9a5374a69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey by Tong Xiao and Jingbo Zhu: A Deep Dive into LLMs\n",
      "\n",
      "TDS Archive\n",
      "Large language models (LLMs) have revolutionized the ﬁeld of artiﬁcial intelligence, particularly in natural language processing. A recent book by Tong Xiao and Jingbo Zhu, serves as a comprehensive guide to the fundamental concepts and techniques behind these groundbreaking models.\n",
      "\n",
      "# The Origin and Impact of Large Language Models\n",
      "LLMs, originating from natural language processing, have become a revolutionary force in artiﬁcial intelligence. Their key innovation lies in acquiring world and language knowledge through large-scale language modeling tasks, creating universal models capable of addressing diverse problems. This approach has profoundly reshaped research methodologies in natural language processing and related ﬁelds, shifting from specialized systems trained from scratch to foundation models obtained through extensive pre-training, ﬁne-tuning, alignment, and prompting.\n",
      "\n",
      "# Core Concepts Covered\n",
      "Xiao and Zhu\\\"s book outlines the basic concepts of large language models and introduces related techniques. The book is divided into four chapters:\n",
      "\n",
      "1.  Pre-training: Covering the basics of pre-training, common pre-training methods, and model architectures.\n",
      "2.  Generative Models: Discussing generative models, the process of building them, scaling up model training, and handling long texts.\n",
      "3.  Prompting Methods: Exploring various prompting strategies, including chain-of-thought reasoning and automatic prompt design.\n",
      "4.  Alignment Methods: Focusing on instruction ﬁne-tuning and alignment based on human feedback.\n",
      "\n",
      "# What is Pre-training and Why Is It Important?\n",
      "Pre-training involves training a neural network on a task different from the one it will ultimately perform. The model is then adapted to the new task through ﬁne-tuning or prompting.\n",
      "\n",
      "There are three primary methods for pre-training neural networks:\n",
      "\n",
      "*   Unsupervised Pre-training: Optimizes the neural network using criteria not directly related to speciﬁc tasks.\n",
      "*   Supervised Pre-training: Trains a neural network on supervised learning tasks before adapting it to a downstream task.\n",
      "*   Self-supervised Learning: Trains a neural network using supervision signals generated by itself.\n",
      "\n",
      "# The Power of Generative Models\n",
      "Large language models are generative models trained on vast amounts of data, allowing them to perform remarkably well in token prediction. This capability enables the transformation of numerous NLP problems into simple text generation tasks through prompting.\n",
      "\n",
      "Key steps in building and applying LLMs:\n",
      "\n",
      "1.  Data Preparation: Gathering and preparing large datasets for training.\n",
      "2.  Model Modiﬁcations: Adjusting model architectures to improve training stability and performance.\n",
      "3.  Distributed Training: Using distributed systems to handle the computational demands of training large models.\n",
      "4.  Scaling Laws: Understanding how model performance scales with factors like model size and training data.\n",
      "\n",
      "# Prompting: Guiding LLMs to Success\n",
      "Prompting involves transforming various NLP problems into text generation tasks by providing specific instructions to large language models. Methods:\n",
      "\n",
      "*   Zero-Shot Learning: Solving NLP problems without additional training by using instruction-based prompts.\n",
      "*   Few-Shot Learning: Demonstrating task-solving through in-context examples to LLMs.\n",
      "\n",
      "# Alignment: Ensuring Ethical and Safe LLMs\n",
      "Alignment focuses on ensuring that LLMs behave in ways that align with human intentions, including being unbiased, truthful, and harmless. Techniques include:\n",
      "\n",
      "*   Instruction Alignment: Fine-tuning LLMs to generate accurate and relevant responses.\n",
      "*   Human Preference Alignment: Using reinforcement learning from human feedback (RLHF) to train LLMs based on human preferences.\n",
      "\n",
      "# Key Takeaways\n",
      "*   LLMs acquire knowledge through large-scale language modeling tasks.\n",
      "*   Pre-training, generative models, prompting, and alignment are fundamental aspects of LLMs.\n",
      "*   Tool learning empowers LLMs to use external tools and enhance their capabilities.\n",
      "\n",
      "# Conclusion\n",
      "Xiao and Zhu’s book offers a comprehensive exploration of the AI agent landscape, providing valuable insights for building and developing AI systems. It highlights the importance of reasoning, planning, and effective tool use in creating autonomous AI agents.\n",
      "If you are interested in learning more, explore the full survey on Arxiv.\n",
      "\"\"\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:50:27.851888Z",
     "start_time": "2025-03-29T18:50:25.371019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.helpers import get_examples\n",
    "from src.output_formats import BlogClassification\n",
    "from src.prompts import prompt_five_shots\n",
    "\n",
    "examples = get_examples()\n",
    "blog_text = generator_response[\"parsed\"].text\n",
    "\n",
    "llm_evaluator = gemini_2_flash.with_structured_output(BlogClassification, include_raw=True)\n",
    "evaluation_chain = prompt_five_shots | llm_evaluator\n",
    "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})"
   ],
   "id": "3dddc49b0f625b8c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:50:28.867521Z",
     "start_time": "2025-03-29T18:50:28.863237Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")",
   "id": "ca2fead73c6dfbd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Average\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:50:30.964697Z",
     "start_time": "2025-03-29T18:50:30.960863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improvements = evaluator_response[\"parsed\"].improvements\n",
    "print(f\"Possible improvements:\")\n",
    "for i, improvement in enumerate(improvements):\n",
    "    print(f\"{i+1}. {improvement}\")"
   ],
   "id": "f65a3ea64157fed0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible improvements:\n",
      "1. Incorporate more real-world examples to illustrate the concepts discussed.\n",
      "2. Add a section discussing the limitations or challenges of the surveyed architectures.\n",
      "3. Use a more engaging and less academic writing style.\n",
      "4. Include a call to action to encourage readers to share their thoughts or experiences with AI agent architectures.\n",
      "5. Add a personal perspective or original analysis of the survey's findings.\n",
      "6. Make the introduction more captivating to immediately draw the reader in.\n",
      "7. Consider adding visuals or interactive elements to enhance engagement, though this is outside the scope of the textual analysis.\n",
      "8. Shorten the title to make it more attractive and focused. For example: \"AI Agent Architectures: A Survey Deep Dive\"\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:50:50.442605Z",
     "start_time": "2025-03-29T18:50:42.153131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.prompts import prompt_retry\n",
    "\n",
    "possible_improvements = \"\\n\".join([f\"{i+1}. {improvement}\" for i, improvement in enumerate(improvements)])\n",
    "\n",
    "# Reflexion\n",
    "generation_chain = prompt_retry | llm_generator\n",
    "generator_response = generation_chain.invoke({\"generated_blog\": blog_text,\n",
    "                                              \"possible_improvements\": possible_improvements})"
   ],
   "id": "7a593dd2daf85cf4",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:54:36.521954Z",
     "start_time": "2025-03-29T18:54:36.518043Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator_response[\"parsed\"].text)",
   "id": "29cde0c08e508057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"# AI Agent Architectures: A Survey Deep Dive\n",
      "\n",
      "Forget science fiction—AI agents are rapidly becoming reality! A groundbreaking survey by Tong Xiao and Jingbo Zhu delves into the exciting landscape of AI agent architectures, particularly focusing on how Large Language Models (LLMs) are driving innovation in reasoning, planning, and tool use. This isn't just another academic paper; it's a roadmap to the future of AI. Let's break it down.\n",
      "\n",
      "# The LLM Revolution: From NLP to Universal Problem Solvers\n",
      "\n",
      "LLMs have exploded onto the scene, transforming artificial intelligence, especially in natural language processing. Xiao and Zhu's survey highlights how these models learn vast amounts of world and language knowledge through massive datasets. This allows them to tackle diverse problems, shifting the paradigm from specialized, custom-built systems to adaptable foundation models. Think of it as moving from building a specific tool for each job to creating a universal adapter that can learn to use almost any tool.\n",
      "\n",
      "# Core Concepts Unpacked: A Guided Tour\n",
      "\n",
      "The survey expertly breaks down the key components of LLMs into four core areas:\n",
      "\n",
      "1.  **Pre-training:** This is where the magic begins. It covers the fundamentals of pre-training, common methods, and the model architectures that make it all possible. Imagine teaching a child the basics of language before they start learning specific subjects.\n",
      "2.  **Generative Models:** This section dives into how these models are built, scaled, and trained to handle massive amounts of text. It's like understanding how a factory is designed to produce a continuous stream of output.\n",
      "3.  **Prompting Methods:** Learn how to guide LLMs with techniques like chain-of-thought reasoning and automatic prompt design. This is the art of asking the right questions to get the best answers.\n",
      "4.  **Alignment Methods:** Ensuring LLMs are ethical and safe is crucial. This section explores instruction fine-tuning and human feedback techniques to keep AI on the right track.\n",
      "\n",
      "# Pre-training: Laying the Foundation\n",
      "\n",
      "Pre-training is like giving a neural network a head start. It involves training the model on a different task than its ultimate purpose, then fine-tuning it for the specific job. Think of it as teaching a robot to walk before asking it to run a marathon.\n",
      "\n",
      "There are three main approaches to pre-training:\n",
      "\n",
      "*   **Unsupervised Pre-training:** The model learns from unlabeled data, identifying patterns and structures without explicit instructions.\n",
      "*   **Supervised Pre-training:** The model learns from labeled data, gaining experience with specific tasks before moving on to a related downstream task.\n",
      "*   **Self-supervised Learning:** The model generates its own training signals, learning to predict missing information or relationships within the data.\n",
      "\n",
      "# Generative Models: Unleashing the Power of Text\n",
      "\n",
      "LLMs are generative models, meaning they can generate new text. They're trained on massive datasets to predict the next word in a sequence, and this ability allows them to perform a wide range of NLP tasks simply by rephrasing them as text generation problems. For example, you can ask an LLM to summarize a document, translate a language, or answer a question, all by providing the right prompt.\n",
      "\n",
      "Key steps in building and using LLMs:\n",
      "\n",
      "1.  **Data Preparation:** Gathering and cleaning vast amounts of text data.\n",
      "2.  **Model Modifications:** Tweaking the model's architecture to improve performance and stability.\n",
      "3.  **Distributed Training:** Using multiple computers to handle the massive computational demands of training.\n",
      "4.  **Scaling Laws:** Understanding how model size, data size, and training time affect performance.\n",
      "\n",
      "# Prompting: The Art of Asking\n",
      "\n",
      "Prompting is the key to unlocking the potential of LLMs. By crafting specific instructions, you can guide the model to perform complex tasks without additional training. Imagine teaching a dog a new trick simply by giving it clear commands.\n",
      "\n",
      "Key prompting methods include:\n",
      "\n",
      "*   **Zero-Shot Learning:** Solving tasks without any examples, relying solely on the instructions in the prompt. \"Write a poem about a cat.\"\n",
      "*   **Few-Shot Learning:** Providing a few examples to guide the model's response. \"Translate to French: The cat is black. Le chat est noir. Translate to French: The dog is white.\"\n",
      "\n",
      "# Alignment: Keeping AI Human-Friendly\n",
      "\n",
      "Alignment ensures that LLMs behave ethically and safely, aligning with human values. This includes being unbiased, truthful, and harmless. Think of it as teaching AI to be a responsible member of society.\n",
      "\n",
      "Key alignment techniques:\n",
      "\n",
      "*   **Instruction Alignment:** Fine-tuning LLMs to follow instructions accurately and generate relevant responses.\n",
      "*   **Human Preference Alignment:** Using human feedback to train LLMs to generate outputs that are preferred by humans.\n",
      "\n",
      "# Limitations and Challenges\n",
      "\n",
      "While LLMs are incredibly powerful, they're not without their limitations. They can be computationally expensive to train and deploy, and they can sometimes generate biased, inaccurate, or even harmful content. Ensuring their responsible use is an ongoing challenge. Issues such as bias in training data, the potential for misuse in generating misinformation, and the resources required for training and deployment all pose significant hurdles.\n",
      "\n",
      "# Key Takeaways\n",
      "\n",
      "*   LLMs learn from massive amounts of text data.\n",
      "*   Pre-training, generative models, prompting, and alignment are fundamental to LLMs.\n",
      "*   Tool learning allows LLMs to use external tools, further expanding their capabilities. Imagine an LLM booking a flight or ordering groceries for you!\n",
      "\n",
      "# Conclusion: The Future is Here\n",
      "\n",
      "Xiao and Zhu's survey provides a valuable overview of the AI agent landscape, offering insights into building and developing intelligent AI systems. It underscores the importance of reasoning, planning, and effective tool use in creating autonomous AI agents. The field is rapidly evolving, and LLMs are at the forefront of this revolution. What exciting possibilities do you envision?\n",
      "\n",
      "If you're eager to dive deeper, check out the full survey on Arxiv. We encourage you to share your thoughts and experiences with AI agent architectures in the comments below! What applications excite you the most? What challenges do you see on the horizon?\n",
      "\"\"\"\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:54:51.200898Z",
     "start_time": "2025-03-29T18:54:46.719100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blog_text = generator_response[\"parsed\"].text\n",
    "evaluator_response = evaluation_chain.invoke({**examples, \"blog_text\": blog_text})"
   ],
   "id": "5206f48504f5deef",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:54:53.532910Z",
     "start_time": "2025-03-29T18:54:53.528362Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Overall assessment: {evaluator_response[\"parsed\"].overall_assessment}\")",
   "id": "ec4b23503b229f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall assessment: Good\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# End-to-end generation test",
   "id": "a9ac6d3e8d32b56f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T20:38:59.082402Z",
     "start_time": "2025-03-29T20:38:58.440535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ],
   "id": "2f1b3ff53ba188bf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T20:39:11.187819Z",
     "start_time": "2025-03-29T20:39:00.204143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.blog_generator import BlogGenerator\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "generator = BlogGenerator()"
   ],
   "id": "bb1b8f5fd61f0acf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 21:39:11,126 - INFO - Loading vector store...\n",
      "2025-03-29 21:39:11,128 - INFO - Loading faiss with AVX2 support.\n",
      "2025-03-29 21:39:11,171 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-03-29 21:39:11,179 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n",
      "2025-03-29 21:39:11,184 - INFO - Vector store loaded successfully.\n",
      "2025-03-29 21:39:11,185 - INFO - BlogGenerator initialized with vector store.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T20:41:06.762914Z",
     "start_time": "2025-03-29T20:41:06.755766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from src.config import PREPROCESSED_BLOG_DATASET_PATH\n",
    "\n",
    "blogs = pd.read_csv(PREPROCESSED_BLOG_DATASET_PATH)"
   ],
   "id": "cbe18c99ef5af93a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T20:42:44.751850Z",
     "start_time": "2025-03-29T20:41:48.291084Z"
    }
   },
   "cell_type": "code",
   "source": "blog = generator.generate_blog(blogs.loc[0, \"url_paper\"])",
   "id": "b85733eac47fe27c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 21:41:48,293 - INFO - Extracting paper text from URL: https://arxiv.org/pdf/2501.09223\n",
      "2025-03-29 21:41:49,481 - INFO - Attempt number 1: Generating blog...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b953c29fb2644d6f97c2ee23aedbfb92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 21:41:50,673 - INFO - Found most similar article.\n",
      "2025-03-29 21:41:59,348 - INFO - Checking request limits before invoking the model...\n",
      "2025-03-29 21:41:59,349 - INFO - Invoking the model...\n",
      "2025-03-29 21:42:11,345 - INFO - Model invoked successfully. Total requests today: 1, RPM: 1, TPM: 184121\n",
      "2025-03-29 21:42:11,346 - INFO - Blog generated successfully.\n",
      "2025-03-29 21:42:11,346 - INFO - Checking request limits before invoking the model...\n",
      "2025-03-29 21:42:11,347 - INFO - Invoking the model...\n",
      "2025-03-29 21:42:26,902 - INFO - Model invoked successfully. Total requests today: 2, RPM: 2, TPM: 198601\n",
      "2025-03-29 21:42:26,903 - INFO - Blog evaluated successfully. Evaluation: Average.\n",
      "2025-03-29 21:42:26,903 - INFO - Blog generation attempt 1 unsuccessful. Retrying...\n",
      "2025-03-29 21:42:26,904 - INFO - Attempt number 2: Generating blog...\n",
      "2025-03-29 21:42:26,905 - INFO - Checking request limits before invoking the model...\n",
      "2025-03-29 21:42:26,905 - INFO - Invoking the model...\n",
      "2025-03-29 21:42:42,558 - INFO - Model invoked successfully. Total requests today: 3, RPM: 3, TPM: 206079\n",
      "2025-03-29 21:42:42,559 - INFO - Blog generated successfully.\n",
      "2025-03-29 21:42:42,559 - INFO - Checking request limits before invoking the model...\n",
      "2025-03-29 21:42:42,560 - INFO - Invoking the model...\n",
      "2025-03-29 21:42:44,747 - INFO - Model invoked successfully. Total requests today: 4, RPM: 4, TPM: 220075\n",
      "2025-03-29 21:42:44,747 - INFO - Blog evaluated successfully. Evaluation: Very Good.\n",
      "2025-03-29 21:42:44,748 - INFO - Saving blog.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T20:42:44.793199Z",
     "start_time": "2025-03-29T20:42:44.789113Z"
    }
   },
   "cell_type": "code",
   "source": "print(blog)",
   "id": "e20ec1fbb312fcb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unlocking the Foundations of Large Language Models: A Comprehensive Guide\n",
      "\n",
      "**Table of Contents**\n",
      "\n",
      "*   [Introduction to Large Language Models (LLMs)](#introduction-to-large-language-models-llms)\n",
      "*   [From Specialized Systems to Foundation Models](#from-specialized-systems-to-foundation-models)\n",
      "*   [Key Concepts and Techniques](#key-concepts-and-techniques)\n",
      "    *   [Pre-training: The Bedrock of LLMs](#pre-training-the-bedrock-of-llms)\n",
      "    *   [Generative Models: Creating Human-Like Text](#generative-models-creating-human-like-text)\n",
      "    *   [Prompting: Guiding LLMs to Perform Tasks](#prompting-guiding-llms-to-perform-tasks)\n",
      "    *   [Alignment: Ensuring LLMs are Helpful and Harmless](#alignment-ensuring-llms-are-helpful-and-harmless)\n",
      "*   [Why This Matters](#why-this-matters)\n",
      "*   [Challenges and Limitations of LLMs](#challenges-and-limitations-of-llms)\n",
      "*   [Ethical Considerations](#ethical-considerations)\n",
      "*   [Future Trends and Directions](#future-trends-and-directions)\n",
      "*   [Dive Deeper](#dive-deeper)\n",
      "*   [Conclusion](#conclusion)\n",
      "*   [Call to Action](#call-to-action)\n",
      "*   [FAQ](#faq)\n",
      "*   [Glossary of Terms](#glossary-of-terms)\n",
      "\n",
      "Are you fascinated by the power of AI-driven language models like GPT but struggling to grasp the underlying principles? Look no further! This comprehensive guide breaks down the essential concepts from the research paper \"Foundations of Large Language Models\" by Tong Xiao and Jingbo Zhu, making complex ideas accessible to everyone. We aim to provide a clear, concise, and engaging overview of LLMs, exploring their foundations, applications, limitations, and ethical considerations. Whether you're a beginner or an expert, this post will offer valuable insights into the rapidly evolving world of LLMs.\n",
      "\n",
      "## Introduction to Large Language Models (LLMs)\n",
      "\n",
      "LLMs are a groundbreaking advancement in artificial intelligence, originating from the field of natural language processing. They've revolutionized how machines understand and generate human language. The core idea behind LLMs is that by training on massive amounts of text data, these models can acquire knowledge about language and the world, enabling them to tackle diverse problems. LLMs, like GPT-3, learn to predict the next word in a sentence, and through this process, they develop a broad understanding of grammar, semantics, and even reasoning. Think of it as teaching a computer to read every book ever written and then asking it to write its own.\n",
      "\n",
      "## From Specialized Systems to Foundation Models\n",
      "\n",
      "The development of LLMs has shifted the paradigm in natural language processing. We've moved away from training specialized systems for each task from scratch, requiring vast amounts of labeled data. Instead, we now use large-scale pre-training to create foundation models. These models are then adapted for specific tasks through techniques like fine-tuning, alignment, and prompting. This approach not only saves time and resources but also allows for more generalizable and robust AI systems. For example, instead of training a separate model for translation, summarization, and question answering, we can fine-tune a single LLM to perform all three tasks.\n",
      "\n",
      "## Key Concepts and Techniques\n",
      "\n",
      "This blog post is structured around the four key chapters of the \"Foundations of Large Language Models\" paper, offering a clear and concise overview of each area:\n",
      "\n",
      "### 1. Pre-training: The Bedrock of LLMs\n",
      "\n",
      "Pre-training is the initial stage where models learn general language understanding. This involves training on massive unlabeled datasets using self-supervision. Common pre-training tasks include:\n",
      "\n",
      "*   **Decoder-only Pre-training:** Predicting the next word in a sequence (used in models like GPT). This is like giving a model the beginning of a sentence and asking it to guess what comes next, over and over again, until it becomes very good at predicting human language.\n",
      "*   **Encoder-only Pre-training:** Masking words in a sentence and training the model to predict the missing words (used in models like BERT). Imagine filling in the blanks in a text; the model learns to understand context and relationships between words.\n",
      "*   **Encoder-Decoder Pre-training:** Using an encoder to process an input sequence and a decoder to generate an output sequence (used in tasks like machine translation). This is akin to converting a sentence from one language to another, requiring the model to understand both languages.\n",
      "\n",
      "### 2. Generative Models: Creating Human-Like Text\n",
      "\n",
      "Generative models are the LLMs we commonly use today. They are typically based on decoder-only Transformers. Key aspects of training generative models include:\n",
      "\n",
      "*   **Decoder-only Transformers:** Understanding the architecture that enables these models to generate text. The Transformer architecture is a neural network design that allows the model to weigh the importance of different words in a sentence, enabling it to capture long-range dependencies.\n",
      "*   **Training LLMs:** Learning the techniques to effectively train these models on vast amounts of data. This involves optimizing the model's parameters to minimize prediction errors, a process that can take weeks or even months on powerful computers.\n",
      "*   **Fine-tuning LLMs:** Adapting pre-trained models to specific tasks. Fine-tuning is like teaching an LLM a specific skill, such as writing marketing copy or answering customer inquiries.\n",
      "*   **Aligning LLMs with the World:** Ensuring that LLMs generate outputs that are truthful and aligned with human values. This is a critical step to prevent LLMs from generating harmful or misleading content.\n",
      "*   **Prompting LLMs:** Mastering the art of prompting to guide LLMs to perform specific tasks. Prompting is like giving the LLM instructions on what you want it to do, such as \"Write a poem about the ocean.\"\n",
      "\n",
      "### 3. Prompting: Guiding LLMs to Perform Tasks\n",
      "\n",
      "Prompting involves designing effective prompts to guide LLMs to perform specific tasks. Different prompting strategies include:\n",
      "\n",
      "*   **In-context Learning:** Providing examples in the prompt to teach the LLM how to perform a task. For instance, you might provide a few examples of questions and answers to teach the LLM how to answer questions in a specific domain.\n",
      "*   **Prompt Engineering Strategies:** Using techniques to craft prompts that elicit desired responses. This could involve using specific keywords, phrasing, or formatting to guide the LLM's output.\n",
      "*   **Chain of Thought (CoT):** Encouraging the LLM to break down complex problems into smaller steps. This can help the LLM to reason more effectively and generate more accurate and coherent outputs.\n",
      "\n",
      "   For example, if you want the LLM to solve a math problem, you could prompt it to first identify the relevant information, then outline the steps required to solve the problem, and finally provide the answer.\n",
      "\n",
      "### 4. Alignment: Ensuring LLMs are Helpful and Harmless\n",
      "\n",
      "Alignment focuses on ensuring that LLMs are helpful, harmless, and aligned with human values. This involves techniques such as:\n",
      "\n",
      "*   **Instruction Fine-tuning:** Training LLMs to follow instructions accurately. This is like teaching the LLM to understand and respond to specific commands.\n",
      "*   **Alignment based on Human Feedback:** Using human preferences to guide LLMs and reward desired behaviors. This involves having humans evaluate the LLM's outputs and providing feedback on their quality and relevance.\n",
      "\n",
      "## Why This Matters\n",
      "\n",
      "LLMs are transforming numerous fields, from customer service and content creation to scientific research and education. By understanding the foundations of LLMs, you can:\n",
      "\n",
      "*   **Better Evaluate AI Technologies:** Make informed decisions about adopting and using LLMs.\n",
      "*   **Develop Custom Applications:** Create tailored solutions that leverage the power of LLMs. For example, you could develop an LLM-powered chatbot for your business or a tool that automatically summarizes research papers.\n",
      "*   **Contribute to the Field:** Engage in research and development to push the boundaries of AI.\n",
      "\n",
      "## Challenges and Limitations of LLMs\n",
      "\n",
      "While LLMs offer tremendous potential, they also have limitations and challenges:\n",
      "\n",
      "*   **Bias:** LLMs can perpetuate and amplify biases present in their training data, leading to unfair or discriminatory outcomes. For example, an LLM trained on biased data might generate stereotypical or offensive content.\n",
      "*   **Lack of Understanding:** LLMs can generate grammatically correct and seemingly coherent text without truly understanding the meaning behind it. This can lead to nonsensical or factually incorrect outputs.\n",
      "*   **Computational Cost:** Training and deploying LLMs requires significant computational resources, making them expensive to develop and maintain.\n",
      "*   **Environmental Impact:** The energy consumption associated with training large language models contributes to carbon emissions and environmental degradation.\n",
      "*   **Security Risks:** LLMs can be vulnerable to adversarial attacks, where malicious actors attempt to manipulate them into generating harmful or inappropriate content.\n",
      "\n",
      "## Ethical Considerations\n",
      "\n",
      "The development and use of LLMs raise important ethical considerations:\n",
      "\n",
      "*   **Transparency:** It's essential to understand how LLMs work and how they make decisions. This requires transparency in the development process and access to information about the training data and algorithms used.\n",
      "*   **Accountability:** Developers and users of LLMs must be held accountable for the consequences of their actions. This requires establishing clear lines of responsibility and mechanisms for redress.\n",
      "*   **Fairness:** LLMs should be designed and used in a way that promotes fairness and avoids discrimination. This requires carefully addressing biases in the training data and algorithms.\n",
      "*   **Privacy:** LLMs should be used in a way that respects individuals' privacy rights. This requires protecting sensitive information and obtaining informed consent before collecting and using personal data.\n",
      "\n",
      "## Future Trends and Directions\n",
      "\n",
      "The field of LLMs is rapidly evolving, with several exciting trends and directions:\n",
      "\n",
      "*   **Multi-modal Learning:** Combining language with other modalities, such as images and audio, to create more versatile and powerful models.\n",
      "*   **Few-shot Learning:** Developing models that can learn from a small number of examples, reducing the need for massive training datasets.\n",
      "*   **Explainable AI (XAI):** Making LLMs more transparent and understandable, allowing users to see why they made a particular decision.\n",
      "*   **Edge Computing:** Deploying LLMs on edge devices, such as smartphones and tablets, to enable real-time processing and reduce reliance on cloud-based servers.\n",
      "\n",
      "## Dive Deeper\n",
      "\n",
      "This blog post provides a high-level overview of the key concepts discussed in \"Foundations of Large Language Models.\" To truly master this subject, we encourage you to:\n",
      "\n",
      "*   **Read the Full Paper:** Access the complete research paper on Arxiv ([https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)) for a detailed understanding of the techniques and methodologies. \n",
      "*   **Experiment with LLMs:** Use online platforms and tools to interact with LLMs and explore their capabilities. Explore platforms like OpenAI's API or Google's AI Platform.\n",
      "*   **Engage with the Community:** Join online forums and discussions to learn from other practitioners and researchers. Platforms like Reddit and Stack Overflow host active AI communities.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Large language models are a rapidly evolving field with immense potential. By grasping the fundamental concepts and techniques outlined in this blog post, you can unlock the power of LLMs and contribute to shaping the future of AI. Start exploring, experimenting, and innovating today!\n",
      "\n",
      "## Call to Action\n",
      "\n",
      "Ready to take the next step? Here are a few things you can do:\n",
      "\n",
      "*   Share this blog post with your network to help others learn about LLMs.\n",
      "*   Sign up for our newsletter to stay up-to-date on the latest developments in AI.\n",
      "*   Attend a workshop or conference on LLMs to deepen your knowledge and connect with experts in the field.\n",
      "\n",
      "## FAQ\n",
      "\n",
      "**Q: What are the main differences between LLMs and traditional machine learning models?**\n",
      "A: LLMs are trained on much larger datasets and have the ability to generate text, while traditional machine learning models are typically used for classification or regression tasks.\n",
      "\n",
      "**Q: How can I get started with LLMs?**\n",
      "A: You can start by reading research papers, experimenting with online platforms, and joining online communities.\n",
      "\n",
      "**Q: What are the ethical implications of using LLMs?**\n",
      "A: LLMs can perpetuate biases and generate harmful content, so it's important to use them responsibly and ethically.\n",
      "\n",
      "## Glossary of Terms\n",
      "\n",
      "*   **Alignment:** The process of ensuring that LLMs are helpful, harmless, and aligned with human values.\n",
      "*   **Fine-tuning:** Adapting a pre-trained LLM to a specific task.\n",
      "*   **In-context Learning:** Providing examples in the prompt to teach the LLM how to perform a task.\n",
      "*   **Large Language Model (LLM):** A type of AI model that is trained on massive amounts of text data and can generate human-like text.\n",
      "*   **Pre-training:** The initial stage where models learn general language understanding.\n",
      "*   **Prompting:** Designing effective prompts to guide LLMs to perform specific tasks.\n",
      "*   **Transformer:** A neural network architecture that is well-suited for natural language processing tasks.\n",
      "\n",
      "**Note:** This blog post is intended for informational purposes only and does not constitute professional advice. The views expressed in this blog post are those of the author and do not necessarily reflect the views of any organization or individual. While we strive to provide accurate and up-to-date information, we make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability, or availability with respect to the blog post or the information, products, services, or related graphics contained on the blog post for any purpose. Any reliance you place on such information is therefore strictly at your own risk. We will not be liable for any loss or damage including without limitation, indirect or consequential loss or damage, or any loss or damage whatsoever arising from loss of data or profits arising out of, or in connection with, the use of this blog post. If you have any questions about the legal requirements for publishing a blog post, please consult with legal counsel.\n",
      "\n",
      "We hope that you found this blog post helpful, and we encourage you to keep learning about LLMs! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
