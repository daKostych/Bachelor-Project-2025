{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:21.989216Z",
     "start_time": "2025-04-01T21:54:13.064061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "if str(Path().resolve().parent) not in sys.path:\n",
    "    sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from src.config import random_seed, PREPROCESSED_BLOG_DATASET_PATH, CLASSIFICATION_MAP\n",
    "from src.text_extraction import extract_blog_text\n",
    "from src.models_setup import gemini_2_flash, gemini_2_flash_lite, gemini_2_flash_thinking\n",
    "from src.prompts import *\n",
    "from src.output_formats import *\n",
    "from src.helpers import extract_llm_assessment"
   ],
   "id": "a76f8b3c83996ddf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Processing",
   "id": "491b0b756a34021a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:22.269810Z",
     "start_time": "2025-04-01T21:54:22.253348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data\n",
    "blogs = pd.read_csv(PREPROCESSED_BLOG_DATASET_PATH)\n",
    "\n",
    "# Split dataset into validation and test set\n",
    "Xval, Xtest, yval_score, ytest_score = train_test_split(\n",
    "    blogs.drop(columns=['normalized_engagement_score']), blogs[\"normalized_engagement_score\"],\n",
    "    test_size=0.4, random_state=random_seed)\n",
    "\n",
    "# Same Xval, Xtest; new explained variable \"engagement_level\"\n",
    "Xval, Xtest, yval_level, ytest_level = train_test_split(\n",
    "    blogs.drop(columns=['engagement_level', 'normalized_engagement_score']), blogs[\"engagement_level\"],\n",
    "    test_size=0.4, random_state=random_seed)\n",
    "\n",
    "print(f\"Size of validation set, X: {Xval.shape}, y: {yval_score.shape}\")\n",
    "print(f\"Size of test set, X: {Xtest.shape}, y: {ytest_score.shape}\")"
   ],
   "id": "3a3e52e28b5746a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation set, X: (30, 10), y: (30,)\n",
      "Size of test set, X: (20, 10), y: (20,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:22.323904Z",
     "start_time": "2025-04-01T21:54:22.308509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The best blog in dataset\n",
    "best_blog_info = Xval.sort_values([\"engagement_score\"], ascending=False).head(1)\n",
    "best_blog_index = best_blog_info.index[0]\n",
    "best_blog_info = best_blog_info.reset_index(drop=True)\n",
    "best_blog_info"
   ],
   "id": "947b2f95eaf6ae23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id         title_blog                                           url_blog  \\\n",
       "0  28  Towards Reasoning  https://medium.com/@saptarshichaudhuri/towards...   \n",
       "\n",
       "           author_blog  author_followers  claps  comments  \\\n",
       "0  Saptarshi Chaudhuri               127    461         1   \n",
       "\n",
       "                                         title_paper  \\\n",
       "0  GSM-Symbolic: Understanding the Limitations of...   \n",
       "\n",
       "                                           url_paper  engagement_score  \n",
       "0  https://arxiv.org/pdf/2410.05229?source=post_p...          3.653543  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_blog</th>\n",
       "      <th>url_blog</th>\n",
       "      <th>author_blog</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>claps</th>\n",
       "      <th>comments</th>\n",
       "      <th>title_paper</th>\n",
       "      <th>url_paper</th>\n",
       "      <th>engagement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Towards Reasoning</td>\n",
       "      <td>https://medium.com/@saptarshichaudhuri/towards...</td>\n",
       "      <td>Saptarshi Chaudhuri</td>\n",
       "      <td>127</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>GSM-Symbolic: Understanding the Limitations of...</td>\n",
       "      <td>https://arxiv.org/pdf/2410.05229?source=post_p...</td>\n",
       "      <td>3.653543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:22.410159Z",
     "start_time": "2025-04-01T21:54:22.399427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The worst blog in dataset\n",
    "worst_blog_info = Xval.sort_values([\"engagement_score\"], ascending=False).tail(1)\n",
    "worst_blog_index = worst_blog_info.index[0]\n",
    "worst_blog_info = worst_blog_info.reset_index(drop=True)\n",
    "worst_blog_info"
   ],
   "id": "8904bb60-4deb-4ac8-bf27-1b4aa58928df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                         title_blog  \\\n",
       "0  18  SMoA: Improving Multi-agent Large Language Mod...   \n",
       "\n",
       "                                            url_blog  author_blog  \\\n",
       "0  https://medium.com/@sulbha.jindal/smoa-improvi...  Sulbha Jain   \n",
       "\n",
       "   author_followers  claps  comments  \\\n",
       "0                41      0         0   \n",
       "\n",
       "                                         title_paper  \\\n",
       "0  SMoA: Improving Multi-agent Large Language Mod...   \n",
       "\n",
       "                          url_paper  engagement_score  \n",
       "0  https://arxiv.org/pdf/2411.03284               0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_blog</th>\n",
       "      <th>url_blog</th>\n",
       "      <th>author_blog</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>claps</th>\n",
       "      <th>comments</th>\n",
       "      <th>title_paper</th>\n",
       "      <th>url_paper</th>\n",
       "      <th>engagement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>SMoA: Improving Multi-agent Large Language Mod...</td>\n",
       "      <td>https://medium.com/@sulbha.jindal/smoa-improvi...</td>\n",
       "      <td>Sulbha Jain</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SMoA: Improving Multi-agent Large Language Mod...</td>\n",
       "      <td>https://arxiv.org/pdf/2411.03284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:22.454477Z",
     "start_time": "2025-04-01T21:54:22.448946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blog = blogs.iloc[16]\n",
    "blog"
   ],
   "id": "82394efd-c053-4a65-b737-be97aa6c2f96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                            17\n",
       "title_blog                     Self-Generated Critiques Boost Reward Modeling...\n",
       "url_blog                       https://medium.com/@sulbha.jindal/self-generat...\n",
       "author_blog                                                          Sulbha Jain\n",
       "author_followers                                                              41\n",
       "claps                                                                         21\n",
       "comments                                                                       0\n",
       "title_paper                    Self-Generated Critiques Boost Reward Modeling...\n",
       "url_paper                                       https://arxiv.org/pdf/2411.16646\n",
       "engagement_score                                                        0.512195\n",
       "normalized_engagement_score                                                 49.4\n",
       "engagement_level                                                            Good\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:30.037786Z",
     "start_time": "2025-04-01T21:54:22.555858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracting clean text from a blog\n",
    "blog_text = extract_blog_text(blog=blog)\n",
    "print(blog_text)"
   ],
   "id": "ee75bcb6-a9af-44c5-b459-0412d51cb3a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Paper — https://arxiv.org/pdf/2411.16646\n",
      "Reinforcement Learning from Human Feedback (RLHF) has become a critical methodology for aligning large language models (LLMs) with human preferences. At the core of RLHF lies the reward model (RM), which is designed to evaluate model outputs by assigning scores that reflect their alignment with human judgments. These scores guide the optimization process during training, such as providing reward signals in Proximal Policy Optimization (PPO), thereby encouraging LLMs to generate responses that are more helpful, honest, and harmless. This iterative process enhances the practical quality of LLM outputs in real-world applications.\n",
      "\n",
      "## Current challenge\n",
      "Typically, reward models are trained using preference pairs and optimized through pairwise logistic loss to produce a scalar score for each response. However, this scalar output is often hard to interpret and underutilizes the inherent language modeling capabilities of LLMs derived from pretraining and post-training. These limitations can weaken the feedback signals in RLHF, resulting in suboptimal policy updates. An alternative approach is the “LLM-as-a-judge” paradigm, where the LLM generates critiques and optionally provides discrete scores as proxies for response quality. This method leverages the model’s reasoning abilities more effectively, potentially addressing some of the shortcomings of traditional reward models\n",
      "Incorporating critiques into reward modeling poses two significant challenges. First, there is the issue of conflicting objectives: generating critiques relies on language modeling capabilities, whereas traditional reward models output scalar values, making their integration into the language modeling process complex. Second, there are limitations with evaluators; off-the-shelf language models often lack the effectiveness needed for evaluation, and fine-tuning these models necessitates costly human-generated or annotated critiques. While knowledge distillation offers a potential solution by enabling a joint training approach to learn how to generate critiques and rewards simultaneously, it falls short when it comes to enhancing frontier models in situations where a stronger teacher model is unavailable. Here is Critic-RM, a new framework from Meta Researchers that enhances reward models using synthetic critiques, without relying on strong LLM teachers.\n",
      "\n",
      "## Methodology\n",
      "Critic-RM utilizes an instruction-finetuned large language model (LLM) as its foundation, generating multiple candidate critiques, each accompanied by a discrete score for individual responses. The process begins with a consistency-guided filtering technique that retains only those critiques whose scores align with human-annotated preference labels.\n",
      "To further improve the quality of these synthetic critiques, two additional strategies — summarization and ranking — are proposed to refine the critiques used in training the reward model. The framework investigates the application of an off-the-shelf instruction-finetuned LLM, for both critique generation and reward modeling. Initially, Critic-RM generates candidate critiques for each prompt-response pair, followed by a filtering step aimed at minimizing the influence of potentially noisy rationales that could lead to incorrect predictions. This approach allows for the augmentation of preference pairs with additional critiques, ultimately enhancing the precision of reward modeling.\n",
      "After generating critiques for each response, the primary challenge is developing an effective training strategy to integrate critique modeling and scalar reward prediction objectives. To address this, we propose a simple weighting strategy that balances these objectives. Initially, the model prioritizes critique modeling loss, then gradually shifts its focus toward reward prediction, utilizing both the response and the critique. This balanced approach enables the model to excel in generating high-quality critiques while maintaining accurate reward predictions.\n",
      "In Critic-RM, an additional step is introduced during inference for each (prompt, response) pair. Given a (prompt, response) pair $$(x, y)$$, the model first generates a critique $$z \\sim q_\\phi(x, y)$$. It then predicts the reward for the response as $$r = r_\\psi(x, [y, z])$$, where the reward prediction incorporates both the response and its associated critique. This process ensures a more nuanced and precise evaluation of responses.\n",
      "\n",
      "## Results\n",
      "Incorporating critiques into reward modeling has demonstrated significant benefits, particularly with the Critic-RM framework, which consistently outperforms the baselines in this study. Specifically, when trained on the same preference data, Critic-RM achieves an improvement of 3.7% to 4.7% over standard reward models. The quality of critiques plays a crucial role; comparisons reveal that other models incorporating critiques show smaller performance gains than Critic-RM when evaluated against the standard reward model. Additionally, performance enhancements are observed for both Critic-RM and its baselines when multiple critiques are generated during inference, particularly benefiting reasoning tasks.\n",
      "\n",
      "## Summary\n",
      "Researchers have developed Critic-RM, an innovative self-critiquing framework aimed at enhancing reward modeling for large language models (LLMs). This novel approach leverages the inherent capabilities of LLMs to generate and refine critiques, implementing a self-improvement mechanism that enhances both the quality of critiques and the accuracy of reward predictions. The findings from this research emphasize the efficacy of Critic-RM in improving reward modeling accuracy and underscore the crucial role of high-quality critiques in this process. Beyond merely enhancing precision in reward modeling, the framework has demonstrated robust performance across various benchmarks, including RewardBench and CrossEval, highlighting its potential to significantly advance the field of language model optimization and alignment with human preferences.\n",
      "\n",
      "## Appendix\n",
      "    * Paper link — https://arxiv.org/pdf/2411.16646\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompt Engineering",
   "id": "664ac8823bdeddb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:30.119791Z",
     "start_time": "2025-04-01T21:54:30.116479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_prompt(prompt_template, model, input_variables):\n",
    "    \"\"\"Testing the operation of LLM with a given prompt template.\"\"\"\n",
    "    test_chain = prompt_template | model\n",
    "    test_response = test_chain.invoke(input_variables)\n",
    "    print(f\"Usage metadata:\\n{test_response.usage_metadata}\")\n",
    "    print(f\"\\nContent:\\n{test_response.content}\")"
   ],
   "id": "965e320c93be2a85",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:34.837582Z",
     "start_time": "2025-04-01T21:54:30.161396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One word answer\n",
    "test_prompt(prompt_simple_answer, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "c724f10e4a890997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1138, 'output_tokens': 3, 'total_tokens': 1141, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Informative\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:38.625271Z",
     "start_time": "2025-04-01T21:54:34.895232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adding numerical assessment\n",
    "test_prompt(prompt_numeric_rating, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "57865550ca9165ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1143, 'output_tokens': 9, 'total_tokens': 1152, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Informative - 8/10\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:43.229756Z",
     "start_time": "2025-04-01T21:54:38.670645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale from 1 to 100 + short comment\n",
    "test_prompt(prompt_short_comment, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "25c655f5694ea127",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1154, 'output_tokens': 102, 'total_tokens': 1256, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "**Rating: 85/100**\n",
      "\n",
      "**Comment:** This is a well-written and concise summary of the Critic-RM paper. It clearly explains the motivation, methodology, results, and significance of the research. The explanation of the core concepts is easy to follow, and the inclusion of the paper link is helpful. A slightly deeper dive into the specific filtering/summarization/ranking techniques used might elevate it further, but as a high-level overview, it's excellent.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:47.329297Z",
     "start_time": "2025-04-01T21:54:43.280572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Emphasizing the importance of engagement\n",
    "test_prompt(prompt_engagement_score, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "95c4d7b07c3fe49a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1160, 'output_tokens': 80, 'total_tokens': 1240, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "**Engagement Level: 55/100**\n",
      "\n",
      "**Comment:** The blog post provides a well-structured summary of a research paper, making it informative and accessible. However, the engagement level is moderate. This could be improved by incorporating interactive elements like questions for the reader, discussion prompts, or comparisons to related work. Including visuals or code snippets would also make it more engaging.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:52.445800Z",
     "start_time": "2025-04-01T21:54:47.385839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assessment based on several criteria\n",
    "test_prompt(prompt_criteria, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "e265f8d2827ffe01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1184, 'output_tokens': 375, 'total_tokens': 1559, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "**Assessment:**\n",
      "\n",
      "*   **Readability:** The text is generally well-written and uses clear language, though some familiarity with LLM and RLHF concepts is assumed. Technical terms are explained, but briefly. (Score: 75)\n",
      "*   **Structure:** The blog post follows a logical structure: introduction, problem statement, proposed solution, methodology, results, and summary. The use of headings and subheadings improves readability. (Score: 85)\n",
      "*   **Attractiveness of the Blog Title:** The title is descriptive and informative, but not particularly catchy. It clearly states the content of the review. (Score: 65)\n",
      "*   **Clarity:** The explanation of the Critic-RM methodology is relatively clear, although a deeper understanding of the underlying mathematical formulas may require reading the original paper. (Score: 70)\n",
      "*   **Audience Appeal:** The blog post is targeted towards researchers and practitioners in the field of LLMs and RLHF. It may not be accessible to a general audience. (Score: 60)\n",
      "*   **Potential for Discussion:** The topic is relevant and current, and the review provides a good overview of the paper. This could spark discussion among researchers interested in reward modeling and LLM alignment. Questions about the limitations of the approach, the trade-offs involved, or potential applications could arise. (Score: 75)\n",
      "\n",
      "**Overall Rating:** 72/100\n",
      "\n",
      "**Comment:**\n",
      "This blog post provides a concise and informative review of the Critic-RM paper. While it may not be accessible to everyone due to its technical nature, it effectively summarizes the key concepts and findings. The potential for discussion is good, particularly among those working in the field. Improving the title's \"hook\" and adding more visuals could boost engagement.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:54:59.003299Z",
     "start_time": "2025-04-01T21:54:52.471534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate assessment\n",
    "test_prompt(prompt_separate_assessment, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "f35c4362bd32245c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1211, 'output_tokens': 808, 'total_tokens': 2019, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, here's an analysis of the blog post's engagement level based on the criteria you provided, along with individual scores and an overall assessment:\n",
      "\n",
      "**Criterion Breakdown and Scores (out of 100):**\n",
      "\n",
      "*   **Readability (75/100):** The writing is generally clear and concise. It avoids overly complex jargon, but some familiarity with LLMs and RLHF is assumed. The use of headings and subheadings helps break up the text. However, the inclusion of mathematical notation ($$(x, y)$$, $$z \\sim q_\\phi(x, y)$$, $$r = r_\\psi(x, [y, z])$$) can reduce readability for those unfamiliar with such notation.\n",
      "*   **Structure (85/100):** The blog post follows a logical structure. It begins with an introduction to RLHF and reward models, then outlines the challenges, presents the Critic-RM methodology, discusses the results, and concludes with a summary. The use of headings and subheadings like \"Current Challenge,\" \"Methodology,\" \"Results,\" and \"Summary\" makes it easy to follow.\n",
      "*   **Informativeness (80/100):** The blog post provides a good overview of the Critic-RM framework and its contributions. It explains the problem it addresses, the proposed solution, and the key findings. The inclusion of the paper link allows readers to delve deeper into the details. It does assume some prior knowledge of the field.\n",
      "*   **Attractiveness of the Blog Title (65/100):** \"Self-Generated Critiques Boost Reward Modeling for Language Models — Paper Review\" is somewhat descriptive but not particularly catchy. It's a functional title that conveys the topic, but it lacks a strong hook to immediately grab attention. Adding a bit more intrigue or a benefit statement could improve it.\n",
      "*   **Clarity (80/100):** The concepts are generally explained clearly, although as mentioned above, some background knowledge is expected. The methodology section could benefit from even more simplification for a broader audience. The use of bullet points or numbered lists to highlight the key steps in the Critic-RM process could improve clarity.\n",
      "*   **Audience Appeal (70/100):** The target audience is likely researchers, students, and practitioners interested in LLMs, RLHF, and reward modeling. The blog post will appeal to those seeking to stay up-to-date with the latest research in the field. However, it's less likely to appeal to a general audience due to its technical nature.\n",
      "*   **Potential for Discussion (70/100):** The blog post has decent potential for discussion. The topic is current and relevant, and the results are interesting. Questions that could arise include:\n",
      "    *   How does Critic-RM compare to other self-improvement techniques for LLMs?\n",
      "    *   What are the limitations of Critic-RM?\n",
      "    *   How can Critic-RM be applied to other tasks or domains?\n",
      "    *   What are the ethical considerations of using self-generated critiques?\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "To calculate the overall assessment, we sum the individual scores and divide by the number of criteria (7):\n",
      "\n",
      "(75 + 85 + 80 + 65 + 80 + 70 + 70) / 7 = 74.29\n",
      "\n",
      "**Therefore, the overall engagement level of this blog post is approximately 74/100.**\n",
      "\n",
      "**Justification:**\n",
      "\n",
      "The blog post is well-structured, informative, and generally clear, which contributes to its engagement level. However, its technical nature, somewhat uninspired title, and the assumption of prior knowledge limit its appeal to a broader audience. The potential for discussion is moderate, as the topic is interesting but requires some expertise to fully engage with.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:55:03.406556Z",
     "start_time": "2025-04-01T21:54:59.059093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add information about profile of the model\n",
    "test_prompt(prompt_with_profile, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "4f0ece304743f9fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1239, 'output_tokens': 713, 'total_tokens': 1952, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, here's my analysis of the blog post, broken down by the criteria you provided, and culminating in an overall assessment:\n",
      "\n",
      "**Criterion Breakdown:**\n",
      "\n",
      "*   **Readability (75/100):** The blog post utilizes technical language that, while necessary for the subject matter, can be challenging for readers without a background in machine learning or natural language processing. The use of bullet points and clear headings improves readability. However, the density of technical terms could be reduced with more explanation.\n",
      "*   **Structure (90/100):** The blog follows a clear and logical structure: introduction, problem statement (\"Current challenge\"), proposed solution (\"Methodology\"), results, and summary. The use of headings and subheadings makes it easy to follow the main points. The inclusion of a direct link to the paper in both the intro and the Appendix is excellent.\n",
      "*   **Informativeness (85/100):** The blog provides a good overview of the research paper. It clearly explains the problem being addressed, the proposed methodology (Critic-RM), and the key findings. However, a deeper dive into the specific details of the filtering, summarization, and ranking techniques used within Critic-RM could enhance the informativeness.\n",
      "*   **Attractiveness of the Blog Title (70/100):** The title, \"Self-Generated Critiques Boost Reward Modeling for Language Models — Paper Review,\" is descriptive but not particularly captivating. It accurately conveys the topic but lacks a hook to draw in a broader audience. A more attention-grabbing title could increase initial interest.\n",
      "*   **Clarity (80/100):** The explanation of the core concepts is generally clear, but some sections become dense with technical jargon. The mathematical notation ($$(x, y)$$, $$z \\sim q_\\phi(x, y)$$, $$r = r_\\psi(x, [y, z])$$) might be opaque to readers without a strong mathematical background. Providing more intuitive explanations or examples would improve clarity.\n",
      "*   **Audience Appeal (65/100):** The blog is primarily geared toward an audience already familiar with machine learning, RLHF, and LLMs. It might not be very accessible or appealing to a general audience or those new to the field. More context and simplification would be needed to broaden its appeal.\n",
      "*   **Potential for Discussion (75/100):** The blog presents a fairly objective summary of the paper. To increase the potential for discussion, the author could include some critical analysis of the methodology, potential limitations, or future directions for research. Posing open-ended questions related to the implications of the research could also stimulate discussion.\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "To calculate the overall assessment, I'll average the scores from each criterion:\n",
      "\n",
      "(75 + 90 + 85 + 70 + 80 + 65 + 75) / 7 = **77.14**\n",
      "\n",
      "**Final Score: 77/100**\n",
      "\n",
      "**Justification for the Overall Score:**\n",
      "\n",
      "The blog post provides a solid overview of the research paper, particularly for those with existing knowledge of the field. It is well-structured and informative. However, the level of technical detail and lack of broader context limit its appeal to a wider audience. Some improvements in readability, clarity, and discussion potential would significantly enhance its overall impact.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:55:08.526271Z",
     "start_time": "2025-04-01T21:55:03.431101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Well-structured sections of the prompt\n",
    "test_prompt(prompt_structured_sections, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "d414143346e2b26f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1342, 'output_tokens': 812, 'total_tokens': 2154, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "Okay, here's an evaluation of the blog post, focusing on the textual content and its effectiveness in communicating scientific research to a broader audience.\n",
      "\n",
      "**Scores and Comments:**\n",
      "\n",
      "*   **Readability:** 70/100 - The writing is generally clear, but the density of technical terms (\"Proximal Policy Optimization,\" \"pairwise logistic loss,\" \"knowledge distillation\") without extensive explanation can hinder readability for a broader audience. Some sentences are also a bit long and complex.\n",
      "*   **Structure:** 80/100 - The blog follows a logical structure: introduction, problem statement, proposed solution (methodology), results, and summary. The use of headings and subheadings is effective in breaking up the text.\n",
      "*   **Informativeness:** 85/100 - The blog provides a good overview of the research paper, covering the key aspects of the problem, the proposed solution, and the results. It successfully extracts and presents the core information.\n",
      "*   **Attractiveness of the blog title:** 65/100 - The title is informative (\"Self-Generated Critiques Boost Reward Modeling for Language Models — Paper Review\") but lacks a strong hook. It's accurate but not particularly engaging.\n",
      "*   **Clarity:** 75/100 - The core concepts are explained reasonably well, but some parts, especially the methodology section with the mathematical notation, might be difficult for readers without a background in machine learning to fully grasp.\n",
      "*   **Audience Appeal:** 60/100 - The blog is likely to appeal to those already familiar with LLMs and RLHF. However, the level of technical detail and jargon might make it less appealing to a general audience interested in AI.\n",
      "*   **Potential for Discussion:** 70/100 - The blog presents a relatively new and interesting approach (Critic-RM). There is potential for discussion around the limitations of the method, its applicability to other tasks, and its ethical implications. However, it could be improved with a more explicit call to action.\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "Overall Score: 71/100\n",
      "\n",
      "**Possible Improvements:**\n",
      "\n",
      "1.  **Simplify Technical Language:** Reduce the use of jargon and provide simpler explanations for technical terms. For example, instead of just mentioning \"Proximal Policy Optimization (PPO),\" briefly explain what PPO does in plain language (e.g., \"a method for training AI models to make better decisions through trial and error\").\n",
      "2.  **Provide More Context:** Expand on the \"Current Challenge\" section. Explain *why* interpretable scalar outputs are important, and *why* the limitations of current reward models are significant. Use relatable examples if possible.\n",
      "3.  **Elaborate on the Benefits:** The \"Results\" section mentions a 3.7% to 4.7% improvement. Explain the real-world impact of this improvement. How does it translate to better LLM performance in practical applications?\n",
      "4.  **Avoid Equations:** Since the blog aims at a wider audience, it is best to avoid equations. Describe them using plain text.\n",
      "5.  **Add a \"Why Should I Care?\" Section:** Explicitly state the broader implications of this research. How might Critic-RM impact the development of AI systems, or how we interact with them?\n",
      "6.  **Include a Call to Action:** Encourage discussion by posing questions at the end of the blog. For example: \"What are your thoughts on using self-generated critiques for reward modeling? What other applications could this approach have?\"\n",
      "7.  **Consider an Analogy:** Explain the core concept of Critic-RM using an analogy that is accessible to a broader audience. This could help readers grasp the core idea without getting bogged down in the technical details.\n",
      "8.  **Explain acronyms:** At first use of a given acronym, the full name must be specified.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:55:22.897284Z",
     "start_time": "2025-04-01T21:55:08.579044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_blog = {\n",
    "    \"blog\": extract_blog_text(blog=best_blog_info.iloc[0]),\n",
    "    \"score\" : yval_score.loc[best_blog_index]\n",
    "}\n",
    "worst_blog = {\n",
    "    \"blog\" : extract_blog_text(blog=worst_blog_info.iloc[0]),\n",
    "    \"score\" : yval_score.loc[worst_blog_index]\n",
    "}"
   ],
   "id": "6781defd233a4319",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:55:26.237778Z",
     "start_time": "2025-04-01T21:55:22.903404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-shot prompt\n",
    "test_prompt(prompt_two_shots, gemini_2_flash, {\n",
    "    \"blog_text\" : blog_text,\n",
    "    \"blog_ex1\" : best_blog[\"blog\"],\n",
    "    \"score_ex1\" : best_blog[\"score\"],\n",
    "    \"blog_ex2\" : worst_blog[\"blog\"],\n",
    "    \"score_ex2\" : worst_blog[\"score\"]\n",
    "})"
   ],
   "id": "d2962a1ec01216be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 3292, 'output_tokens': 488, 'total_tokens': 3780, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "```json\n",
      "{\n",
      "  \"overall_assessment\": 60,\n",
      "  \"explanation\": \"The blog post provides a decent overview of the research paper, explaining the key concepts and methodology in a way that's understandable for readers with some background in machine learning or NLP. However, there are areas where it could be improved to enhance clarity and engagement.\\n\\nReadability: The text is generally readable, but the inclusion of formulas without sufficient context might deter some readers. \\nStructure: The structure is logical, with clear sections like \\\"Current challenge,\\\" \\\"Methodology,\\\" \\\"Results,\\\" and \\\"Summary.\\\" This makes it easy to follow the main points of the paper. However, some sections could benefit from further elaboration.\\nInformativeness: The blog provides a good summary of the paper's key contributions and findings. However, it lacks depth in explaining the technical details and implications of the research.\\nAttractiveness of the blog title: The title is descriptive but not particularly attention-grabbing. It clearly states the topic but could be more engaging.\\nClarity: The explanation of the methodology could be clearer, especially regarding the technical aspects of the Critic-RM framework. The use of jargon without sufficient explanation may confuse some readers.\\nAudience appeal: The blog post is likely to appeal to readers interested in reinforcement learning, language models, and AI alignment. However, it may not be accessible to a broader audience without a background in these areas.\\nPotential for discussion: The blog post has some potential for discussion, particularly around the implications of the research for RLHF and language model alignment. However, it could be more thought-provoking by raising critical questions or exploring alternative perspectives.\\n\\n\",\n",
      "  \"possible_improvements\": [\n",
      "    \"Provide more context and explanation for technical terms and concepts.\",\n",
      "    \"Include real-world examples or analogies to illustrate the benefits of Critic-RM.\",\n",
      "    \"Add a section discussing the limitations of the research or potential areas for future work.\",\n",
      "    \"Use visuals or diagrams to illustrate the methodology and results.\",\n",
      "    \"Incorporate more storytelling or narrative elements to make the content more engaging.\",\n",
      "    \"Make the title more attention-grabbing.\",\n",
      "    \"Avoid jargon when possible or explain it thoroughly if it must be used.\",\n",
      "    \"Consider adding a 'Key Takeaways' section.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:56:04.703986Z",
     "start_time": "2025-04-01T21:55:26.267167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "excellent_blog_index = yval_level[yval_level == \"Excellent\"].index.min()\n",
    "excellent_blog = extract_blog_text(blog=Xval.loc[excellent_blog_index])\n",
    "with open(\"../data/examples/excellent_blog\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(excellent_blog)\n",
    "\n",
    "very_good_blog_index = yval_level[yval_level == \"Very Good\"].index.min()\n",
    "very_good_blog = extract_blog_text(blog=Xval.loc[very_good_blog_index])\n",
    "with open(\"../data/examples/very_good_blog\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(very_good_blog)\n",
    "\n",
    "good_blog_index = yval_level[yval_level == \"Good\"].index.min()\n",
    "good_blog = extract_blog_text(blog=Xval.loc[good_blog_index])\n",
    "with open(\"../data/examples/good_blog\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(good_blog)\n",
    "\n",
    "average_blog_index = yval_level[yval_level == \"Average\"].index.min()\n",
    "average_blog = extract_blog_text(blog=Xval.loc[average_blog_index])\n",
    "with open(\"../data/examples/average_blog\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(average_blog)\n",
    "\n",
    "bad_blog_index = yval_level[yval_level == \"Bad\"].index.min()\n",
    "bad_blog = extract_blog_text(blog=Xval.loc[bad_blog_index])\n",
    "with open(\"../data/examples/bad_blog\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(bad_blog)"
   ],
   "id": "eec0c242bc95cd78",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:56:07.509076Z",
     "start_time": "2025-04-01T21:56:04.734334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5-shot prompt with verbal classification\n",
    "test_prompt(prompt_five_shots, gemini_2_flash, {\n",
    "    \"blog_text\": blog_text,\n",
    "    \"excellent_blog\": excellent_blog,\n",
    "    \"very_good_blog\": very_good_blog,\n",
    "    \"good_blog\": good_blog,\n",
    "    \"average_blog\": average_blog,\n",
    "    \"bad_blog\": bad_blog\n",
    "})"
   ],
   "id": "d75decc38b3b05a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 11803, 'output_tokens': 325, 'total_tokens': 12128, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "```json\n",
      "{\n",
      "  \"engagement_level\": \"Good\",\n",
      "  \"explanation\": \"The blog post presents a decent overview of the research paper on Critic-RM, a framework for enhancing reward modeling in LLMs using self-generated critiques. The structure is logical, presenting the background, challenges, methodology, results, and a summary. The information is relatively clear, although some sections require familiarity with RLHF and LLM concepts. However, the blog lacks a strong hook and engaging elements that would significantly increase its appeal to a broader audience.\",\n",
      "  \"possible_improvements\": [\n",
      "    \"Add a more engaging introduction to capture the reader's attention, such as a real-world problem or a relatable scenario where improved reward modeling can make a difference.\",\n",
      "    \"Simplify complex technical terms and concepts to make the blog more accessible to readers without a deep background in machine learning. Use analogies or examples.\",\n",
      "    \"Incorporate more visual cues, such as bolding key phrases or using bullet points more extensively, to improve readability and highlight important information.\",\n",
      "    \"Elaborate on the potential implications and applications of Critic-RM in a more practical and understandable way.\",\n",
      "    \"Integrate questions throughout the text to encourage reflection and engagement from the reader.\",\n",
      "    \"Expand the conclusion to include a call to action, inviting readers to explore the paper further or discuss their thoughts on the topic.\",\n",
      "    \"The title is descriptive but not very attention-grabbing. Consider rewording it to be more intriguing or highlight the key benefit of the research.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:56:12.830490Z",
     "start_time": "2025-04-01T21:56:07.577492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zero-shot Chain-of-Thoughts\n",
    "test_prompt(prompt_zero_cot, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "75b33b0d24d84862",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1412, 'output_tokens': 620, 'total_tokens': 2032, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "```json\n",
      "{\n",
      "  \"analysis\": {\n",
      "    \"readability\": \"The readability is moderate. While the language is generally clear, the presence of technical terms like 'Proximal Policy Optimization (PPO),' 'pairwise logistic loss,' and mathematical notations such as '$$r = r_\\\\psi(x, [y, z])$$' without sufficient context may hinder comprehension for readers unfamiliar with machine learning. The writing style is somewhat academic, which might deter a broader audience.\",\n",
      "    \"structure\": \"The blog follows a logical structure, starting with an introduction to RLHF and reward models, then moving to the challenges, the proposed methodology (Critic-RM), results, and a summary. The use of headings and subheadings helps to break up the text and improve readability. The structure is sound and aids in understanding the topic.\",\n",
      "    \"informativeness\": \"The blog provides a decent overview of the Critic-RM framework and its benefits. It explains the core concepts and the methodology used. However, it could be more informative by providing more concrete examples or elaborating on the practical implications of the research. It assumes a certain level of background knowledge in the field.\",\n",
      "    \"attractiveness_of_the_blog_title\": \"The title 'Self-Generated Critiques Boost Reward Modeling for Language Models — Paper Review' is relatively informative but not particularly engaging. It clearly indicates the topic but lacks a hook to draw in a broader audience. It is very specific and academic, which may deter casual readers.\",\n",
      "    \"clarity\": \"The clarity is acceptable but could be improved. While the main ideas are presented clearly, the technical details and jargon might confuse readers without a background in machine learning. More explanation of the concepts and potentially simplifying the language would enhance clarity.\",\n",
      "    \"audience_appeal\": \"The audience appeal is limited. The blog is primarily targeted at researchers and practitioners in the field of natural language processing and machine learning. The technical nature of the content and the lack of broader context make it less appealing to a general audience.\",\n",
      "    \"potential_for_discussion\": \"The potential for discussion is moderate. While the topic is interesting and relevant to current research trends, the blog's technical nature might limit the number of people who can actively participate in a discussion. The potential for discussion would be higher if the blog included more open-ended questions or highlighted the broader implications of the research.\",\n",
      "    \"overall_engagement_level\": \"Average\"\n",
      "  },\n",
      "  \"possible_improvements\": [\n",
      "    \"Simplify the language and reduce the use of technical jargon to improve readability.\",\n",
      "    \"Provide more context and explanations for the key concepts and terms.\",\n",
      "    \"Add more concrete examples to illustrate the practical implications of the research.\",\n",
      "    \"Reframe the title to be more engaging and appeal to a broader audience.\",\n",
      "    \"Include open-ended questions to encourage discussion and engagement.\",\n",
      "    \"Elaborate on the broader implications of the research and its potential impact.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:56:18.465836Z",
     "start_time": "2025-04-01T21:56:12.837295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generated knowledge\n",
    "test_prompt(prompt_generated_knowledge, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "d01a4933256406e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1395, 'output_tokens': 904, 'total_tokens': 2299, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "```json\n",
      "{\n",
      "  \"key_analysis\": {\n",
      "    \"readability\": \"Assess sentence structure, vocabulary, and the use of jargon. Aim for clarity and conciseness, making the content accessible to a broad audience without sacrificing accuracy.\",\n",
      "    \"structure\": \"Evaluate the logical flow of information, the use of headings and subheadings, and the overall organization of the blog post. A well-structured post is easy to follow and understand.\",\n",
      "    \"informativeness\": \"Determine the depth and breadth of the information provided. Does the blog post adequately explain the research and its implications? Is the information accurate and up-to-date?\",\n",
      "    \"attractiveness_of_blog_title\": \"Analyze how well the title captures the essence of the blog post and its ability to draw the reader in. A good title is both informative and intriguing.\",\n",
      "    \"clarity\": \"Examine the lucidity of the explanations and the absence of ambiguity. Ensure that complex concepts are simplified without oversimplification.\",\n",
      "    \"audience_appeal\": \"Consider the target audience and the extent to which the blog post caters to their interests and knowledge level. A relevant and engaging post will resonate with its intended readership.\",\n",
      "    \"potential_for_discussion\": \"Evaluate the blog post's ability to stimulate thought and encourage interaction. Does it present novel ideas, pose questions, or invite comments and debate?\"\n",
      "  },\n",
      "  \"readability\": \"Good. The language is mostly clear, but some sentences are complex and may require rereading for non-experts. The use of technical terms like 'Proximal Policy Optimization (PPO)' and 'pairwise logistic loss' without immediate explanation slightly reduces readability for a general audience.\",\n",
      "  \"structure\": \"Very Good. The blog post is well-structured with clear headings and subheadings that logically guide the reader through the content. The sections 'Current challenge', 'Methodology', 'Results', and 'Summary' provide a coherent flow of information.\",\n",
      "  \"informativeness\": \"Good. The blog post provides a good overview of the research paper, explaining the problem, proposed solution (Critic-RM), and results. However, more details on the specific benchmarks (RewardBench and CrossEval) and their relevance would enhance the informativeness.\",\n",
      "  \"attractiveness_of_blog_title\": \"Average. The title 'Self-Generated Critiques Boost Reward Modeling for Language Models — Paper Review' is informative but lacks a strong hook to immediately grab the attention of a broader audience. It's very specific, which may deter casual readers.\",\n",
      "  \"clarity\": \"Good. The explanations are generally clear, especially regarding the methodology and the benefits of Critic-RM. However, the mathematical notation  $$r = r_\\\\psi(x, [y, z])$$, while concise, may not be easily understood by all readers.\",\n",
      "  \"audience_appeal\": \"Average. The blog post primarily appeals to readers with some background in machine learning and natural language processing. The level of technical detail and the focus on specific models and algorithms might alienate those without prior knowledge.\",\n",
      "  \"potential_for_discussion\": \"Good. The blog post presents a novel approach (Critic-RM) and highlights its benefits, which could stimulate discussion among researchers and practitioners in the field. The mention of improved performance on specific benchmarks also provides a basis for further investigation and debate.\",\n",
      "  \"overall_engagement_level\": \"Good\",\n",
      "  \"possible_improvements\": [\n",
      "    \"Provide brief, accessible explanations or definitions of key technical terms (e.g., RLHF, PPO, pairwise logistic loss) early in the blog post or in a glossary to improve readability for a broader audience.\",\n",
      "    \"Add more context and details about the benchmarks (RewardBench and CrossEval) to enhance the informativeness and allow readers to better understand the significance of the results.\",\n",
      "    \"Consider including an analogy or real-world example to illustrate the problem of reward modeling and how Critic-RM addresses it, thereby increasing audience appeal.\",\n",
      "    \"Rephrase the mathematical notation $$r = r_\\\\psi(x, [y, z])$$ in plain language or provide a more intuitive explanation to improve clarity for non-technical readers.\",\n",
      "    \"Pose explicit questions at the end of the blog post to directly encourage discussion and engagement from readers.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T21:56:22.663519Z",
     "start_time": "2025-04-01T21:56:18.512749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Meta prompt\n",
    "test_prompt(prompt_meta, gemini_2_flash, {\"blog_text\": blog_text})"
   ],
   "id": "fd51953b104b8476",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "{'input_tokens': 1595, 'output_tokens': 709, 'total_tokens': 2304, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Content:\n",
      "```json\n",
      "{\n",
      "    \"Referenced blog to evaluate\": \"Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\",\n",
      "    \"Step 1\": \"Analyze the readability of the blog. Is the text easy to understand? Are the sentences clear and well-structured? The readability is moderate. While the sentences are generally well-structured, the technical jargon and concepts related to LLMs and reinforcement learning might be challenging for readers without a background in the field. The use of abbreviations like RLHF and PPO without consistent initial explanation hurts readability.\",\n",
      "    \"Step 2\": \"Evaluate the structure of the blog. Does it follow a logical flow? Are the sections well-organized? The structure is logical. It starts with an introduction to RLHF and reward models, presents the challenges, explains the methodology of Critic-RM, discusses the results, and provides a summary. The section headings are clear and helpful.\",\n",
      "    \"Step 3\": \"Consider the informativeness. Does the blog provide valuable, well-researched information? The blog is informative, providing a concise overview of the Critic-RM framework and its benefits. It references the original paper, allowing readers to delve deeper. However, it remains a high-level overview.\",\n",
      "    \"Step 4\": \"Analyze the attractiveness of the blog title. Is the title engaging and does it accurately represent the content of the blog? The title is reasonably engaging and accurately reflects the content of the blog, highlighting the core concept of self-generated critiques and their impact on reward modeling.\",\n",
      "    \"Step 5\": \"Evaluate the clarity of the blog. Are the ideas presented clearly, without ambiguity or unnecessary complexity? The clarity is good overall, but the explanation of the Critic-RM methodology could benefit from more simplification and concrete examples. The mathematical notation might also be intimidating for some readers. It assumes high familiarity with reinforcement learning concepts.\",\n",
      "    \"Step 6\": \"Assess the audience appeal. Would the intended audience find the blog interesting? Is the tone appropriate for the target readers? The blog would likely appeal to researchers and practitioners in the field of LLMs and reinforcement learning. The tone is professional and technical, suitable for this audience.\",\n",
      "    \"Step 7\": \"Consider the potential for discussion. Does the blog invite the reader to engage in further thought or discussion? The blog has some potential for discussion, particularly regarding the limitations of the Critic-RM framework and its applicability to different types of LLMs and tasks.\",\n",
      "    \"Step 8\": \"After completing the analysis for each criterion, summarize the overall engagement level using one of the following ratings: \\\"Excellent\\\", \\\"Very Good\\\", \\\"Good\\\", \\\"Average\\\", \\\"Bad\\\".\",\n",
      "    \"Step 9\": \"Write down possible improvements to the blog based on your analysis.\",\n",
      "    \"Overall engagement level\": \"Good\",\n",
      "    \"Possible improvements\": [\n",
      "        \"Provide more context and explanation of technical terms and concepts (e.g., RLHF, PPO) to make the blog accessible to a wider audience.\",\n",
      "        \"Simplify the explanation of the Critic-RM methodology with more concrete examples and less jargon.\",\n",
      "        \"Consider replacing or explaining the mathematical notation to avoid intimidating less technically inclined readers.\",\n",
      "        \"Add a section discussing the limitations of the Critic-RM framework and potential areas for future research.\",\n",
      "        \"Include a call to action or questions to encourage reader engagement and discussion.\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance testing on a validation set",
   "id": "42ba3b4016446497"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Models\n",
    "gemini_2_flash_structured = gemini_2_flash.with_structured_output(BlogEvaluation, include_raw=True)\n",
    "gemini_2_flash_lite_structured = gemini_2_flash_lite.with_structured_output(BlogEvaluation, include_raw=True)\n",
    "gemini_2_flash_thinking_structured = gemini_2_flash_thinking.with_structured_output(BlogEvaluation, include_raw=True)"
   ],
   "id": "1948429058ba2465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Numeric evaluation with two-shot prompt\n",
   "id": "5a89b91f6ae71f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash",
   "id": "4def44ceeac2464c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T00:18:48.176770Z",
     "start_time": "2025-03-07T00:10:26.683066Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: 75.0\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: 78.0\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: 68.0\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: 55.0\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: 30.0\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: 68.0\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: 78.0\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: 78.0\n",
      "\n",
      "Error processing blog \"What is the System 2 LLM or AI Chatbot?\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: 75.0\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "Error processing blog \"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: 75.0\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: 78.0\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: 78.0\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: 79.0\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: 75.0\n",
      "\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: 75.0\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: 65.0\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: 95.0\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: 80.0\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: 89.0\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: 85.0\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: 72.0\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: 90.0\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: 78.0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25,
   "source": [
    "flash_assessment = extract_llm_assessment(Xval, prompt_two_shots, gemini_2_flash_structured,\n",
    "                                        {\n",
    "                                            \"blog_ex1\" : best_blog[\"blog\"],\n",
    "                                            \"score_ex1\" : best_blog[\"score\"],\n",
    "                                            \"blog_ex2\" : worst_blog[\"blog\"],\n",
    "                                            \"score_ex2\" : worst_blog[\"score\"]\n",
    "                                        })"
   ],
   "id": "da5079b87b06741b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T00:18:48.249109Z",
     "start_time": "2025-03-07T00:18:48.238297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_RMSE_val = metrics.root_mean_squared_error(yval_score, flash_assessment)\n",
    "print(f\"Root Mean Square Error on validation set: {flash_RMSE_val:.1f}\")\n",
    "flash_MAE_val = metrics.mean_absolute_error(yval_score, flash_assessment)\n",
    "print(f\"Mean Absolute Error on validation set: {flash_MAE_val:.1f}\")"
   ],
   "id": "e9044640f219f9e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on validation set: 43.7\n",
      "Mean Absolute Error on validation set: 36.2\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash-Lite",
   "id": "1dfab44828216871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash Thinking Experimental 01-21",
   "id": "aed226d302a1a5cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalized numeric evaluation with two-shot prompt",
   "id": "17f55795a4a23bed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash",
   "id": "d8d07b699d01dbe4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T00:18:48.317750Z",
     "start_time": "2025-03-07T00:18:48.304407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_assessment_scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "scaled_flash_assessment = flash_assessment_scaler.fit_transform(flash_assessment.values.reshape(-1, 1))\n",
    "flash_RMSE_val_scaled = metrics.root_mean_squared_error(yval_score, scaled_flash_assessment)\n",
    "print(f\"Root Mean Square Error on validation set after MinMax normalization of LLM output: {flash_RMSE_val_scaled:.1f}\")\n",
    "flash_MAE_val_scaled = metrics.mean_absolute_error(yval_score, scaled_flash_assessment)\n",
    "print(f\"Mean Absolute Error on validation set after MinMax normalization of LLM output: {flash_MAE_val_scaled:.1f}\")"
   ],
   "id": "31b95d26c369dc8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on validation set after MinMax normalization of LLM output: 40.7\n",
      "Mean Absolute Error on validation set after MinMax normalization of LLM output: 32.2\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash-Lite",
   "id": "54eccbfdf87f73e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash Thinking Experimental 01-21",
   "id": "7f31b27d5c9cb735"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification with five-shot prompt",
   "id": "3ebdf65a1e8e0c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T00:18:48.395372Z",
     "start_time": "2025-03-07T00:18:48.385281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Models\n",
    "gemini_2_flash_structured = gemini_2_flash.with_structured_output(BlogClassification, include_raw=True)\n",
    "gemini_2_flash_lite_structured = gemini_2_flash_lite.with_structured_output(BlogClassification, include_raw=True)\n",
    "gemini_2_flash_thinking_structured = gemini_2_flash_thinking.with_structured_output(BlogClassification, include_raw=True)"
   ],
   "id": "d1dcf1c333bb0dd4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash",
   "id": "2877d4bdc1df2522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T00:36:41.678112Z",
     "start_time": "2025-03-07T00:29:04.780631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_classification = extract_llm_assessment(Xval, prompt_five_shots, gemini_2_flash_structured,\n",
    "                                            examples=\n",
    "                                            {\n",
    "                                                \"excellent_blog\" : excellent_blog,\n",
    "                                                \"very_good_blog\" : very_good_blog,\n",
    "                                                \"good_blog\" : good_blog,\n",
    "                                                \"average_blog\" : average_blog,\n",
    "                                                \"bad_blog\" : bad_blog\n",
    "                                            })"
   ],
   "id": "2dfd4534cbd2fd9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: Average\n",
      "\n",
      "Error processing blog \"What is the System 2 LLM or AI Chatbot?\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: Excellent\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: Bad\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: Excellent\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T00:36:41.741015Z",
     "start_time": "2025-03-07T00:36:41.734801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_accuracy = metrics.accuracy_score(yval_level, flash_classification)\n",
    "print(f\"Accuracy score on validation set: {flash_accuracy * 100:.2f}%\")\n",
    "\n",
    "flash_RMSE_val_verbal = metrics.root_mean_squared_error(yval_level.map(CLASSIFICATION_MAP), flash_classification.map(CLASSIFICATION_MAP))\n",
    "flash_MAE_val_verbal = metrics.mean_absolute_error(yval_level.map(CLASSIFICATION_MAP), flash_classification.map(CLASSIFICATION_MAP))\n",
    "print(f\"Root Mean Square Error on validation set after using the verbal classification: {flash_RMSE_val_verbal:.1f}\")\n",
    "print(f\"Mean Absolute Error on validation set after using verbal classification: {flash_MAE_val_verbal:.1f}\")"
   ],
   "id": "6a4be2f4c2ba6fd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on validation set: 40.00%\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash-Lite",
   "id": "40427d9275d23c4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash Thinking Experimental 01-21",
   "id": "7901f9b549df92eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification with Chain-of-Thoughts prompt",
   "id": "dc053bdfc57fb475"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:33:56.906862Z",
     "start_time": "2025-03-27T14:33:56.901472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemini_2_flash_structured = gemini_2_flash.with_structured_output(BlogClassificationCoT, include_raw=True)\n",
    "gemini_2_flash_lite_structured = gemini_2_flash_lite.with_structured_output(BlogClassificationCoT, include_raw=True)\n",
    "gemini_2_flash_thinking_structured = gemini_2_flash_thinking.with_structured_output(BlogClassificationCoT, include_raw=True)"
   ],
   "id": "670f3957a8d39e44",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash",
   "id": "d75bf334e480ba3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:39:59.601789Z",
     "start_time": "2025-03-27T14:33:57.944379Z"
    }
   },
   "cell_type": "code",
   "source": "flash_cot_classification = extract_llm_assessment(Xval, prompt_zero_cot, gemini_2_flash_structured, examples={})",
   "id": "a50232b9cfd23652",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: Very Good\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:41:00.465529Z",
     "start_time": "2025-03-27T14:41:00.454848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_cot_accuracy = metrics.accuracy_score(yval_level, flash_cot_classification)\n",
    "print(f\"Accuracy score on validation set: {flash_cot_accuracy * 100:.2f}%\")\n",
    "\n",
    "flash_RMSE_val_cot = metrics.root_mean_squared_error(yval_level.map(CLASSIFICATION_MAP), flash_cot_classification.map(CLASSIFICATION_MAP))\n",
    "flash_MAE_val_cot = metrics.mean_absolute_error(yval_level.map(CLASSIFICATION_MAP), flash_cot_classification.map(CLASSIFICATION_MAP))\n",
    "print(f\"Root Mean Square Error on validation set after using Chain-of-Thoughts: {flash_RMSE_val_cot:.1f}\")\n",
    "print(f\"Mean Absolute Error on validation set after using verbal Chain-of-Thoughts: {flash_MAE_val_cot:.1f}\")"
   ],
   "id": "550a0e50096ef385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error on validation set after using the verbal classification: 1.6\n",
      "Mean Absolute Error on validation set after using verbal classification: 1.4\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash-Lite",
   "id": "6032517d7976c3b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash Thinking Experimental 01-21",
   "id": "f94e815f060f5ac2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification with generated knowledge prompt",
   "id": "e250e83e9f9e71b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:56:47.874146Z",
     "start_time": "2025-03-27T14:56:47.867332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemini_2_flash_structured = gemini_2_flash.with_structured_output(BlogClassificationGK, include_raw=True)\n",
    "gemini_2_flash_lite_structured = gemini_2_flash_lite.with_structured_output(BlogClassificationGK, include_raw=True)\n",
    "gemini_2_flash_thinking_structured = gemini_2_flash_thinking.with_structured_output(BlogClassificationGK, include_raw=True)"
   ],
   "id": "363b342044992416",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash",
   "id": "ec775e5bc1c1d463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:04:42.583400Z",
     "start_time": "2025-03-27T14:56:48.890750Z"
    }
   },
   "cell_type": "code",
   "source": "flash_gk_classification = extract_llm_assessment(Xval, prompt_generated_knowledge, gemini_2_flash_structured, examples={})",
   "id": "e47f7b6d1fbc5563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "Error processing blog \"What is the System 2 LLM or AI Chatbot?\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "Error processing blog \"What is the System 2 LLM or AI Chatbot?\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: Very Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:04:42.638521Z",
     "start_time": "2025-03-27T15:04:42.629198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_gk_accuracy = metrics.accuracy_score(yval_level, flash_gk_classification)\n",
    "print(f\"Accuracy score on validation set: {flash_gk_accuracy * 100:.2f}%\")\n",
    "\n",
    "flash_RMSE_val_gk = metrics.root_mean_squared_error(yval_level.map(CLASSIFICATION_MAP), flash_gk_classification.map(CLASSIFICATION_MAP))\n",
    "flash_MAE_val_gk = metrics.mean_absolute_error(yval_level.map(CLASSIFICATION_MAP), flash_gk_classification.map(CLASSIFICATION_MAP))\n",
    "print(f\"Root Mean Square Error on validation set after using generated knowledge: {flash_RMSE_val_gk:.1f}\")\n",
    "print(f\"Mean Absolute Error on validation set after using generated knowledge: {flash_MAE_val_gk:.1f}\")"
   ],
   "id": "77cf2859a62f6568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on validation set: 16.67%\n",
      "Root Mean Square Error on validation set after using the verbal classification: 1.6\n",
      "Mean Absolute Error on validation set after using verbal classification: 1.4\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash-Lite",
   "id": "9ceffcb13c483c5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash Thinking Experimental 01-21",
   "id": "2e3cf1d715f15939"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification with Meta prompt",
   "id": "7a9ec025c0557b24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:04:42.684172Z",
     "start_time": "2025-03-27T15:04:42.674395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemini_2_flash_structured = gemini_2_flash.with_structured_output(BlogClassificationMP, include_raw=True)\n",
    "gemini_2_flash_lite_structured = gemini_2_flash_lite.with_structured_output(BlogClassificationMP, include_raw=True)\n",
    "gemini_2_flash_thinking_structured = gemini_2_flash_thinking.with_structured_output(BlogClassificationMP, include_raw=True)"
   ],
   "id": "cdc283c4aec76168",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash",
   "id": "eac4d58afa00abaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:13:32.004363Z",
     "start_time": "2025-03-27T15:04:42.737855Z"
    }
   },
   "cell_type": "code",
   "source": "flash_mp_classification = extract_llm_assessment(Xval, prompt_meta, gemini_2_flash_structured, examples={})",
   "id": "8df3c8603f1f8cb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Blog ID: 35\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 21\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Innateness, AlphaZero, and Artificial Intelligence\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 31\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Acquisition of Chess Knowledge in AlphaZero\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 20\n",
      "Blog title: ChatGPT vs Bing … and the urgent need for Responsible AI\n",
      "Referenced paper title: Adaptive Test Generation Using a Large Language Model\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 48\n",
      "Blog title: Data Centric AI — LLAVA\n",
      "Referenced paper title: Visual Instruction Tuning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 18\n",
      "Blog title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "Referenced paper title: SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 30\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: Artificial Intelligence for the Metaverse: A Survey\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 8\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 19\n",
      "Blog title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "Referenced paper title: Recurrent drafter for fast speculative decoding in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "Error processing blog \"What is the System 2 LLM or AI Chatbot?\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "----------\n",
      "Blog ID: 7\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 27\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Proximal Policy Optimization Algorithms\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 5\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Guiding Language Model Reasoning with Planning Tokens\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 24\n",
      "Blog title: Evaluating The AI Scientist\n",
      "Referenced paper title: The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 22\n",
      "Blog title: Reflections on Innateness in Machine Learning\n",
      "Referenced paper title: Implicit Regularization in Deep Learning\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 36\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: AutoGPT+P: Affordance-based Task Planning with Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 12\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Let's Verify Step by Step\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 3\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 6\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Distilling System 2 into System 1\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 40\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Learning to Use Tools via Cooperative and Interactive Agents\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 46\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
      "LLM Assessment: Average\n",
      "\n",
      "Error processing blog \"Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\":\n",
      "'NoneType' object has no attribute 'overall_assessment'\n",
      " Retrying...\n",
      "----------\n",
      "Blog ID: 17\n",
      "Blog title: Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Referenced paper title: Self-Generated Critiques Boost Reward Modeling for Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 26\n",
      "Blog title: Deep Reinforcement Learning: The Algorithms for Game Dev\n",
      "Referenced paper title: Playing Atari with Deep Reinforcement Learning\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 32\n",
      "Blog title: How reinforcement learning affects human behavior?\n",
      "Referenced paper title: A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 28\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 47\n",
      "Blog title: How AI Is Changing the Way We Code\n",
      "Referenced paper title: From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 9\n",
      "Blog title: What is the System 2 LLM or AI Chatbot?\n",
      "Referenced paper title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "LLM Assessment: Average\n",
      "\n",
      "----------\n",
      "Blog ID: 34\n",
      "Blog title: Do I need to be polite to my LLM?\n",
      "Referenced paper title: Do Llamas Work in English? On the Latent Language of Multilingual Transformers\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 38\n",
      "Blog title: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
      "Referenced paper title: From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 29\n",
      "Blog title: Towards Reasoning\n",
      "Referenced paper title: Chain-of-Thought Reasoning Without Prompting\n",
      "LLM Assessment: Good\n",
      "\n",
      "----------\n",
      "Blog ID: 1\n",
      "Blog title: Training Large Language Models: From TRPO to GRPO\n",
      "Referenced paper title: Foundations of Large Language Models\n",
      "LLM Assessment: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:13:32.089741Z",
     "start_time": "2025-03-27T15:13:32.081390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flash_mp_accuracy = metrics.accuracy_score(yval_level, flash_mp_classification)\n",
    "print(f\"Accuracy score on validation set: {flash_mp_accuracy * 100:.2f}%\")\n",
    "\n",
    "flash_RMSE_val_mp = metrics.root_mean_squared_error(yval_level.map(CLASSIFICATION_MAP), flash_mp_classification.map(CLASSIFICATION_MAP))\n",
    "flash_MAE_val_mp = metrics.mean_absolute_error(yval_level.map(CLASSIFICATION_MAP), flash_mp_classification.map(CLASSIFICATION_MAP))\n",
    "print(f\"Root Mean Square Error on validation set after using the verbal classification: {flash_RMSE_val_mp:.1f}\")\n",
    "print(f\"Mean Absolute Error on validation set after using verbal classification: {flash_MAE_val_mp:.1f}\")"
   ],
   "id": "ac062eeb66bd3762",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on validation set: 23.33%\n",
      "Root Mean Square Error on validation set after using the verbal classification: 1.6\n",
      "Mean Absolute Error on validation set after using verbal classification: 1.3\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash-Lite",
   "id": "8db210ded0978ef5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gemini 2.0 Flash Thinking Experimental 01-21",
   "id": "6fed36a6ffe26ab1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
