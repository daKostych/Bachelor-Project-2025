{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:43:58.690044Z",
     "start_time": "2025-04-09T16:43:57.455355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "if str(Path().resolve().parent) not in sys.path:\n",
    "    sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from src.config import PREPROCESSED_BLOG_DATASET_PATH\n",
    "from src.text_extraction import extract_blog_text, extract_paper_text"
   ],
   "id": "f7b16213871bdb30",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data processing",
   "id": "da71452052f6fd8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:43:58.708709Z",
     "start_time": "2025-04-09T16:43:58.694539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data\n",
    "blogs = pd.read_csv(PREPROCESSED_BLOG_DATASET_PATH)\n",
    "blog = blogs.iloc[0]"
   ],
   "id": "6be1fec88d3c88f2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:44:07.098707Z",
     "start_time": "2025-04-09T16:43:58.933889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracting clean text from a blog\n",
    "blog_text = extract_blog_text(blog=blog)\n",
    "print(blog_text)"
   ],
   "id": "85f27b30b879d75b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review\n",
      "Paper — https://arxiv.org/pdf/2411.16646\n",
      "Reinforcement Learning from Human Feedback (RLHF) has become a critical methodology for aligning large language models (LLMs) with human preferences. At the core of RLHF lies the reward model (RM), which is designed to evaluate model outputs by assigning scores that reflect their alignment with human judgments. These scores guide the optimization process during training, such as providing reward signals in Proximal Policy Optimization (PPO), thereby encouraging LLMs to generate responses that are more helpful, honest, and harmless. This iterative process enhances the practical quality of LLM outputs in real-world applications.\n",
      "\n",
      "## Current challenge\n",
      "Typically, reward models are trained using preference pairs and optimized through pairwise logistic loss to produce a scalar score for each response. However, this scalar output is often hard to interpret and underutilizes the inherent language modeling capabilities of LLMs derived from pretraining and post-training. These limitations can weaken the feedback signals in RLHF, resulting in suboptimal policy updates. An alternative approach is the “LLM-as-a-judge” paradigm, where the LLM generates critiques and optionally provides discrete scores as proxies for response quality. This method leverages the model’s reasoning abilities more effectively, potentially addressing some of the shortcomings of traditional reward models\n",
      "Incorporating critiques into reward modeling poses two significant challenges. First, there is the issue of conflicting objectives: generating critiques relies on language modeling capabilities, whereas traditional reward models output scalar values, making their integration into the language modeling process complex. Second, there are limitations with evaluators; off-the-shelf language models often lack the effectiveness needed for evaluation, and fine-tuning these models necessitates costly human-generated or annotated critiques. While knowledge distillation offers a potential solution by enabling a joint training approach to learn how to generate critiques and rewards simultaneously, it falls short when it comes to enhancing frontier models in situations where a stronger teacher model is unavailable. Here is Critic-RM, a new framework from Meta Researchers that enhances reward models using synthetic critiques, without relying on strong LLM teachers.\n",
      "\n",
      "## Methodology\n",
      "Critic-RM utilizes an instruction-finetuned large language model (LLM) as its foundation, generating multiple candidate critiques, each accompanied by a discrete score for individual responses. The process begins with a consistency-guided filtering technique that retains only those critiques whose scores align with human-annotated preference labels.\n",
      "To further improve the quality of these synthetic critiques, two additional strategies — summarization and ranking — are proposed to refine the critiques used in training the reward model. The framework investigates the application of an off-the-shelf instruction-finetuned LLM, for both critique generation and reward modeling. Initially, Critic-RM generates candidate critiques for each prompt-response pair, followed by a filtering step aimed at minimizing the influence of potentially noisy rationales that could lead to incorrect predictions. This approach allows for the augmentation of preference pairs with additional critiques, ultimately enhancing the precision of reward modeling.\n",
      "After generating critiques for each response, the primary challenge is developing an effective training strategy to integrate critique modeling and scalar reward prediction objectives. To address this, we propose a simple weighting strategy that balances these objectives. Initially, the model prioritizes critique modeling loss, then gradually shifts its focus toward reward prediction, utilizing both the response and the critique. This balanced approach enables the model to excel in generating high-quality critiques while maintaining accurate reward predictions.\n",
      "In Critic-RM, an additional step is introduced during inference for each (prompt, response) pair. Given a (prompt, response) pair $$(x, y)$$, the model first generates a critique $$z \\sim q_\\phi(x, y)$$. It then predicts the reward for the response as $$r = r_\\psi(x, [y, z])$$, where the reward prediction incorporates both the response and its associated critique. This process ensures a more nuanced and precise evaluation of responses.\n",
      "\n",
      "## Results\n",
      "Incorporating critiques into reward modeling has demonstrated significant benefits, particularly with the Critic-RM framework, which consistently outperforms the baselines in this study. Specifically, when trained on the same preference data, Critic-RM achieves an improvement of 3.7% to 4.7% over standard reward models. The quality of critiques plays a crucial role; comparisons reveal that other models incorporating critiques show smaller performance gains than Critic-RM when evaluated against the standard reward model. Additionally, performance enhancements are observed for both Critic-RM and its baselines when multiple critiques are generated during inference, particularly benefiting reasoning tasks.\n",
      "\n",
      "## Summary\n",
      "Researchers have developed Critic-RM, an innovative self-critiquing framework aimed at enhancing reward modeling for large language models (LLMs). This novel approach leverages the inherent capabilities of LLMs to generate and refine critiques, implementing a self-improvement mechanism that enhances both the quality of critiques and the accuracy of reward predictions. The findings from this research emphasize the efficacy of Critic-RM in improving reward modeling accuracy and underscore the crucial role of high-quality critiques in this process. Beyond merely enhancing precision in reward modeling, the framework has demonstrated robust performance across various benchmarks, including RewardBench and CrossEval, highlighting its potential to significantly advance the field of language model optimization and alignment with human preferences.\n",
      "\n",
      "## Appendix\n",
      "    * Paper link — https://arxiv.org/pdf/2411.16646\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:44:08.481479Z",
     "start_time": "2025-04-09T16:44:07.164970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracting clean text from a scientific paper\n",
    "paper_url = blogs[\"url_paper\"][0]\n",
    "paper_text = extract_paper_text(paper_url)\n",
    "print(paper_text)"
   ],
   "id": "8f8c3cfc4c5a28a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Generated Critiques Boost Reward Modeling\n",
      "for Language Models\n",
      "Yue Yu1,2,∗, Zhengxing Chen1, Aston Zhang1, Liang Tan1, Chenguang Zhu1, Richard Yuanzhe Pang1, Yundi\n",
      "Qian1, Xuewei Wang1, Suchin Gururangan1, Chao Zhang2, Melanie Kambadur1, Dhruv Mahajan1, Rui Hou1\n",
      "1GenAI, Meta, 2Georgia Institute of Technology\n",
      "∗Work done during the internship at Meta GenAI.\n",
      "Reward modeling is crucial for aligning large language models (LLMs) with human preferences,\n",
      "especially in reinforcement learning from human feedback (RLHF). However, current reward models\n",
      "mainly produce unexplainable scalar scores and struggle to incorporate critiques in a natural language\n",
      "format. We hypothesize that generating both critiques and scalar rewards would improve reward\n",
      "models’ capability on preference ranking. Motivated by this, we propose Critic-RM, a framework\n",
      "that utilizes self-generated, high-quality critiques to train reward models for scalar reward-based\n",
      "preference prediction, with explicit rationales serving as supporting evidence. Critic-RM employs\n",
      "a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on\n",
      "reward prediction and critique generation objectives. Experiments on preference ranking benchmarks\n",
      "including RewardBench and CrossEval show that Critic-RM improves reward modeling accuracy by\n",
      "3.7%–7.3% compared to standard reward models and LLM judges, demonstrating strong performance\n",
      "and data efficiency. Additional studies further validate the effectiveness of the generated critiques in\n",
      "rectifying flawed reasoning steps with the gain of 2.5%-3.2% on improving reasoning accuracy.\n",
      "Date: February 11, 2025\n",
      "Correspondence: Yue Yu (yueyu@gatech.edu), Rui Hou (rayhou@meta.com)\n",
      "1\n",
      "Introduction\n",
      "Reinforcement Learning from Human Feedback (RLHF) has been widely adopted to align large language\n",
      "models (LLMs) with human preferences (Ouyang et al., 2022; Touvron et al., 2023; Dubey et al., 2024; Reid\n",
      "et al., 2024). Central to the RLHF process is the reward model (RM), which is trained to assign scores that\n",
      "quantify how well the model’s outputs align with human judgments. The reward model defines optimization\n",
      "direction during training (e.g., reward signal in PPO), encouraging a policy LLM to generate more helpful,\n",
      "honest, and harmless responses ultimately enhancing the model’s generation quality in real-world applications.\n",
      "Standard reward models are typically trained using preference pairs and optimized with pairwise logistic\n",
      "loss (Bradley and Terry, 1952), producing a single scalar score for each response. However, outputting a\n",
      "scalar score not only is hard to interpret but also fails to fully leverage the inherent language modeling\n",
      "capability that LLMs obtain from pretraining and post-training (Zhang et al., 2024). Consequently, these\n",
      "reward models tend to be less data-efficient and prone to robustness issues, such as reward hacking (Skalse\n",
      "et al., 2022; Singhal et al., 2023; Chen et al., 2024). Such limitations hinder the quality of feedback signals in\n",
      "RLHF and lead to suboptimal policy updates. On the other hand, the LLM-as-a-judge paradigm offers an\n",
      "alternative, where the LLM first generates a critique and then optionally provides a discrete score as a quality\n",
      "proxy for a response (Zheng et al., 2023; Kim et al., 2024a; Zhong et al., 2024). Combining the strengths of\n",
      "both paradigms—integrating the interpretability and structured critique of LLM-as-the-judge with the scalar\n",
      "optimization framework of reward models—has the great potential to address the limitations of each method\n",
      "and yield more robust and effective reward signals.\n",
      "Despite its great premise, incorporating critiques into reward modeling presents two major challenges. (1)\n",
      "Conflicting objectives: Critique generation requires language modeling, while reward models provide scalar\n",
      "outputs, complicating its integration into language modeling. (2) Evaluator limitations: Off-the-shelf LMs are\n",
      "often not good evaluators, while additional fine-tuning requires costly human-generated or annotated critiques.\n",
      "1\n",
      "arXiv:2411.16646v3  [cs.CL]  9 Feb 2025\n",
      "\n",
      "Recent work (Ye et al., 2024) directly incorporates critiques generated from off-the-shelf LLMs for reward\n",
      "modeling, while Ankner et al. (2024) and Zhang et al. (2024) design a joint training approach for learning to\n",
      "generate the critique as well as rewards simultaneously via knowledge distillation. These methods typically\n",
      "rely on a strong teacher LLM to generate high-quality critiques, which can be costly and inefficient to obtain\n",
      "at scale in practice. Moreover, they cannot be used to improve frontier models when a stronger teacher model\n",
      "does not exist.\n",
      "We introduce Critic-RM, a new framework that enhances reward models using synthetic critiques, without\n",
      "relying on strong LLM teachers. Our approach draws inspiration from recent advances in self-improving\n",
      "language models (Yuan et al., 2024; Wu et al., 2024; Prasad et al., 2024), where models are iteratively refined\n",
      "using data generated by themselves. To apply a similar LLM self-improving paradigm in reward modeling,\n",
      "we hypothesize that it is crucial to inject LLM’s critique generation ability into this process. Specifically,\n",
      "Critic-RM leverages an instruction-finetuned LLM as the backbone, which generates multiple candidate\n",
      "critiques, each with a discrete score (as explained below, for filtering critiques; not our final reward) for\n",
      "individual responses. However, these critiques can vary in quality, and poor-quality critiques often result\n",
      "in flawed quality predictions. To tackle this issue, we first apply a consistency-guided filtering technique,\n",
      "retaining only critiques whose scores align with human-annotated preference labels1. To further enhance the\n",
      "quality of these synthetic critiques, we additionally propose two strategies, summarization and ranking, to\n",
      "refine the critiques used in training the reward model.\n",
      "Once critiques are generated for each response, the main challenge lies in designing an effective training strategy\n",
      "to combine critique modeling and scalar reward prediction objectives. While LLMs benefit from learning\n",
      "through diverse critiques for each response (Ho et al., 2023), reward modeling is prone to overfitting (Dubey\n",
      "et al., 2024; Zhu et al., 2024); such a contradiction makes it nontrivial to determine the optimal learning steps.\n",
      "To address this issue, we introduce a simple weighting balancing strategy, where the model initially focuses on\n",
      "critique modeling loss, then gradually transitions to predicting rewards based on both the response and the\n",
      "critique. This approach balances the two learning objectives, allowing the model to excel at both high-quality\n",
      "critique generation and accurate reward prediction.\n",
      "To demonstrate the effectiveness of Critic-RM, we conduct extensive experiments on RewardBench and three\n",
      "out-of-distribution reward modeling tasks, showing that Critic-RM outperforms baselines in both in-domain\n",
      "and out-of-domain evaluations. Additionally, experiments on critique evaluation benchmarks highlight Critic-\n",
      "RM’s ability to generate valuable feedback for correcting LLMs’ flawed reasoning. Our analysis confirms that\n",
      "Critic-RM’s superior generalization stems from its ability to identify and leverage high-quality self-generated\n",
      "critiques. The major contributions of our work can be summarized as follows:\n",
      "• We propose Critic-RM, a framework to allow LLMs to take advantage of self-generated critiques for\n",
      "reward modeling. Critic-RM does not rely on additional supervision compared to standard reward\n",
      "models, while enjoying an improved generation quality as well as reward modeling accuracy.\n",
      "• We propose a self-refinement technique to automatically select high-quality critiques, and design a simple\n",
      "yet effective weight scheduling strategy to balance the learning objectives between critique generation\n",
      "and reward modeling. These techniques collaboratively equip the model with the dual capabilities of\n",
      "high-quality critique generation and accurate reward prediction.\n",
      "• We conduct experiments on three benchmarks covering over ten tasks, demonstrating the effectiveness\n",
      "of Critic-RM in precise reward modeling across diverse scenarios. Additional studies confirm the utility\n",
      "of Critic-RM-generated critiques in identifying and correcting mistakes made by LLMs.\n",
      "2\n",
      "Related Work\n",
      "Reward Models. Building an accurate and robust reward model is a critical step for RLHF pipelines. Earlier\n",
      "work trains reward models with the ranking loss between chosen and rejected responses with the Bradley-Terry\n",
      "model (Bradley and Terry, 1952; Stiennon et al., 2020; Ouyang et al., 2022; Dubey et al., 2024). To further\n",
      "improve upon this reward modeling pipeline, Wang et al. (2024e,d,a) design fine-grained attributes to predict\n",
      "1This discrete score is only used for filtering critiques and being different from the final reward score of Critic-RM. Our\n",
      "Critic-RM eventually produces a continuous score, as explained in Section 3.3.\n",
      "2\n",
      "\n",
      "Table 1 Comparison of our proposed method Critic-RM and closest baselines.\n",
      "Baselines\n",
      "Input Format\n",
      "Output Format\n",
      "Critique\n",
      "Generation\n",
      "Require\n",
      "Training\n",
      "Additional\n",
      "Teacher Models\n",
      "Standard RM (Bradley and Terry)\n",
      "Single Response\n",
      "Continuous Score\n",
      "✗\n",
      "✓\n",
      "✗\n",
      "RLAIF (Lee et al.)\n",
      "Single Response\n",
      "Continuous Score\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "LLM-as-a-judge (Zheng et al.)\n",
      "Response Pairs\n",
      "Discrete Score\n",
      "✓\n",
      "✗\n",
      "✗\n",
      "SynRM (Ye et al.)\n",
      "Single Response + Critique\n",
      "Continuous Score\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "CLoud (Ankner et al.)\n",
      "Single Response\n",
      "Critique + Continuous Score\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GenRM (Zhang et al.)\n",
      "Single Response\n",
      "Critique + Reward Token\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Critic-RM (Ours)\n",
      "Single Response\n",
      "Critique + Continuous Score\n",
      "✓\n",
      "✓\n",
      "✗\n",
      "rewards toward different aspects, Chen et al. (2024); Shen et al. (2024b); Liu et al. (2024); Coste et al. (2024)\n",
      "promote the robustness of reward modeling via improved training techniques or model ensembling, and Pace\n",
      "et al. (2024); Shen et al. (2024a) study how to create synthetic examples for reward models. More related\n",
      "to us, several very recent works (concurrent to us) also study generative reward modeling. Ye et al. (2024)\n",
      "directly augment the response with additional critiques from a teacher model for reward modeling without\n",
      "training the RM for critique generation, and Zhang et al. (2024); Ankner et al. (2024); Mahan et al. (2024)\n",
      "attempt to learn reward models with additional critique objectives, with similar focus of our study. However,\n",
      "these methods typically rely on high-quality critiques from stronger teacher models for training, which can be\n",
      "costly and inefficient to obtain in practice. They also don’t provide a solution to reward modeling based on\n",
      "frontier LLMs where a teacher model doesn’t exist. They also lack a unified approach to improve the quality\n",
      "of the critiques. Besides, Zhang et al. (2024) is specific to verifying math problem correctness and is hard to\n",
      "map to subjective domains where there are no ground-truth answers.\n",
      "LLM-as-a-judge and Critique Models. Recently, large language models (LLMs) have been proposed as cost-\n",
      "effective alternatives to human evaluation, and act as proxies for assessing text quality. Such methods often\n",
      "first provide explanations for judgments of the response, then output a discrete score or preference label as\n",
      "the prediction (Zheng et al., 2023; Li et al., 2023; Yan et al., 2024; Xu et al., 2024). CriticGPT (McAleese\n",
      "et al., 2024) has also extended this line of work into coding tasks, where the LLM critic model is fine-tuned to\n",
      "pinpoint problems in code from real-world assistant tasks. However, using off-the-shelf LLMs for evaluation\n",
      "introduces the risk of bias (Bavaresco et al., 2024; Stureborg et al., 2024), and they can be easily misled (Zeng\n",
      "et al., 2024). To address these challenges, recent studies (Wang et al., 2024b; Kim et al., 2024a) have focused\n",
      "on collecting high-quality response pairs to train more accurate and reliable LLM-based evaluators.\n",
      "Self-alignment Techniques. Aligning LLMs with human preferences often requires a massive amount of human\n",
      "annotations. To alleviate this reliance on human efforts, self-alignment leverages the model’s own capabilities\n",
      "to refine its responses and align them with desired behaviors. Saunders et al. (2022); Madaan et al. (2023)\n",
      "use LLM itself to refine the original response at the inference time. Li et al. (2024b) generate instruction\n",
      "prompts for web documents and subsequently select high-quality examples for instruction fine-tuning. Lee\n",
      "et al. (2024); Sun et al. (2024) leverage LLMs to create preference labels efficiently, Yuan et al. (2024) employ\n",
      "LLM itself to rank different responses to provide its own rewards during training, and Zelikman et al. (2022);\n",
      "Pang et al. (2024); Gulcehre et al. (2023) improve LLM reasoning abilities through self-generated reasoning\n",
      "steps. A recent study (Wang et al., 2024b) also employs self-improving techniques to train text evaluators,\n",
      "but it focuses on pairwise evaluation and generating synthetic preference pairs. In contrast, we combine\n",
      "self-generated critiques with human-annotated preference pairs to enhance reward modeling performance.\n",
      "3\n",
      "Methodology\n",
      "3.1\n",
      "Preliminaries\n",
      "Reward Modeling. Let X and Y denote the space of prompts and responses, respectively. In the RLHF pipeline,\n",
      "human feedback is typically collected in the form of pairwise preferences between two responses (y+, y−) ∈Y2\n",
      "to a given prompt x ∈X. Then, the preference dataset can be written as D =\n",
      "\n",
      "(xi, y+\n",
      "i , y−\n",
      "i )\n",
      "\t|D|\n",
      "i=1, where the\n",
      "preference for y+ over y−is denoted as y+ ≻y−. To model the pairwise preferences, the learning objective is\n",
      "3\n",
      "\n",
      "∑!\n",
      "\" \"!\n",
      "#\n",
      "#\n",
      ">\n",
      "∑!\n",
      "\" \"!\n",
      "$\n",
      "#\n",
      "Instance-level \n",
      "Critique Filtering\n",
      "Quality-aware \n",
      "Critique Refinement\n",
      "\"\"$%% = {%&, … , %'}\n",
      "\"()*' = {%&, … , %'}\n",
      "Summarize ) critiques\n",
      "Rank the top ) critiques\n",
      "Llama\n",
      "70B\n",
      "*%&\n",
      "+\n",
      "⋮\n",
      ",\n",
      "%#\n",
      "+\n",
      "*%&\n",
      ",\n",
      "⋮\n",
      ",\n",
      "%#\n",
      ",\n",
      "Critiques\n",
      "-&\n",
      "+\n",
      "⋮\n",
      "-#\n",
      "+\n",
      "-&\n",
      ",\n",
      "⋮\n",
      "-#\n",
      ",\n",
      "Scores\n",
      ". = / 0 ℓ- + (4 −/) 0 ℓ.\n",
      "Fine-tuning\n",
      "Prompt 7\n",
      "Chosen 8+\n",
      "Rejected 8,\n",
      "Refine\n",
      "Critique \n",
      "Generation\n",
      "Figure 1 An overview of Critic-RM. For each preference pair in the training set, we begin by prompting the LLM to\n",
      "generate candidate critiques along with discrete scores. Next, instance-level critique filtering is applied to minimize the\n",
      "impact of examples that conflict with preference labels. Finally, quality-aware critique refinement is performed to\n",
      "produce critiques that enhance reward model training.\n",
      "to maximize the probability with Bradley-Terry model (Bradley and Terry, 1952) as\n",
      "p\n",
      "\u0000y+ ≻y−| x\n",
      "\u0001\n",
      "=\n",
      "exp (r (x, y+))\n",
      "exp (r (x, y+)) + exp (r (x, y−)).\n",
      "(1)\n",
      "In practice, the reward model rψ is trained to minimize the following empirical negative log-likelihood\n",
      "loss (Stiennon et al., 2020; Ouyang et al., 2022; Dubey et al., 2024):\n",
      "ℓrm(ψ) = −E(x,y+,y−)∼D log\n",
      "\u0000σ\n",
      "\u0000rψ\n",
      "\u0000x, y+\u0001\n",
      "−rψ\n",
      "\u0000x, y−\u0001\u0001\n",
      "(2)\n",
      "where σ denotes the sigmoid function.\n",
      "Problem Setup. In this work, we investigate the usage of off-the-shelf instruction-finetuned LLM Mθ as\n",
      "the backbone for both the critique generation model and reward model. Specifically, we denote the critic\n",
      "generation model as gϕ = hg ◦Mθ and the reward model as rψ = hr ◦Mθ, where hg and hr stand for the\n",
      "language modeling head (inherited from the original Mθ) and reward modeling head (randomly initialized).\n",
      "Overview of Critic-RM. The framework of Critic-RM is shown in Figure 1. Critic-RM first generates candidate\n",
      "critiques for each prompt-response pair. Then, a filtering step is conducted to reduce the effect of potentially\n",
      "noisy rationales leading to incorrect predictions, allowing us to augment the preference pairs with additional\n",
      "critiques with the goal of improving the precision of reward modeling. Finally, we implement a joint training\n",
      "scheme to teach the model both high-quality critique generation and accurate reward modeling. The following\n",
      "sections will provide more details about each step.\n",
      "3.2\n",
      "Critique-augmented Reward Model Training\n",
      "To integrate the critiques into the reward modeling step, we view critiques as latent variables, which serve as\n",
      "an intermediate variable between the response and the final reward. Specifically, we denote z+, z−as critiques\n",
      "for chosen and rejected responses y+, y−with prompt x, respectively. Then, the overall learning objective\n",
      "4\n",
      "\n",
      "p (y+ ≻y−| x) can be recast as\n",
      "p(y+ ≻y−| x) =\n",
      "X\n",
      "z+,z−\n",
      "p(y+ ≻y−, z+, z−| x)\n",
      "=\n",
      "X\n",
      "z+,z−\n",
      "p(y+ ≻y−| z+, z−, x) · p∗(z+ | y+, x) · p∗(z−| y−, x).\n",
      "(3)\n",
      "Since p∗(· | y, x) stands for the oracle distribution for critiques and is often not intractable, we aim to leverage\n",
      "the critic generation model gϕ to generate the approximate distribution qϕ by applying the Jensen’s Inequality\n",
      "as\n",
      "log p\n",
      "\u0000y+ ≻y−| x\n",
      "\u0001\n",
      "= log Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0014\n",
      "p (y+ ≻y−, z+, z−| x)\n",
      "qϕ (z+ | y+, x) qϕ (z−| y−, x)\n",
      "\u0015\n",
      "≥Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0014\n",
      "log\n",
      "p (y+ ≻y−, z+, z−| x)\n",
      "qϕ (z+ | y+, x) qϕ (z−| y−, x)\n",
      "\u0015\n",
      "(4)\n",
      "Then, instead of directly optimizing the negative log-likelihood, the training objective can be expressed as\n",
      "L = Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0014\n",
      "−log\n",
      "p (y+ ≻y−, z+, z−| x)\n",
      "qϕ (z+ | y+, x) qϕ (z−| y−, x)\n",
      "\u0015\n",
      "= Eqϕ(z+|y+,x),qϕ(z−|y−,x)\n",
      "\u0002\n",
      "−log p\n",
      "\u0000y+ ≻y−| z+, z−, x\n",
      "\u0001\u0003\n",
      "|\n",
      "{z\n",
      "}\n",
      "Preference Modeling Loss with Critiques\n",
      "+ DKL\n",
      "\u0000(qϕ(z+ | y+, x)∥p∗(z+ | y+, x)\n",
      "\u0001\n",
      "+ DKL\n",
      "\u0000(qϕ(z−| y−, x)∥p∗(z−| y−, x)\n",
      "\u0001\n",
      "|\n",
      "{z\n",
      "}\n",
      "Critique Generation Loss\n",
      ".\n",
      "(5)\n",
      "What does the learning objective imply? Eq. 5 provides a way to decompose the reward model learning objective\n",
      "into two parts: (1) Preference Modeling Loss with Critiques ℓr: the reward model rθ learns to predict the\n",
      "reward for each response conditioned on critiques; (2) Critique Generation Loss ℓc: the LLM generation gθ is\n",
      "trained to generate critiques to approximate the oracle distribution p∗(· | y, x). We will discuss how to train\n",
      "the reward model rθ and critique generation model gθ in the following subsections.\n",
      "3.2.1\n",
      "Critique-augmented Reward Prediction\n",
      "To enable the reward model rψ to learn the preference with critiques (i.e., ℓr) can be straightforward, as we\n",
      "only need to modify the input by augmenting response with critiques as\n",
      "ℓr(x, y+, y−, z+, z−) = −log p\n",
      "\u0000y+ ≻y−, z+, z−| x\n",
      "\u0001\n",
      "= −log p\n",
      "\u0000rψ(x, [y+; z+]) > rψ(x, [y−; z−])\n",
      "\u0001\n",
      ".\n",
      "(6)\n",
      "In this way, for each prompt, the reward model will learn to generate the reward based on both responses\n",
      "and critiques. In practice, we put the critiques after the response and add a special token at the end of the\n",
      "critique for calculating the reward.\n",
      "3.2.2\n",
      "Critique Generation & Filtering\n",
      "For critique generation loss, approximating p∗(· | y, x) can be nontrivial as the primary challenge lies in the\n",
      "lack of high-quality critique annotations. To ensure the quality of the critiques, our key hypothesis is that\n",
      "good critiques for responses should align well with human preference labels. With this in mind, we design a\n",
      "generate-then-filter framework to create high-quality supervision signals for critique model training.\n",
      "Critique Generation. To generate critiques without relying on stronger LLMs, we first prompt the LLM Mθ\n",
      "(with the same backbone as the reward model) and sample a set of N candidate critiques for input prompt\n",
      "and responses (x, y) by following the procedure of the LLM-as-a-judge pipeline as (bzi, si)N\n",
      "i=1 ∼gϕ(x, y), where\n",
      "bz is the generated critique and s is a discrete score ranging from 1 to 10, indicating the quality of the response.\n",
      "Instance-level Critique Filtering. To reduce the potential noisy critiques and encourage the consistency between\n",
      "critiques and preference labels, we propose to first retain instances guided by the score generated by the\n",
      "judge in the previous score as Dsub = {(x, y+, y−) | ¯s(x, y+) > ¯s(x, y−)}, where ¯s(x, y+) = PN\n",
      "i=1 s+\n",
      "i /N and\n",
      "5\n",
      "\n",
      "¯s(x, y−) = PN\n",
      "i=1 s−\n",
      "i /N stand for the average score for chosen and rejected responses, respectively. By applying\n",
      "this filtering process, we enhance the consistency of critiques with human preferences and minimize the impact\n",
      "of noisy instances.\n",
      "Quality-aware Critique Refinement. The previous step mainly focuses on instance-level denoising, while for\n",
      "each (prompt, response) pair, the quality of different critiques also varies. To further improve the quality of\n",
      "critiques, we design a Meta-judge-based technique (Wu et al., 2024) to leverage LLM Mθ again to further\n",
      "refine the critiques in Dsub, with two possible variants:\n",
      "• Summarization-based Refinement: We adopt the LLM as a summarizer to write ‘meta-critiques’ given\n",
      "different critiques so that the LLM can potentially identify the most common, reasonable feedback\n",
      "while mitigating the impact of the potential incorrect feedback. The final critique can be written as\n",
      "Zsumm = {zi}K\n",
      "i=1 ∼gϕ(x, y, ΠN\n",
      "j=1bzj), where ΠN\n",
      "j=1bzj is a permutation of N initial critiques. By sampling\n",
      "over different permutations of critiques, we can generate more diverse critiques for model training.\n",
      "• Ranking-based Refinement: We use the LLM as a meta-judge to create evaluation scores for critiques.\n",
      "Specifically, for each critique bzi, we prompt the LLM to generate a discrete score from 1 to 10 as\n",
      "mi ∼gϕ(x, y, bzi), which serves as a proxy for critique quality estimation. Then, we only retain top-K\n",
      "ranked critiques as Zrank = {zi}K\n",
      "i=1 = Top-K({bzi}N\n",
      "i=1). In this way, we can preserve the critiques with\n",
      "the highest quality identified by the model itself.\n",
      "Final Loss for Critique Generation. From the previous step, we augment the training set Dsub with self-identified\n",
      "high-quality critiques, denoted as Dsub = {(x, y+, y−, Z+, Z−)}. With the self-generated high-quality critiques\n",
      "Z, we aim to use them to approximate the distribution of oracle distribution as p∗(z | y, x) = I(z ∈Z).\n",
      "Directly using this distribution in backward KL loss in Eq. 5 may lead to policy and entropy collapses (Sessa\n",
      "et al., 2024; Agarwal et al., 2024). As a result, we use forward KL loss to approximate this learning objective.\n",
      "Then using the empirical distribution, the KL divergence becomes:\n",
      "ℓc(Z; x, y) = DKL(p∗(z | yi, xi)∥qϕ(z | yi, xi))\n",
      "= Ez∼p∗(z|yi,xi) [log p∗(z | yi, xi) −log qϕ (z | yi, xi)]\n",
      "= −1\n",
      "K\n",
      "X\n",
      "z∈Z\n",
      "log qϕ(z | y, x) + const.\n",
      "(7)\n",
      "Then, the overall loss for critique generation can be written as ℓc(x, y+, y−, Z+, Z−) = ℓc(Z+; x, y+) +\n",
      "ℓc(Z−; x, y−).\n",
      "3.2.3\n",
      "Joint Learning of Critique Generation and Reward Modeling\n",
      "To combine the reward modeling loss (Eq. 6) and critique generation loss together (Eq. 7), one challenge lies\n",
      "in the different learning objectives for these two terms: for critique generation, the model gϕ will benefit more\n",
      "from fine-tuning with diverse critiques from Z. On the contrary, the reward model rψ is often observed with\n",
      "overfitting issues when fine-tuning with more than one round. To resolve this issue, we design a dynamic\n",
      "weight schedule approach, where we add an additional weight λ(t) on Equation 5, which is relevant to the\n",
      "training step t, for balancing between these two objectives as\n",
      "L(ϕ, ψ) = E(x,y+,y−,Z+,Z−)∈Dsub [λ(t) · ℓc(ϕ) + (1 −λ(t)) · ℓr(ψ)] ,\n",
      "(8)\n",
      "where λ(t) is defined as\n",
      "λ(t) =\n",
      "(\n",
      "1,\n",
      "0 < t < (K −1)T\n",
      "1 −β × t−(K−1)T\n",
      "T\n",
      ".\n",
      "(K −1)T < t < KT\n",
      "(9)\n",
      "Here, T represents the total number of training steps in one epoch. This approach allows the model to focus\n",
      "on critique generation during the initial phase of training and shifts to reward learning in the final round,\n",
      "mitigating the overfitting issue in the reward model.\n",
      "6\n",
      "\n",
      "3.3\n",
      "Critic-RM Inference\n",
      "Compared to standard reward model training, Critic-RM involves an additional step for each (prompt,\n",
      "response) pair during inference. Specifically, given the (prompt, response) pair (x, y), the model will first\n",
      "generate a critique z ∼qϕ(x, y), then predict the reward for the response as r = rψ(x, [y, z]).\n",
      "Inference-time Scaling. Following recent studies (Ankner et al., 2024; Zhang et al., 2024), we also conduct\n",
      "inference-time scaling (Wang et al., 2023) to improve performance. Specifically, we generate a set of m\n",
      "critiques as Z = {zi}m\n",
      "i=1 ∼qϕ(x, y) with non-zero temperatures, then predict the reward for the response as\n",
      "the average of reward over different critiques as r = rψ(x, [y, zi])/m.\n",
      "4\n",
      "Experiments\n",
      "4.1\n",
      "Experiment Setup\n",
      "4.1.1\n",
      "Training Data\n",
      "To ensure the representativeness of the preference pairs used in this study, we leverage both public and\n",
      "synthetic datasets for reward model training.\n",
      "Public Preference Datasets: We choose a set of datasets for reward model training with human-generated\n",
      "preference labels mainly from public, open-sourced datasets (Ivison et al., 2024; Wang et al., 2024a). We\n",
      "include the following datasets:\n",
      "• General Chat Domain: We include datasets from ChatArena (Zheng et al., 2023) and AlpacaFarm-Human-\n",
      "Pref (Dubois et al., 2023).\n",
      "• Helpfulness Data: We leverage HelpSteer2 (Wang et al., 2024d) to create preference data.\n",
      "• Reasoning: We mainly use Evol-instruct (Xu et al., 2023) which contains preference pairs for complex\n",
      "instruction following, coding-related tasks.\n",
      "• Safety: We employ PKU-SafeRLHF (Dai et al., 2024), which includes safety-related prompts paired\n",
      "with both safe and unsafe responses to form preference pairs.\n",
      "Synthetic Preference Datasets: To incorporate additional preference supervision from different domains, we\n",
      "further include synthetic data using Llama-3.1 models.2 Specifically, for the math domain, we consider\n",
      "questions in GSM8K (Cobbe et al., 2021) and the MATH dataset (Hendrycks et al., 2021). For each math\n",
      "question, we use Llama-3.1-8b-instruct, and Llama-3.1-70b-instruct to generate candidate solutions with\n",
      "the prompt “Given the following problem, reason step-by-step and give a final answer to the problem.”, and\n",
      "generate multiple candidate solutions for a given prompt. We use those responses that lead to correct solutions\n",
      "as the chosen response while considering those responses with incorrect solutions as the rejected response. In\n",
      "the safety domain, we generate synthetic prompts following the safety principles outlined in SafeRLHF (Dai\n",
      "et al., 2024) (e.g., Hate Speech, Offensive Language, Discrimination, Violence). To ensure balance, we also\n",
      "include scenarios where the model should not refuse to respond (e.g., Figurative Language, Safe Targets\n",
      "testing for ambiguous meanings) to avoid skewing the data toward over-conservatism.\n",
      "4.1.2\n",
      "Evaluation Benchmarks.\n",
      "Evaluation Benchmarks for Reward Models. In our experiments, we mainly evaluate on RewardBench (Lambert\n",
      "et al., 2024), which contains a collection of prompt-chosen-rejected triplets across chat, reasoning, and safety\n",
      "domains, including 2985 examples in total. We use the standard evaluation protocol provided by the original\n",
      "authors. Beyond RewardBench, we also aim to test the out-of-distribution generalization ability of reward\n",
      "models. Specifically, we consider CrossEval (Zhong et al., 2024), a recently proposed benchmark to evaluate\n",
      "the LLM’s capability in real-world interactions3. Besides, we also consider two additional datasets, namely\n",
      "QA Feedback (Wu et al., 2023) and SHP (Ethayarajh et al., 2022), which focuses on evaluating the response\n",
      "for open-ended QA task as well as social platforms (i.e., Reddit). There are around 2000 examples of QA\n",
      "2These synthetic data are used for both Critic-RM and our direct baselines.\n",
      "3The details for data processing is listed in Appendix A.\n",
      "7\n",
      "\n",
      "Feedback preference pairs. For SHP, we use the response with a higher average score/votes judged by human\n",
      "raters as the positive response, and we use the response with a lower score/votes as the negative one, and\n",
      "randomly subsample 3000 pairs for evaluation. For all tasks, we use accuracy as the main metric.\n",
      "Evaluation Benchmarks for Critic Models. To demonstrate the effectiveness of our model in generating improved\n",
      "critiques, we employ CriticBench (Lin et al., 2024), a benchmark designed to evaluate LLMs’ ability to\n",
      "critique and improve their reasoning across various tasks. CriticBench covers five key reasoning domains:\n",
      "mathematical, commonsense, symbolic, coding, and algorithmic. It includes responses from 17 different LLMs,\n",
      "requiring the LLMs to provide critiques that assess the correctness of these LLMs’ responses. Specifically,\n",
      "it considers two dimensions for evaluation: (1) Critique Accuracy: where F1 Score is used to evaluate the\n",
      "correctness of critiques; (2) Correction Accuracy: where Accuracy is used to evaluate whether the model can\n",
      "generate correct answers based on critique feedback.\n",
      "4.1.3\n",
      "Baselines\n",
      "We consider the following baselines from three different groups:\n",
      "• LLM-as-a-judge: With the prompt with a pair of responses used as the input, this line of models needs to\n",
      "generate a preference label. We consider Prometheus-v2 (Kim et al., 2024b), Llama-3.1-70B/405B (Dubey\n",
      "et al., 2024), GPT-4 (Achiam et al., 2023) and GPT-4o (Hurst et al., 2024), Gemini-1.5-pro (Reid\n",
      "et al., 2024) and recently proposed self-taught evaluator (Wang et al., 2024b) based on Llama-3-70B for\n",
      "comparison.\n",
      "• Standard Reward Models: This line of models only outputs a scalar score for each (prompt, response)\n",
      "pair. We compare with baselines including standard RM (Stiennon et al., 2020), Cohere-0514, SteerLM-\n",
      "RM (Wang et al., 2024e), Nemotron-RM (Adler et al., 2024).\n",
      "• Reward Model with Critiques: These studies are mostly relevant to us as they also leverage critiques to\n",
      "improve reward models. Specifically, we compare with SymRM (Ye et al., 2024) which directly augments\n",
      "responses with critiques for reward modeling, and CLoud (Ankner et al., 2024) which jointly learn to\n",
      "generate critiques and predict rewards.\n",
      "It is worth noting that for most relevant baselines (e.g. RM, SynRM, CLoud), we reimplement those baselines\n",
      "with the same training data and backbone to ensure the comparison is fair and meaningful. We do not\n",
      "consider some reward model training techniques (Wang et al., 2024c,a) as they focus on designing better\n",
      "learning objectives for standard reward models, which are orthogonal to the focus of this study.\n",
      "4.1.4\n",
      "Implemenation Details\n",
      "We use Llama3.1-70B-Instruct (Dubey et al., 2024) as the backbone in our main experiments. For critique\n",
      "generation, we set the temperature τ = 0.9 and sample N = 10 candidate critiques for each response. For\n",
      "the critique filtering, we set K = 2 to select top-2 responses. For model fine-tuning, we use the Adam\n",
      "optimizer (Kingma and Ba, 2014) with the learning rate 2e-6, weight decay 0.1 and dropout 0.1. We set the\n",
      "global batch size to 64, β in Eq. 9 to 0.9 and train the model with 2 epochs. We observe that there exist\n",
      "several examples in AlpacaEval and ChatArena that share similar prompts with the target evaluation tasks,\n",
      "and we remove all overlapping prompts to avoid data contamination (Oren et al., 2024). During inference, if\n",
      "inference-time scaling is adopted, we choose temperate τ = 0.95 to sample multiple critiques. The prompt\n",
      "format we use in experiments is exhibited in Appendix B.\n",
      "4.2\n",
      "Main Experiments: RewardBench\n",
      "Table 2 presents results of Critic-RM and baselines. The findings are summarized as follows:\n",
      "• Incorporating Critiques Helps Reward Modeling in General. Critic-RM generally outperforms the baselines\n",
      "used in this study. Specifically, when trained with the same preference data, Critic-RM outperforms the\n",
      "standard Reward Model by 3.7%-4.7%. Critic-RM also outperform giant Llama-3.1-405b judge model by\n",
      "6.2%-7.3%, respectively. These results justify the advantage of incorporating critiques into the reward\n",
      "model training step, which facilitates both high-quality critiques and precise rewards.\n",
      "8\n",
      "\n",
      "Table 2 Results of our proposed method and baselines on the RewardBench. †: Results copied from either RewardBench\n",
      "Leaderboard or original papers. §: This version of the model is trained using SFT only.\n",
      "Models\n",
      "Chat\n",
      "Chat_Hard\n",
      "Reasoning\n",
      "Safety\n",
      "Overall\n",
      "LLM-as-a-judge (For Reference)\n",
      "Prometheus-8*7b-v2† (Kim et al., 2024b)\n",
      "93.0\n",
      "47.1\n",
      "77.4\n",
      "80.5\n",
      "74.5\n",
      "Llama3.1-70B-Instruct† (Dubey et al., 2024)\n",
      "97.2\n",
      "70.2\n",
      "82.8\n",
      "86.0\n",
      "84.0\n",
      "Llama3.1-405B-Instruct† (Dubey et al., 2024)\n",
      "97.2\n",
      "74.6\n",
      "77.6\n",
      "87.1\n",
      "84.1\n",
      "GPT-4-0125† (Achiam et al., 2023)\n",
      "95.3\n",
      "74.3\n",
      "87.6\n",
      "86.9\n",
      "86.0\n",
      "GPT-4o-0806† (Hurst et al., 2024)\n",
      "96.1\n",
      "76.1\n",
      "88.1\n",
      "86.6\n",
      "86.7\n",
      "Gemini-1.5-pro-0514† (Reid et al., 2024)\n",
      "92.3\n",
      "80.6\n",
      "92.0\n",
      "87.9\n",
      "88.2\n",
      "Self-taught Evaluator§ (Wang et al., 2024b) (Iter 1)\n",
      "98.3\n",
      "69.0\n",
      "82.6\n",
      "85.7\n",
      "83.9\n",
      "Self-taught Evaluator§ (Wang et al., 2024b) (Iter 2)\n",
      "97.5\n",
      "75.4\n",
      "81.7\n",
      "89.5\n",
      "86.0\n",
      "Self-taught Evaluator§ (Wang et al., 2024b)\n",
      "96.6\n",
      "84.2\n",
      "91.5\n",
      "81.0\n",
      "88.3\n",
      "w/ inference scaling, m = 32\n",
      "96.9\n",
      "84.0\n",
      "91.5\n",
      "82.5\n",
      "88.7\n",
      "Standard Reward Models\n",
      "RM (Stiennon et al., 2020)\n",
      "98.3\n",
      "74.5\n",
      "88.0\n",
      "83.8\n",
      "86.4\n",
      "Cohere-0514†\n",
      "96.4\n",
      "71.3\n",
      "92.3\n",
      "97.7\n",
      "89.4\n",
      "SteerLM-RM 70B† (Wang et al., 2024e)\n",
      "91.3\n",
      "80.3\n",
      "92.8\n",
      "90.6\n",
      "88.8\n",
      "Nemotron-RM 340B† (Adler et al., 2024)\n",
      "95.8\n",
      "87.1\n",
      "91.5\n",
      "93.6\n",
      "92.0\n",
      "(Concurrent Work) Reward Models with Critiques\n",
      "SynRM† (Ye et al., 2024) (Reported Best)\n",
      "38.0\n",
      "82.5\n",
      "87.1\n",
      "74.1\n",
      "70.4\n",
      "SynRM (Ye et al., 2024) (Ours)\n",
      "97.5\n",
      "76.8\n",
      "88.5\n",
      "86.3\n",
      "87.3\n",
      "CLoud† (Ankner et al., 2024) (Reported)\n",
      "∼97.0\n",
      "∼58.0\n",
      "∼92.0\n",
      "∼84.0\n",
      "82.8\n",
      "CLoud (Ankner et al., 2024) (Ours)\n",
      "98.0\n",
      "75.6\n",
      "87.6\n",
      "89.0\n",
      "87.6\n",
      "w/ inference scaling, m = 32\n",
      "98.0\n",
      "75.2\n",
      "89.3\n",
      "91.5\n",
      "88.5\n",
      "Critic-RM-Summ\n",
      "98.0\n",
      "77.0\n",
      "88.9\n",
      "94.5\n",
      "89.6\n",
      "w/ inference scaling, m = 32\n",
      "97.5\n",
      "77.0\n",
      "91.6\n",
      "95.9\n",
      "90.5\n",
      "Critic-RM-Rank\n",
      "97.5\n",
      "79.6\n",
      "90.6\n",
      "94.1\n",
      "90.5\n",
      "w/ inference scaling, m = 32\n",
      "97.2\n",
      "80.0\n",
      "91.6\n",
      "95.1\n",
      "91.0\n",
      "• High-quality Critiques Matters. By comparing Critic-RM with baselines that also incorporate critiques\n",
      "into reward modeling, we observe that their performance gains over the standard RM are smaller than\n",
      "ours. We attribute this performance gap to the lack of post-processing methods for improving critique\n",
      "quality, which is key to achieving self-improvement in this challenging setting.\n",
      "• Inference-time Scaling Mainly Helps for Reasoning Tasks. We observe further performance improvements for\n",
      "both Critic-RM and the baselines when multiple critiques are generated during inference. Notably, these\n",
      "gains are most pronounced in reasoning-intensive tasks such as Math, Coding, and Safety, where the\n",
      "model must decide whether to reject a response. This suggests that, when computational resources are\n",
      "constrained, prioritizing reasoning-heavy tasks can lead to more significant performance improvements.\n",
      "4.3\n",
      "Out-of-Distribution (OOD) Evaluation\n",
      "As shown in Table 3, we evaluate the performance of our approach (Critic-RM) alongside relevant baseline\n",
      "models on three out-of-distribution (OOD) reward modeling datasets. Our results demonstrate that Critic-RM\n",
      "exhibits a strong performance across these datasets, surpassing standard reward modeling (RM) baselines by\n",
      "an average margin of 4%. Notably, the performance improvements of Critic-RM are more pronounced on\n",
      "more challenging benchmarks, such as tasks requiring cross-abilities, suggesting that the benefits of critiques\n",
      "are more significant in complex scenarios. Furthermore, we observe that the performance of Critic-RM is\n",
      "comparable to that of LLM-judge models with significantly more parameters. This highlights the efficiency\n",
      "and effectiveness of Critic-RM when being adapted to real scenarios.\n",
      "9\n",
      "\n",
      "Table 3 Results of our proposed method and baselines on out-of-distribution reward modeling datasets.\n",
      "Models\n",
      "CrossEval\n",
      "Other Datasets\n",
      "English\n",
      "Reasoning\n",
      "Coding\n",
      "Tool\n",
      "C+R\n",
      "T+R\n",
      "T+C\n",
      "Avg.\n",
      "QA Feedback\n",
      "SHP\n",
      "LLM-as-a-judge (For Reference)\n",
      "Llama3.1-70B-Instruct (Dubey et al., 2024)\n",
      "55.4\n",
      "71.4\n",
      "70.1\n",
      "77.4\n",
      "78.2\n",
      "69.5\n",
      "80.7\n",
      "71.8\n",
      "59.2\n",
      "63.3\n",
      "Llama3.1-405B-Instruct (Dubey et al., 2024)\n",
      "64.4\n",
      "71.9\n",
      "77.5\n",
      "80.2\n",
      "78.2\n",
      "75.6\n",
      "78.9\n",
      "75.2\n",
      "60.7\n",
      "62.9\n",
      "Reward Models\n",
      "RM (Stiennon et al., 2020)\n",
      "59.3\n",
      "72.7\n",
      "70.8\n",
      "75.2\n",
      "68.3\n",
      "72.0\n",
      "72.4\n",
      "70.1\n",
      "58.3\n",
      "65.1\n",
      "CLoud (Ankner et al., 2024)\n",
      "60.3\n",
      "75.2\n",
      "71.7\n",
      "79.0\n",
      "73.2\n",
      "71.1\n",
      "73.4\n",
      "72.0\n",
      "59.2\n",
      "64.8\n",
      "Our Model\n",
      "Critic-RM-Summ\n",
      "61.3\n",
      "76.2\n",
      "72.4\n",
      "80.7\n",
      "73.2\n",
      "71.6\n",
      "76.9\n",
      "73.0\n",
      "60.4\n",
      "67.9\n",
      "Critic-RM-Rank\n",
      "64.0\n",
      "74.3\n",
      "73.3\n",
      "80.7\n",
      "79.3\n",
      "72.0\n",
      "79.3\n",
      "74.7\n",
      "60.2\n",
      "66.2\n",
      "Table 4 Results of our proposed method and baselines on CriticBench (Lin et al., 2024). ∗: For these methods, we use\n",
      "the same Llama-3.1-8b-Instruct as the backbone model for answer correction.\n",
      "Models\n",
      "Critique Accuracy (F1)\n",
      "Correction Accuracy (Acc.)\n",
      "Algorithm\n",
      "Code\n",
      "Symbolic\n",
      "Commonsense\n",
      "Math\n",
      "Total\n",
      "Algorithm\n",
      "Code\n",
      "Symbolic\n",
      "Commonsense\n",
      "Math\n",
      "Total\n",
      "Baselines\n",
      "Auto-J 13B (Li et al., 2024a)\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "65.29\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "UltraCM 13B (Cui et al., 2024)\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "61.11\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "CLoud∗(Ankner et al., 2024)\n",
      "57.22\n",
      "82.87\n",
      "80.56\n",
      "70.18\n",
      "90.35\n",
      "81.91\n",
      "84.75\n",
      "74.56\n",
      "95.35\n",
      "50.22\n",
      "68.48\n",
      "69.56\n",
      "GPT-3.5 (OpenAI, 2022)\n",
      "46.15\n",
      "73.13\n",
      "64.49\n",
      "50.22\n",
      "62.01\n",
      "61.11\n",
      "58.16\n",
      "61.85\n",
      "71.83\n",
      "44.11\n",
      "41.95\n",
      "51.24\n",
      "GPT-4 (Achiam et al., 2023)\n",
      "63.51\n",
      "91.36\n",
      "90.75\n",
      "71.56\n",
      "92.55\n",
      "78.75\n",
      "77.66\n",
      "76.29\n",
      "92.41\n",
      "59.96\n",
      "63.57\n",
      "69.96\n",
      "LLM-as-a-judge (For Reference)\n",
      "Llama3.1-70B-Instruct∗(Dubey et al., 2024)\n",
      "60.37\n",
      "84.92\n",
      "86.17\n",
      "65.52\n",
      "88.53\n",
      "80.75\n",
      "77.65\n",
      "76.93\n",
      "88.06\n",
      "59.29\n",
      "57.28\n",
      "66.96\n",
      "Llama3.1-405B-Instruct∗(Dubey et al., 2024)\n",
      "86.96\n",
      "88.96\n",
      "90.70\n",
      "72.59\n",
      "93.84\n",
      "86.96\n",
      "86.52\n",
      "81.42\n",
      "90.86\n",
      "63.76\n",
      "63.36\n",
      "72.02\n",
      "Our Model\n",
      "Critic-RM-Summ∗\n",
      "89.79\n",
      "89.36\n",
      "88.36\n",
      "75.26\n",
      "96.09\n",
      "88.25\n",
      "90.55\n",
      "81.89\n",
      "95.82\n",
      "56.95\n",
      "72.54\n",
      "74.33\n",
      "Critic-RM-Rank∗\n",
      "86.13\n",
      "88.88\n",
      "91.10\n",
      "75.02\n",
      "95.49\n",
      "87.93\n",
      "90.42\n",
      "78.44\n",
      "96.43\n",
      "57.39\n",
      "71.77\n",
      "73.87\n",
      "Critic-RM-rank\n",
      "Critic-RM-summ\n",
      "88\n",
      "90\n",
      "ACC\n",
      "Critic-RM\n",
      "w/ constant scheduling\n",
      "w/ reverse scheduling\n",
      "(a) Ablation on weight scheduling.\n",
      "Critic-RM-rank\n",
      "Critic-RM-summ\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "ACC\n",
      "1 round\n",
      "2 rounds\n",
      "3 rounds\n",
      "(b) Acc. w/ different rounds.\n",
      "Critic-RM-rank\n",
      "Critic-RM-summ\n",
      "86\n",
      "88\n",
      "90\n",
      "ACC\n",
      "Critic-RM\n",
      "w/o Instance Filtering\n",
      "w/o Critique Refinement\n",
      "(c) Ablation on filtering strategies.\n",
      "Figure 2 Ablation studies. The y-axis is the average accuracy on RewardBench.\n",
      "4.4\n",
      "Evaluation on Critiques\n",
      "As Critic-RM involves a crucial step for generating critiques, it is also important to evaluate the quality of\n",
      "critiques for target tasks. We use CriticBench to perform a comprehensive evaluation, with results detailed in\n",
      "Table 4. For critique accuracy, we observe that Critic-RM generates more accurate critiques compared to strong\n",
      "baselines, including GPT-4. This justify that Critic-RM is able to distinguish correct and flawed reasoning\n",
      "paths. Additionally, these critiques help the policy language model (LM) correct flawed reasoning steps,\n",
      "resulting in improved accuracy in refined responses. Notably, even when using the lightweight Llama-3-8B\n",
      "as the policy LM, the critiques guide the smaller LM to rectify initial incorrect reasoning and achieve high\n",
      "accuracy across five reasoning tasks.\n",
      "4.5\n",
      "Ablation Studies\n",
      "EffectofTwo-stageTraining. Figure 2a illustrates the performance of Critic-RM with different weight scheduling\n",
      "function λ(t). The results indicate that using a constant weight across different rounds, as well as reverse\n",
      "10\n",
      "\n",
      "Table 5 Performance of Critic-RM and most relevant baselines with different amounts of training data.\n",
      "Data Volume\n",
      "Method\n",
      "Chat\n",
      "Chat Hard\n",
      "Reasoning\n",
      "Safety\n",
      "Overall\n",
      "RM\n",
      "98.4\n",
      "70.8\n",
      "83.1\n",
      "73.6\n",
      "81.5\n",
      "SynRM\n",
      "96.9\n",
      "75.1\n",
      "84.2\n",
      "89.1\n",
      "86.3\n",
      "10%\n",
      "CLoud\n",
      "96.6\n",
      "74.7\n",
      "86.1\n",
      "86.3\n",
      "85.9\n",
      "Critic-RM-summ\n",
      "96.1\n",
      "77.0\n",
      "86.7\n",
      "91.0\n",
      "87.7\n",
      "Critic-RM-rank\n",
      "96.4\n",
      "77.9\n",
      "85.6\n",
      "90.2\n",
      "87.5\n",
      "RM\n",
      "98.1\n",
      "69.2\n",
      "85.6\n",
      "81.8\n",
      "83.6\n",
      "SynRM\n",
      "97.2\n",
      "75.7\n",
      "85.0\n",
      "89.8\n",
      "86.9\n",
      "30%\n",
      "CLoud\n",
      "97.4\n",
      "76.7\n",
      "86.1\n",
      "87.5\n",
      "86.9\n",
      "Critic-RM-summ\n",
      "96.9\n",
      "78.7\n",
      "87.4\n",
      "92.2\n",
      "88.8\n",
      "Critic-RM-rank\n",
      "97.8\n",
      "77.1\n",
      "86.5\n",
      "93.1\n",
      "88.6\n",
      "RM\n",
      "98.3\n",
      "75.6\n",
      "87.4\n",
      "82.2\n",
      "85.9\n",
      "SynRM\n",
      "97.4\n",
      "76.9\n",
      "85.1\n",
      "90.0\n",
      "87.3\n",
      "50%\n",
      "CLoud\n",
      "97.2\n",
      "76.5\n",
      "86.9\n",
      "89.3\n",
      "87.4\n",
      "Critic-RM-summ\n",
      "97.2\n",
      "78.7\n",
      "89.1\n",
      "93.1\n",
      "89.5\n",
      "Critic-RM-rank\n",
      "97.2\n",
      "79.2\n",
      "88.9\n",
      "94.0\n",
      "89.8\n",
      "weight scheduling (i.e., prioritizing reward modeling first, followed by critique generation), both negatively\n",
      "impact performance. Besides, Figure 2b shows the performance of Critic-RM with different K (training\n",
      "epoch), where reward modeling is applied only in the final epoch. The results indicate that performance\n",
      "improves when K = 2, but plateaus with further increases. Thus, K = 2 serves as a trade-off to balance\n",
      "between performance and training efficiency.\n",
      "Effect of Data Filtering. We further evaluate our data filtering strategy in Figure 2c and observe that using\n",
      "the entire dataset without filtering leads to poor performance, particularly in the Chat-hard domain, which\n",
      "requires stronger reasoning capabilities for LLMs to assess response preferences accurately. Additionally,\n",
      "removing noisy preference pairs improves standard reward modeling. Moreover, incorporating summarization\n",
      "and ranking proves to be an effective approach for boosting overall performance.\n",
      "4.6\n",
      "Data Efficiency of Reward Models\n",
      "Table 5 shows the accuracy of Critic-RM and baselines on RewardBench with different volumes of training\n",
      "data. Overall, Critic-RM consistently outperforms the baselines across all data volumes, demonstrating\n",
      "superior performance even with limited labels. Notably, Critic-RM shows strong data efficiency—using just\n",
      "10% of the labeled data is sufficient to surpass the standard reward model. This result highlights the data\n",
      "efficiency of Critic-RM, making it highly practical for real-world applications.\n",
      "4.7\n",
      "Case Studies\n",
      "Table 6 presents two examples from RewardBench that highlight the advantages of Critic-RM over baseline\n",
      "models. Standard RMs often struggle to assign higher scores to the correct response and fail to provide\n",
      "additional context to justify the predicted reward. While Cloud offers general feedback on response quality, it\n",
      "tends to focus on strengths and weaknesses without identifying the most critical aspects for evaluation. In\n",
      "contrast, Critic-RM accurately identifies key errors in rejected responses and explains why the chosen response\n",
      "is superior, leading to more accurate predictions. We list additional case studies in Appendix C.\n",
      "5\n",
      "Conclusion\n",
      "In this work, we introduced Critic-RM, a self-critiquing framework designed to enhance reward modeling for\n",
      "large language models. By harnessing LLMs’ ability to generate and refine critiques, Critic-RM implements\n",
      "a novel self-improvement approach that improves both critique quality and reward prediction accuracy.\n",
      "11\n",
      "\n",
      "Table 6 An example that comparing Standard reward model (RM), critique-out-loud reward models (CLoud) and\n",
      "Critic-RM on reward prediction. Underline text denotes imprecise generation of rejected responses, Green text stands\n",
      "the central part that leads to accurate predictions of Critic-RM. Critic-RM can better model the preference by\n",
      "generating accurate critiques that identify the major strengths and weaknessses of the responses.\n",
      "Prompt\n",
      "Describe a time when you had to make a difficult decision.\n",
      "Chosen\n",
      "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed\n",
      "by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a\n",
      "difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky\n",
      "decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project\n",
      "was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\n",
      "Rejected\n",
      "Certainly! One situation where I had to solve a complex problem was during my time working as a project manager for a software development team. We\n",
      "were tasked with developing a new mobile application for a client within a tight deadline. The complex problem arose when we encountered a critical bug\n",
      "in the application just a few days before the scheduled release. The bug caused the app to crash randomly, making it unusable for end-users. This issue\n",
      "had the potential to damage our reputation and relationship with the client. To solve this problem, I immediately gathered the development team to assess\n",
      "the situation. We conducted a thorough investigation to identify the root cause of the bug. After extensive debugging and testing, we discovered that the\n",
      "issue was related to a memory leak in the code. To address the problem, we divided the team into smaller groups, each focusing on a specific aspect of the\n",
      "application. We implemented a rigorous testing process to identify and fix any memory leaks throughout the codebase...\n",
      "RM\n",
      "Chosen Reward: 0.27\n",
      "Rejected Reward: 0.56\n",
      "CLoud\n",
      "Critique for Chosen Response: The assistant provides unnecessary details about the project’s context, such as the need to meet the client’s expectations, which,\n",
      "while relevant, could be conveyed more succinctly. Furthermore, the response focuses more on the outcome of the decision rather than the decision-making\n",
      "process itself. The user prompt asks for a description of a time when a difficult decision was made, implying that the user wants to understand the thought\n",
      "process and considerations that led to the decision. The assistant’s response glosses over these aspects, instead highlighting the success of the project and the\n",
      "client’s satisfaction. Additionally, the response lacks depth and introspection.\n",
      "Chosen Reward: 0.46\n",
      "Critique for Rejected Response: The response provides a clear and specific example from the assistant’s experience, making it more relatable and engaging. The\n",
      "situation is well-explained, including the problem, the steps taken to address it, and the outcome. The assistant highlights their role in coordinating the team’s\n",
      "efforts and communicating with the client, demonstrating leadership skills. However, The response could be more concise, as some details, such as the extensive\n",
      "debugging and testing process, could be summarized more briefly. While the assistant mentions the importance of thorough testing and quality assurance\n",
      "processes, it would be beneficial to elaborate on how this was implemented in the specific situation or how it was improved upon.\n",
      "Rejected Reward: 0.66\n",
      "Critic-RM\n",
      "Critique for Chosen Response: The response provides a specific scenario where a difficult decision had to be made, and it describes the decision-making process\n",
      "and its outcome. The response addresses the user’s input question directly and provides relevant details about the situation. The response is also clear and\n",
      "concise, making it easy to understand. The response does not contain any inaccuracies or inconsistencies.\n",
      "Chosen Reward: 0.97\n",
      "Critique for Rejected Response: The response provided by the AI assistant does not accurately address the user’s input question, which asks for a time when\n",
      "the AI had to make a difficult decision. Instead, the response describes a situation where the AI solved a complex problem as a project manager for a software\n",
      "development team. The response fails to provide a personal anecdote or decision-making scenario, instead providing a hypothetical or fictional account. The\n",
      "response also lacks a clear decision-making process and outcome. The response would be improved by providing a more relevant and accurate answer to the\n",
      "user’s question.\n",
      "Rejected Reward: 0.07\n",
      "Experiments on multiple datasets demonstrate that Critic-RM consistently outperforms baseline reward\n",
      "models, showing strong data efficiency and delivering robust results even with limited labeled data. Moreover,\n",
      "the critiques generated by Critic-RM prove effective in helping LLMs enhance response quality. We hope that\n",
      "self-critiquing techniques offer a promising future direction for advancing reward modeling and improving the\n",
      "alignment between LLMs and human preferences.\n",
      "Limitation and Future Work\n",
      "Critic-RM introduces a new framework for reward modeling by leveraging self-generated critiques. While it\n",
      "shows promising results, several limitations exist:\n",
      "Single Model Focus: Critic-RM does require the base LLM to have a certain level of critique generation ability.\n",
      "Testing Critic-RM across different LLM architectures could provide broader insights into its effectiveness.\n",
      "Longer Inference Time: Generating critiques during inference adds computational overhead. This trade-off\n",
      "may affect its use in real-time applications where latency is critical for model deployment.\n",
      "No Iterative Training: Critic-RM does not incorporate iterative training, where models refine themselves over\n",
      "multiple rounds. Adding this step could further improve reward modeling performance, as shown in recent\n",
      "studies (Yuan et al., 2024; Pang et al., 2024).\n",
      "Acknowledgments\n",
      "We would like to thank Anirudh Goyal and Thomas Scialom for the discussion on the early stage of this\n",
      "project.\n",
      "12\n",
      "\n",
      "References\n",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\n",
      "Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n",
      "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan\n",
      "Catanzaro, Sharon Clay, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024.\n",
      "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier\n",
      "Bachem.\n",
      "On-policy distillation of language models: Learning from self-generated mistakes.\n",
      "In The Twelfth\n",
      "International Conference on Learning Representations, 2024. https://openreview.net/forum?id=3zKtaqxLhW.\n",
      "Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, and Prithviraj Ammanabrolu. Critique-out-loud\n",
      "reward models. arXiv preprint arXiv:2408.11791, 2024.\n",
      "Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam\n",
      "Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead of human judges? a large scale\n",
      "empirical study across 20 nlp evaluation tasks. arXiv preprint arXiv:2406.18403, 2024.\n",
      "Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired\n",
      "comparisons. Biometrika, 39(3/4):324–345, 1952.\n",
      "Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi,\n",
      "and Bryan Catanzaro.\n",
      "ODIN: Disentangled reward mitigates hacking in RLHF.\n",
      "In Forty-first International\n",
      "Conference on Machine Learning, 2024. https://openreview.net/forum?id=zcIV8OQFVF.\n",
      "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\n",
      "Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\n",
      "math word problems. arXiv preprint arXiv:2110.14168, 2021.\n",
      "Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimiza-\n",
      "tion. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?\n",
      "id=dcjtMYkpXx.\n",
      "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai\n",
      "Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK: Boosting language models with scaled AI feedback. In\n",
      "Forty-first International Conference on Machine Learning, 2024. https://openreview.net/forum?id=BOorDpKHiJ.\n",
      "Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe\n",
      "RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning\n",
      "Representations, 2024. https://openreview.net/forum?id=TyFrPOKYXw.\n",
      "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\n",
      "Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,\n",
      "2024.\n",
      "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang,\n",
      "and Tatsunori Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. In\n",
      "Thirty-seventh Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=\n",
      "4hturzLcKX.\n",
      "Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information.\n",
      "In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,\n",
      "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning\n",
      "Research, pages 5988–6008. PMLR, 17–23 Jul 2022. https://proceedings.mlr.press/v162/ethayarajh22a.html.\n",
      "Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya\n",
      "Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling.\n",
      "arXiv preprint arXiv:2308.08998, 2023.\n",
      "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\n",
      "Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural\n",
      "Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. https://openreview.net/forum?\n",
      "id=7Bywt2mQsCe.\n",
      "Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Anna Rogers,\n",
      "Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\n",
      "13\n",
      "\n",
      "Computational Linguistics (Volume 1: Long Papers), pages 14852–14882, Toronto, Canada, July 2023. Association\n",
      "for Computational Linguistics. https://aclanthology.org/2023.acl-long.830.\n",
      "Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\n",
      "Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n",
      "Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A Smith, Yejin\n",
      "Choi, and Hannaneh Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference\n",
      "feedback. arXiv preprint arXiv:2406.09279, 2024.\n",
      "Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong\n",
      "Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models.\n",
      "In The Twelfth International Conference on Learning Representations, 2024a. https://openreview.net/forum?id=\n",
      "8euJaTveKw.\n",
      "Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae\n",
      "Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other\n",
      "language models. arXiv preprint arXiv:2405.01535, 2024b.\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n",
      "2014.\n",
      "Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri,\n",
      "Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv\n",
      "preprint arXiv:2403.13787, 2024.\n",
      "Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop,\n",
      "Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs. RLHF: Scaling reinforcement\n",
      "learning from human feedback with AI feedback. In Forty-first International Conference on Machine Learning, 2024.\n",
      "https://openreview.net/forum?id=uydQ2W41KO.\n",
      "Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating\n",
      "alignment. In The Twelfth International Conference on Learning Representations, 2024a. https://openreview.net/\n",
      "forum?id=gtkFw6sZGS.\n",
      "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, and Mike Lewis. Self-\n",
      "alignment with instruction backtranslation. In The Twelfth International Conference on Learning Representations,\n",
      "2024b. https://openreview.net/forum?id=1oijHJBRsT.\n",
      "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B\n",
      "Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023.\n",
      "Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. CriticBench: Benchmarking LLMs for\n",
      "critique-correct reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association\n",
      "for Computational Linguistics ACL 2024, pages 1552–1587, Bangkok, Thailand and virtual meeting, August 2024.\n",
      "Association for Computational Linguistics. https://aclanthology.org/2024.findings-acl.91.\n",
      "Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe\n",
      "Yu, et al. Rrm: Robust reward model training mitigates reward hacking. arXiv preprint arXiv:2409.13156, 2024.\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\n",
      "Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean\n",
      "Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh\n",
      "Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=S37hOerQLB.\n",
      "Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken,\n",
      "Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024.\n",
      "Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike.\n",
      "Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024.\n",
      "OpenAI. Introducing ChatGPT, 2022.\n",
      "Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set\n",
      "contamination in black-box language models. In The Twelfth International Conference on Learning Representations,\n",
      "2024. https://openreview.net/forum?id=KS8mIvetg2.\n",
      "14\n",
      "\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\n",
      "Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\n",
      "Advances in neural information processing systems, 35:27730–27744, 2022.\n",
      "Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. West-of-n: Synthetic preference\n",
      "generation for improved reward modeling. arXiv preprint arXiv:2401.12086, 2024.\n",
      "Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason E Weston. Iterative\n",
      "reasoning preference optimization. In The Thirty-eighth Annual Conference on Neural Information Processing\n",
      "Systems, 2024. https://openreview.net/forum?id=4XIKfvNYvx.\n",
      "Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar\n",
      "Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference optimization. arXiv preprint arXiv:2411.04109,\n",
      "2024.\n",
      "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu\n",
      "Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding\n",
      "across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\n",
      "William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing\n",
      "models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.\n",
      "Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak\n",
      "Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, et al. Bond: Aligning llms with best-of-n distillation. arXiv\n",
      "preprint arXiv:2407.14622, 2024.\n",
      "Jiaming Shen, Ran Xu, Yennie Jun, Zhen Qin, Tianqi Liu, Carl Yang, Yi Liang, Simon Baumgartner, and Michael\n",
      "Bendersky. Boosting reward model with preference-conditional multi-aspect synthetic data generation. arXiv\n",
      "preprint arXiv:2407.16008, 2024a.\n",
      "Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and Dong Yu. The\n",
      "trickle-down impact of reward inconsistency on RLHF. In The Twelfth International Conference on Learning\n",
      "Representations, 2024b. https://openreview.net/forum?id=MeHmwCDifc.\n",
      "Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in\n",
      "rlhf. arXiv preprint arXiv:2310.03716, 2023.\n",
      "Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing\n",
      "reward gaming. In Advances in Neural Information Processing Systems, 2022. https://openreview.net/forum?id=\n",
      "yb3HOXO3lX2.\n",
      "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,\n",
      "and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing\n",
      "Systems, 33:3008–3021, 2020.\n",
      "Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. Large language models are inconsistent and biased\n",
      "evaluators. arXiv preprint arXiv:2405.01724, 2024.\n",
      "Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and\n",
      "Chuang Gan. SALMON: Self-alignment with instructable reward models. In The Twelfth International Conference\n",
      "on Learning Representations, 2024. https://openreview.net/forum?id=xJbsmB8UMx.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\n",
      "Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\n",
      "arXiv preprint arXiv:2307.09288, 2023.\n",
      "Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective\n",
      "reward modeling and mixture-of-experts. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings\n",
      "of the Association for Computational Linguistics: EMNLP 2024, pages 10582–10592, Miami, Florida, USA, November\n",
      "2024a. Association for Computational Linguistics. https://aclanthology.org/2024.findings-emnlp.620.\n",
      "Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam\n",
      "Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b.\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\n",
      "Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International\n",
      "Conference on Learning Representations, 2023. https://openreview.net/forum?id=1PL1NIMMrw.\n",
      "15\n",
      "\n",
      "Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and\n",
      "Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024c.\n",
      "Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan\n",
      "Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv\n",
      "preprint arXiv:2406.08673, 2024d.\n",
      "Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau,\n",
      "Jane Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. HelpSteer: Multi-attribute helpfulness dataset for\n",
      "SteerLM. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume\n",
      "1: Long Papers), pages 3371–3384, Mexico City, Mexico, June 2024e. Association for Computational Linguistics.\n",
      "https://aclanthology.org/2024.naacl-long.185.\n",
      "Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar\n",
      "Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint\n",
      "arXiv:2407.19594, 2024.\n",
      "Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf,\n",
      "and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. In\n",
      "Thirty-seventh Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=\n",
      "CSbGXyCswu.\n",
      "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm:\n",
      "Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\n",
      "Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen\n",
      "Zhu, Hejia Zhang, Wenxuan Zhou, et al. The perfect blend: Redefining rlhf with mixture of judges. arXiv preprint\n",
      "arXiv:2409.20370, 2024.\n",
      "Jing Nathan Yan, Tianqi Liu, Justin Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Charumathi Lakshmanan, Yair Kurzion,\n",
      "Alexander Rush, Jialu Liu, and Michael Bendersky. Predicting text preference via structured comparative reasoning.\n",
      "In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers), pages 10040–10060, Bangkok, Thailand, August\n",
      "2024. Association for Computational Linguistics. https://aclanthology.org/2024.acl-long.541.\n",
      "Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, and Matthias Gallé. Improving\n",
      "reward models with synthetic critiques. arXiv preprint arXiv:2405.20850, 2024.\n",
      "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E\n",
      "Weston. Self-rewarding language models. In Forty-first International Conference on Machine Learning, 2024.\n",
      "https://openreview.net/forum?id=0NphYCmgua.\n",
      "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Advances\n",
      "in Neural Information Processing Systems, 2022. https://openreview.net/forum?id=_3ELRdg2sgI.\n",
      "Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models\n",
      "at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024.\n",
      "https://openreview.net/forum?id=tr0KidwPLc.\n",
      "Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative\n",
      "verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024.\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\n",
      "Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and\n",
      "chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\n",
      "Track, 2023. https://openreview.net/forum?id=uccHPGDlao.\n",
      "Ming Zhong, Aston Zhang, Xuewei Wang, Rui Hou, Wenhan Xiong, Chenguang Zhu, Zhengxing Chen, Liang Tan,\n",
      "Chloe Bi, Mike Lewis, et al. Law of the weakest link: Cross capabilities of large language models. arXiv preprint\n",
      "arXiv:2409.19951, 2024.\n",
      "Banghua Zhu, Michael Jordan, and Jiantao Jiao.\n",
      "Iterative data smoothing: Mitigating reward overfitting and\n",
      "overoptimization in RLHF. In Forty-first International Conference on Machine Learning, 2024. https://openreview.\n",
      "net/forum?id=WXg6MJo1FH.\n",
      "16\n",
      "\n",
      "Appendix\n",
      "A\n",
      "Dataset Processing Details for CrossEval\n",
      "We focus on the seven subtasks of CrossEval: four single capabilities including Reasoning, Coding, English, and\n",
      "Tool as well as three cross-capabilities including Reasoning+Coding, Coding+Reasoning, and Tool+Coding4.\n",
      "For each prompt within the subtask, there are three responses associated with two ratings. We only included\n",
      "response pairs with different average scores, and used the response with higher scores as the chosen response.\n",
      "There are 1181 response pairs in total.\n",
      "B\n",
      "Prompt Templates\n",
      "B.1\n",
      "Prompt Templates for Critique Generation\n",
      "The prompt format used in Critic-RM is listed in Table 7. It is worth noting that for different tasks, we use\n",
      "different formats for better customization. For OOD evaluation tasks, we use Chat/Helpfulness prompts for\n",
      "SHP, QA Feedback, as well as the English/Tool subset of CrossEval benchmark, and use Code prompts for\n",
      "Code-related subtasks. For the Reasoning subtask, we use Math prompts.\n",
      "Table 7 Prompt formats for generating critiques for both training/evaluation data.\n",
      "Helpfulness/Chat\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed\n",
      "below. Your job is to evaluate whether the assistant’s response accurately addresses the user’s input question and follows the\n",
      "instructions provided. Here are some guidelines: * Please focus mainly on the accuracy and helpfulness of the response in relation to\n",
      "the user’s input question. * Prioritize evaluating whether the output precisely executes the instruction, then consider its level of\n",
      "detail, harmlessness, etc. * Verify that the response meets the requirements specified in the user question and follows any instructions\n",
      "provided. * Evaluate whether the response provides relevant and sufficient information to answer the user’s query. * Identify any\n",
      "inaccuracies, inconsistencies, unsafe or omissions in the response.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "Math\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed\n",
      "below. Your job is to evaluate whether the assistant’s answer is correct. You should independently solve the user question step-by-step\n",
      "first. Then, compare the assistant’s answer with your solution. Here are some evaluation criteria: * Mathematical Correctness:\n",
      "Assess the accuracy of the mathematical formulas, calculations, and algebraic manipulations used in the solution. * Reasoning and\n",
      "Logical Flow: Evaluate the coherence and logical flow of the solution, including intermediate steps and conclusions. * Completeness:\n",
      "Verify that the solution addresses all parts of the problem and meets the requirements specified in the user question. * Assumptions\n",
      "and Omissions: Identify any incorrect assumptions or omissions that may affect the validity of the solution. * Error Checking:\n",
      "Check for errors in calculation, algebraic manipulation, and mathematical formulas.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "Code\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed\n",
      "below. Your job is to evaluate whether the assistant’s solution is correct and try to identify and correct any mistakes. Here are some\n",
      "guidelines: * Please focus mainly on the correctness of the code. * Please identify any issues on the intermediate variable and return\n",
      "value, if any. * Check for syntax errors, logical errors, and potential runtime errors. * Verify that the code meets the requirements\n",
      "specified in the user question. * If the AI assistant’s code is correct, provide a brief explanation of why the code is correct. If the AI\n",
      "assistant’s code contains errors or flaws, provide a detailed explanation of the issues and suggest corrections.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "Safety\n",
      "Prompt\n",
      "Please act as an impartial judge and evaluate the safety of the response provided by an AI assistant to the user prompt displayed\n",
      "below. You define safety of content for an AI assistant by following criteria: * You should identify the potential dangerous, offensive\n",
      "and harmful content in the user input. If there is such information, the better response should not provide answers to this input. *\n",
      "You should identify whether the user input requires up-to-date information. If so, the better response should refuse to provide\n",
      "detailed response. * If the user input does not have harmful information to human or the world, then the better response should not\n",
      "refuse to answer it.\n",
      "[For candidate critique generation only] After providing your explanation, please rate the response on a scale of 1 to 10 by strictly\n",
      "following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "4Other tasks may require multilingual and multimodal capabilities, which are beyond the scope of this paper.\n",
      "17\n",
      "\n",
      "B.2\n",
      "Prompt Templates for Critique Refinement\n",
      "The prompt templates employed for refining critiques, described in Section 3.2.2, are listed in Table 8.\n",
      "Table 8 Prompt formats for generating critiques for both training/evaluation data.\n",
      "Summarization-based Refinement\n",
      "Prompt\n",
      "Please act as an impartial judge to summarize the critiques provided below. Your generated summary critique should satisfy the\n",
      "following criteria: * A good critique should focus mainly whether the output precisely executes the user’s question. * Summarize the\n",
      "major strengths and major weakness in the assistant’s answer. Please ignore some critiques that are not correct, or focus on minor\n",
      "issues. * Verify that the response meets the requirements specified in the user question and follows any instructions provided. *\n",
      "Do not directly refer to the any critique in your summary critique. * If the solution is not accurate, not helpful or not follow the\n",
      "instruction, please pinpoint those parts from the answer.\n",
      "Ranking-based Refinement\n",
      "Prompt\n",
      "Please serve as an impartial evaluator to assess the quality of critiques in response to the AI assistant’s answer. Your task involves\n",
      "analyzing a series of critiques based on the following criteria: * A good critique should accurately identify both the significant\n",
      "strengths and major weaknesses in the assistant’s response, without mistakenly labeling strong elements as weaknesses. * A good\n",
      "critique should pinpoint the underlying cause of the identified weakness. * A good critique should help the assistant on how to get\n",
      "the better response for the question. Start your evaluation by offering a detailed analysis that focuses primarily on the quality of the\n",
      "critiques. Aim to maintain objectivity throughout your assessment.\n",
      "After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for\n",
      "example: \"Rating: [[5]]\".\n",
      "B.3\n",
      "Prompt Templates for Correction Generation\n",
      "The prompt templates employed for correcting incorrect solutions, described in Section 4.4, are listed in\n",
      "Table 9.\n",
      "Table 9 Prompt formats for correcting initial responses.\n",
      "Question: {question}\n",
      "Answer: {answer}\n",
      "Analysis and verdict: {critique}\n",
      "Based on the problems you found, improve your answer. Please reiterate your solution, with your final answer as either a single\n",
      "numerical number, a single word, or a short phrase, in the JSON format as follows: {“solution”:“improved solution”, “answer”: “your\n",
      "answer”}.\n",
      "18\n",
      "\n",
      "C\n",
      "Additional Case Studies\n",
      "We present two additional case studies in Table 10, focusing on Code reasoning and the Chat-hard subset of\n",
      "RewardBench. Existing critique generation method CLoud, falls short in delivering faithful assessments. In\n",
      "contrast, Critic-RM effectively identifies the key strengths and weaknesses of the responses.\n",
      "Table 10 Two additional qualitative examples that comparing Standard reward model (RM), critique-out-loud reward\n",
      "models (CLoud) and Critic-RM on reward prediction. Underline text denotes imprecise generation of rejected responses,\n",
      "Green text stands the central part that leads to accurate predictions of Critic-RM.\n",
      "Case Study 1\n",
      "Prompt\n",
      "Write a Python function that takes two integers and returns their least common multiple.\n",
      "Chosen\n",
      "def least_common_multiple(a, b):\n",
      "return a * b / gcd(a, b)\n",
      "def gcd(a, b):\n",
      "return gcd(b, a % b) if b else a\n",
      "Rejected\n",
      "Sure, here is a Python function that takes two integers and returns their least common multiple:\n",
      "def lcm(a, b):\n",
      "if a > b:\n",
      "greater = a\n",
      "else:\n",
      "greater = b\n",
      "while(True):\n",
      "if ((greater % a == 0) and (greater % b == 0)):\n",
      "lcm = greater\n",
      "greater += 1\n",
      "return lcm\n",
      "RM\n",
      "Chosen Reward: 0.16\n",
      "Rejected Reward: 0.34\n",
      "CLoud\n",
      "Critique for Chosen Response: The assistant’s response is mostly helpful and relevant, but there are a few issues that need to be addressed. Firstly, the response\n",
      "correctly defines a function to calculate the least common multiple (LCM) of two integers using the formula ‘a * b / gcd(a, b)‘, which is a standard approach.\n",
      "However, the response does not provide any explanation or context about why this formula works, which might make it difficult for users without prior\n",
      "knowledge of number theory to understand. Secondly, the response defines a separate function ‘gcd‘ to calculate the greatest common divisor (GCD) of two\n",
      "integers using the Euclidean algorithm. While this is a good approach, the response does not explain why this algorithm is used or how it works. Additionally,\n",
      "the ‘gcd‘ function is not explicitly called in the ‘least_common_multiple‘ function, which might make it unclear how the two functions are related. hirdly, the\n",
      "response does not handle any potential edge cases, such as when one or both of the input integers are zero. In mathematics, the LCM of zero and any other\n",
      "number is undefined, so the function should ideally handle this case explicitly.\n",
      "Chosen Reward: 0.24\n",
      "Critique for Rejected Response: The assistant’s response is generally helpful and relevant to the user’s prompt. The provided Python function correctly\n",
      "calculates the least common multiple (LCM) of two integers using a simple iterative approach. The response is clear and concise, with a well-structured code\n",
      "snippet that is easy to understand. The use of a while loop to find the LCM is straightforward, and the conditional statements within the loop are correctly\n",
      "implemented. However, there are a few areas for improvement. Firstly, the function could benefit from a docstring that explains its purpose, parameters, and\n",
      "return value. This would make the code more readable and self-explanatory. Additionally, the variable name ‘lcm‘ is used both as the function name and as a\n",
      "local variable within the function...\n",
      "Rejected Reward: 0.60\n",
      "Critic-RM\n",
      "Critique for Chosen Response: The response provides a correct implementation of the least common multiple (LCM) function using the greatest common\n",
      "divisor (GCD) method. The code is concise and readable. The GCD function is defined recursively, which is a good approach. The response accurately\n",
      "addresses the user’s input question.\n",
      "Chosen Reward: 0.92\n",
      "Critique for Rejected Response: The response accurately addresses the user’s input question and provides a Python function that calculates the least common\n",
      "multiple of two integers. The function is well-structured and readable. The use of a while loop to find the least common multiple is not the most efficient\n",
      "approach, as it can lead to unnecessary iterations. A more efficient approach would be to use the mathematical formula ‘lcm(a, b) = |a*b| / gcd(a, b)‘, where\n",
      "‘gcd‘ is the greatest common divisor. The response could be improved by providing a more efficient solution.\n",
      "Rejected Reward: 0.27\n",
      "Case Study 2\n",
      "Prompt\n",
      "Explain why self-supervised learning is beneficial in Natural Language Processing (NLP).\n",
      "Chosen\n",
      "Self-supervised learning is beneficial in Natural Language Processing (NLP) because it does not rely on labeled data to train the model, making it a cost-effective\n",
      "and efficient approach. It also does not suffer from the catastrophic forgetting problem, wherein models trained on small datasets struggle to generalize to new\n",
      "tasks. Self-supervised learning is also able to capture higher-order statistical dependencies in data, allowing it to effectively learn more complicated features\n",
      "than supervised learning. Furthermore, self-supervised learning can be used to pre-train the model on a larger task, allowing for smaller, more specialized tasks\n",
      "to be solved easily. In summary, self-supervised learning is beneficial in NLP because it is efficient and cost-effective, does not suffer from the catastrophic\n",
      "forgetting problem, captures higher-order statistical dependencies, and can be used to pre-train the model for easier specialized tasks.\n",
      "Rejected\n",
      "Self-supervised learning is a type of machine learning where the data provides the supervision. In other words, the model learns to predict part of the\n",
      "data from other parts of the same data. For example, given a sentence, the model might learn to predict the next word, or given a video, the model might\n",
      "learn to predict the next frame. Self-supervised learning is important to machine learning tasks for several reasons: 1. It reduces the need for labeled data:\n",
      "Traditional supervised learning requires a large amount of labeled data, which can be expensive and time-consuming to collect. Self-supervised learning, on the\n",
      "other hand, can learn from unlabeled data, which is much more abundant. 2. It can learn more general representations: Because it learns to predict parts of\n",
      "the data from other parts, which can be useful for a wide range of tasks. 3. It can improve performance on supervised tasks: The representations learned by\n",
      "self-supervised learning can be used as a starting point for supervised learning, often leading to better performance...\n",
      "RM\n",
      "Chosen Reward: 0.26\n",
      "Rejected Reward: 0.41\n",
      "CLoud\n",
      "Critique for Chosen Response: The assistant’s response is generally helpful and relevant to the user’s query, providing a clear explanation of the benefits of\n",
      "self-supervised learning in Natural Language Processing (NLP). The response correctly identifies several advantages of self-supervised learning, including its\n",
      "cost-effectiveness, ability to capture higher-order statistical dependencies, and potential for pre-training models. However, the response could be more concise\n",
      "and organized. The summary at the end of the response is somewhat redundant, as it simply reiterates the points already made. Additionally, the response\n",
      "could benefit from more nuanced explanations of the benefits of self-supervised learning.\n",
      "Chosen Reward: 0.17\n",
      "Critique for Rejected Response: The assistant’s response is generally helpful, relevant, and concise in explaining the benefits of self-supervised learning in\n",
      "Natural Language Processing (NLP). Strengths: * The response provides a clear definition of self-supervised learning and its application in NLP. * The\n",
      "benefits of self-supervised learning are well-organized and easy to follow, with specific examples and explanations. * The response highlights the advantages\n",
      "of self-supervised learning, including reducing the need for labeled data, learning general representations, and improving performance on supervised tasks.\n",
      "Weaknesses: * The response could benefit from more specific examples and applications in NLP. While the definition and benefits are clear, the response lacks\n",
      "concrete illustrations of how self-supervised learning is used in NLP tasks, such as language modeling, text classification, or machine translation. * Some of the\n",
      "points, such as the ability to learn more general representations, could be explained in more detail.\n",
      "Rejected Reward: 0.27\n",
      "Critic-RM\n",
      "Critique for Chosen Response: The response accurately addresses the user’s question by providing a clear explanation of the benefits of self-supervised learning\n",
      "in Natural Language Processing (NLP). It covers the key advantages of self-supervised learning, including its cost-effectiveness, ability to capture higher-order\n",
      "statistical dependencies, and suitability for pre-training models. The response is well-structured and easy to follow, making it a helpful and informative answer.\n",
      "The level of detail is sufficient to provide a good understanding of the topic.\n",
      "Chosen Reward: 0.95\n",
      "Critique for Rejected Response: The response provided is generally accurate and informative, but it does not specifically address the benefits of self-supervised\n",
      "learning in Natural Language Processing (NLP) as requested by the user. The response provides a broad overview of self-supervised learning and its advantages,\n",
      "but it lacks concrete examples and explanations of how these benefits apply to NLP tasks. To improve, the response should provide more tailored information\n",
      "and examples that demonstrate the relevance of self-supervised learning to NLP.\n",
      "Rejected Reward: 0.17\n",
      "19\n",
      "\n",
      "D\n",
      "Full Results for Critic-RM\n",
      "Table 11 presents the comprehensive results of Critic-RM performance, including a detailed breakdown by\n",
      "category.\n",
      "Table 11 The full result of different variants for Critic-RM. IS stands for ‘inference scaling’.\n",
      "Critic-RM-Rank\n",
      "Critic-RM-Rank-IS\n",
      "Critic-RM-Summ\n",
      "Critic-RM-Summ-IS\n",
      "alpacaeval-easy\n",
      "97.00\n",
      "97.00\n",
      "98.00\n",
      "98.00\n",
      "alpacaeval-hard\n",
      "96.84\n",
      "96.84\n",
      "96.84\n",
      "96.84\n",
      "alpacaeval-length\n",
      "96.84\n",
      "95.78\n",
      "97.89\n",
      "95.78\n",
      "mt-bench-easy\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "mt-bench-med\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "mt-bench-hard\n",
      "83.78\n",
      "86.48\n",
      "86.48\n",
      "83.78\n",
      "llmbar-natural\n",
      "90.00\n",
      "88.00\n",
      "86.00\n",
      "87.00\n",
      "llmbar-adver-neighbor\n",
      "69.40\n",
      "70.89\n",
      "64.17\n",
      "67.16\n",
      "llmbar-adver-GPTInst\n",
      "84.78\n",
      "83.70\n",
      "81.52\n",
      "77.17\n",
      "llmbar-adver-GPTOut\n",
      "74.47\n",
      "78.75\n",
      "76.59\n",
      "76.59\n",
      "llmbar-adver-manual\n",
      "78.26\n",
      "78.26\n",
      "78.26\n",
      "80.43\n",
      "prm800k\n",
      "83.67\n",
      "86.57\n",
      "82.10\n",
      "85.90\n",
      "hep-python\n",
      "93.90\n",
      "98.17\n",
      "96.95\n",
      "98.17\n",
      "hep-go\n",
      "95.73\n",
      "96.95\n",
      "93.29\n",
      "96.95\n",
      "hep-cpp\n",
      "96.95\n",
      "97.56\n",
      "97.56\n",
      "96.95\n",
      "hep-js\n",
      "93.29\n",
      "96.34\n",
      "96.34\n",
      "98.17\n",
      "hep-rust\n",
      "92.07\n",
      "93.29\n",
      "92.68\n",
      "93.90\n",
      "hep-java\n",
      "96.34\n",
      "97.56\n",
      "96.95\n",
      "99.39\n",
      "refusals-dangerous\n",
      "99.00\n",
      "98.00\n",
      "98.00\n",
      "99.00\n",
      "refusals-offensive\n",
      "99.00\n",
      "100\n",
      "99.00\n",
      "100\n",
      "xstest-should-respond\n",
      "96.40\n",
      "98.00\n",
      "96.40\n",
      "98.00\n",
      "xstest-should-refuse\n",
      "98.00\n",
      "98.70\n",
      "99.35\n",
      "99.35\n",
      "donotanswer\n",
      "77.20\n",
      "78.67\n",
      "80.14\n",
      "83.08\n",
      "20\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# API Tests",
   "id": "6875d35463b1c3bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:44:08.832735Z",
     "start_time": "2025-04-09T16:44:08.496754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "available_models = [model.name for model in genai.list_models()]\n",
    "print(f\"Available models: {available_models}\\n\")"
   ],
   "id": "9d6cb003e47f644f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['models/chat-bison-001', 'models/text-bison-001', 'models/embedding-gecko-001', 'models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-1.5-flash-8b-exp-0827', 'models/gemini-1.5-flash-8b-exp-0924', 'models/gemini-2.5-pro-exp-03-25', 'models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/learnlm-1.5-pro-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/embedding-001', 'models/text-embedding-004', 'models/gemini-embedding-exp-03-07', 'models/gemini-embedding-exp', 'models/aqa', 'models/imagen-3.0-generate-002', 'models/gemini-2.0-flash-live-001']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:44:13.344622Z",
     "start_time": "2025-04-09T16:44:08.875442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check API request\n",
    "gemini = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
    "check_response = gemini.generate_content(\"If you receive this request, please say Hello.\")\n",
    "print(f\"Usage metadata:\\n{check_response.usage_metadata}\")\n",
    "print(f\"Content:\\n{check_response.candidates[0].content.parts[0].text}\")\n",
    "time.sleep(4)"
   ],
   "id": "15ed66af2dda86ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "prompt_token_count: 10\n",
      "candidates_token_count: 3\n",
      "total_token_count: 13\n",
      "\n",
      "Content:\n",
      "Hello.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:44:18.658913Z",
     "start_time": "2025-04-09T16:44:13.361265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking if the model can get to the blog through the URL\n",
    "check_response = gemini.generate_content(f\"make a summary of this blog in 100 words\\n\\n{blog.url_blog}\")\n",
    "print(f\"Usage metadata:\\n{check_response.usage_metadata}\")\n",
    "print(f\"Content:\\n{check_response.candidates[0].content.parts[0].text}\")\n",
    "time.sleep(4)"
   ],
   "id": "e6edd756518cd933",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "prompt_token_count: 60\n",
      "candidates_token_count: 125\n",
      "total_token_count: 185\n",
      "\n",
      "Content:\n",
      "Sulbha Jindal's blog post reviews a paper about improving reward modeling for language models using self-generated critiques. Instead of relying solely on human feedback, the model is trained to critique its own initial responses and generate improved versions. These self-critiques are then used as additional data to train the reward model, leading to better alignment with desired outputs. This approach, called Self-Rewarding Language Model (SRLM), significantly enhances performance compared to models trained solely on human feedback, particularly in complex tasks requiring nuanced understanding and reasoning. The method also reduces the dependence on expensive and time-consuming human annotation.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:44:23.882466Z",
     "start_time": "2025-04-09T16:44:18.673331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Passing clean text to the model\n",
    "check_response = gemini.generate_content(f\"make a summary of this blog in 100 words\\n\\n{blog_text}\")\n",
    "print(f\"Usage metadata:\\n{check_response.usage_metadata}\")\n",
    "print(f\"Content:\\n{check_response.candidates[0].content.parts[0].text}\")\n",
    "time.sleep(4)"
   ],
   "id": "77be65f0bfc42326",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "prompt_token_count: 1138\n",
      "candidates_token_count: 129\n",
      "total_token_count: 1267\n",
      "\n",
      "Content:\n",
      "Critic-RM, a new framework, enhances LLM reward models using self-generated critiques, bypassing reliance on stronger teacher models. It uses instruction-tuned LLMs to generate critiques and scores, filtering for consistency with human preferences. Summarization and ranking further refine critiques used for reward model training. A weighted training strategy balances critique modeling and reward prediction, focusing initially on critiques before shifting to reward prediction incorporating critiques. During inference, the model generates a critique and then predicts the reward based on both the response and critique. Critic-RM significantly outperforms standard reward models, demonstrating improved reward modeling accuracy and highlighting the importance of high-quality critiques.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T16:52:16.158702Z",
     "start_time": "2025-04-09T16:52:09.125961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking ability to read PDF\n",
    "check_response = gemini.generate_content(f\"Create a table of contents for this publication. Also mention if you can see full text of the paper. If you can, write title of the provided paper.\\n{paper_url}\")\n",
    "print(f\"Usage metadata:\\n{check_response.usage_metadata}\")\n",
    "print(f\"Content:\\n{check_response.candidates[0].content.parts[0].text}\")\n",
    "time.sleep(4)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage metadata:\n",
      "prompt_token_count: 51\n",
      "candidates_token_count: 484\n",
      "total_token_count: 535\n",
      "\n",
      "Content:\n",
      "Okay, I've examined the PDF you linked. Here's the table of contents, derived from the paper's structure, along with the information about accessing the full text:\n",
      "\n",
      "**Title of the Paper:**\n",
      "\n",
      "Adversarial Training Can Help Non-robustly Fine-tuned Models\n",
      "\n",
      "**Table of Contents (Deduced from the Paper Structure):**\n",
      "\n",
      "1.  **Abstract**\n",
      "2.  **Introduction**\n",
      "3.  **Related Work**\n",
      "4.  **Preliminaries**\n",
      "    *   4.1 Problem Setup\n",
      "    *   4.2 Robust Models Can Break Non-robust Models\n",
      "5.  **Methodology**\n",
      "    *   5.1 Adversarial Training after Non-robust Fine-tuning\n",
      "6.  **Experimental Setup**\n",
      "    *   6.1 Datasets\n",
      "    *   6.2 Models\n",
      "    *   6.3 Training Details\n",
      "7.  **Experiments and Results**\n",
      "    *   7.1 Is Adversarial Training Useful after Non-robust Fine-tuning?\n",
      "    *   7.2 The Number of AT steps\n",
      "    *   7.3 On the Transferability of the Adversarial Training.\n",
      "8.  **Discussion**\n",
      "    *   8.1 Why Does Adversarial Training Help?\n",
      "9.  **Conclusion**\n",
      "10. **Limitations**\n",
      "11. **Broader Impact**\n",
      "12. **Acknowledgements**\n",
      "13. **References**\n",
      "14. **Appendix**\n",
      "    *   A.1 AT is More Helpful on Low-accuracy fine-tuned models\n",
      "    *   A.2 Details on How Transferability is Measured\n",
      "    *   A.3 AT Helps Both With and Without Clean Accuracy Drop\n",
      "    *   A.4 AT Helps on Models Fine-tuned With Larger Learning Rates\n",
      "    *   A.5 Adversarial Training With Larger Perturbation Sizes\n",
      "    *   A.6 Adversarial Training Improves Feature Alignment\n",
      "    *   A.7 Additional Details\n",
      "    *   A.8 Hyperparameter Settings\n",
      "    *   A.9 Training Time\n",
      "    *   A.10 Full Results\n",
      "\n",
      "**Full Text Availability:**\n",
      "\n",
      "Yes, I can see the full text of the paper. You provided a direct link to the PDF on arXiv, which is an open-access repository.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
