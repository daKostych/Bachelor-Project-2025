# Recurrent drafter for fast speculative decoding in Large Language Models— Paper Review

Paper Link — https://arxiv.org/pdf/2403.09919

Researchers at Apple have developed an innovative solution called ReDrafter to address the critical challenge of inference speed in large language models (LLMs). This novel approach tackles the inherent slowness of text generation in AI applications like chatbots by introducing a speculative decoding technique. ReDrafter leverages a recurrent neural network (RNN) to generate draft tokens in parallel with the main model, significantly reducing response latency.

## What is speculative decoding ?

Speculative decoding is a technique designed to speed up inference in large language models by utilizing smaller, more efficient models to predict potential sequences. Instead of generating each token one by one, these candidate sequences are quickly verified by the main LLM. This method helps alleviate memory bandwidth constraints by minimizing the number of passes through the LLM. However, it’s important to consider that using draft models can introduce additional overhead. Therefore, the reduction in LLM calls must be substantial enough to outweigh these costs and achieve a net speedup.

## Methodolgy

In this paper, the authors introduce ReDrafter, a novel approach for accelerating LLM inference. ReDrafter’s performance gains are driven by three key aspects:
    * RNN-based draft model: ReDrafter employs an RNN conditioned on the LLM’s hidden states as the draft model. It uses the output from the last layers of the LLM transformer and embeddings of historical tokens as inputs. This design leverages local temporal dependencies, enhancing draft prediction accuracy and converting computational resources into speedups.
    * Beam search and dynamic tree attention: The draft model utilizes beam search for inference, maintaining diversity and optimality in candidate response generation. ReDrafter applies a dynamic tree attention algorithm to eliminate duplicated prefixes among candidates, revealing a tree structure over beam search results and significantly reducing computational overhead.
    * Knowledge distillation training: The system is trained through knowledge distillation from the LLM, improving alignment between the draft model’s predictions and those of the LLM. This approach effectively shifts computational load from inference time to training time.

During each inference step, ReDrafter alternates between using a draft model to generate tokens and having the LLM verify and accept them. The system is most efficient when all candidate tokens are accepted, meaning the draft model’s predictions align with the LLM’s within a specified range T. The KL divergence serves as a natural loss function to optimize this alignment

## Results

ReDrafter significantly speeds up Vicuna inference in MT-Bench, achieving up to a 3.5x acceleration using a PyTorch implementation on Nvidia H100 GPUs. To demonstrate its practicality in production, ReDrafter was integrated into TensorRT-LLM, resulting in up to a 2.5x speedup on the same GPUs. Additionally, its effectiveness for on-device applications was confirmed by implementing the approach in MLX and testing it on Metal GPUs in Apple Silicon chips, where it achieved up to a 2.3x speedup.

The method achieves remarkable performance improvements by using a lightweight RNN draft model conditioned on the LLM’s hidden states. By employing techniques like beam search and dynamic tree attention, ReDrafter can accelerate text generation up to 3.5x on high-performance GPUs and 2.3x on Apple Silicon chips. This breakthrough promises to enhance the responsiveness of AI-powered applications, making interactions with large language models more seamless and efficient

Paper Link — https://arxiv.org/pdf/2403.09919