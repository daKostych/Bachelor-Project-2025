# SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents — Paper Review

Paper Link — https://arxiv.org/pdf/2411.03284

This paper review discusses the evolution and challenges of multi-agent Large Language Model (LLM) systems, and introduces a new approach called Sparse Mixture-of-Agents (SMoA).

Multi-agent LLM systems have been developed to allow different agents to focus on specific tasks, avoiding the need for extensive retraining. Early approaches used a layer-based structure, but this was limited by processing queries one agent at a time. The Mixture-of-Agents (MoA) method improved on this by allowing simultaneous processing of queries by multiple agents, with an aggregator combining their outputs.

However, MoA faces two main challenges:

1. High computational costs: While MoA is faster, it requires more overall computational power, which limits scalability and efficiency.

2. Lack of diverse thinking: MoA agents tend to generate similar responses, reducing effectiveness in tasks requiring varied perspectives.

To address these issues, the authors propose Sparse Mixture-of-Agents (SMoA), inspired by sparse mixture-of-experts (SMoE) designs. SMoA introduces two new agent types:

1. Judge LLM: Selects high-quality responses for the next round.
2. Moderator LLM: Controls information flow and decides when to end the process.

This approach aims to reduce unnecessary data processing, improving efficiency and scalability. Additionally, SMoA assigns distinct roles to each agent, promoting diverse thinking and problem-solving approaches. This design is influenced by the expert diversity principle used in SMoE to balance workload among experts.

By implementing these changes, SMoA seeks to overcome the limitations of previous multi-agent LLM systems and enhance their practical utility.

The key contributions of the paper are:

1. Identification of limitations in existing multi-agent LLM frameworks:
The authors highlight two main issues with current approaches, particularly the Mixture-of-Agents (MoA) method:
— High token computational cost, which limits scalability and efficiency
— Lack of diverse thinking among LLM agents, leading to homogenized responses

2. Proposal of a novel architecture — Sparse Mixture-of-Agents (SMoA):
This new approach is designed to address the limitations of existing frameworks by:
— Introducing sparsity in agent interactions through two new agent types: Judge LLM and Moderator LLM
— Assigning distinct roles to each LLM agent to promote diverse thinking

3. Experimental validation:
The authors conducted extensive experiments across various tasks to demonstrate that:
— SMoA achieves comparable performance to MoA
— SMoA uses significantly fewer computational resources than MoA

4. In-depth analysis and insights:
The paper provides:
— A comparison of different multi-agent methods
— Detailed discussion of the advantages of SMoA over existing approaches

These contributions aim to advance the field of multi-agent LLM systems by improving efficiency, scalability, and diversity of thinking while maintaining high performance levels. The SMoA architecture represents a significant step forward in addressing the challenges faced by current multi-agent LLM frameworks.