# Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review

Paper — https://arxiv.org/pdf/2411.16646

Reinforcement Learning from Human Feedback (RLHF) has become a critical methodology for aligning large language models (LLMs) with human preferences. At the core of RLHF lies the reward model (RM), which is designed to evaluate model outputs by assigning scores that reflect their alignment with human judgments. These scores guide the optimization process during training, such as providing reward signals in Proximal Policy Optimization (PPO), thereby encouraging LLMs to generate responses that are more helpful, honest, and harmless. This iterative process enhances the practical quality of LLM outputs in real-world applications.

## Current challenge

Typically, reward models are trained using preference pairs and optimized through pairwise logistic loss to produce a scalar score for each response. However, this scalar output is often hard to interpret and underutilizes the inherent language modeling capabilities of LLMs derived from pretraining and post-training. These limitations can weaken the feedback signals in RLHF, resulting in suboptimal policy updates. An alternative approach is the “LLM-as-a-judge” paradigm, where the LLM generates critiques and optionally provides discrete scores as proxies for response quality. This method leverages the model’s reasoning abilities more effectively, potentially addressing some of the shortcomings of traditional reward models

Incorporating critiques into reward modeling poses two significant challenges. First, there is the issue of conflicting objectives: generating critiques relies on language modeling capabilities, whereas traditional reward models output scalar values, making their integration into the language modeling process complex. Second, there are limitations with evaluators; off-the-shelf language models often lack the effectiveness needed for evaluation, and fine-tuning these models necessitates costly human-generated or annotated critiques. While knowledge distillation offers a potential solution by enabling a joint training approach to learn how to generate critiques and rewards simultaneously, it falls short when it comes to enhancing frontier models in situations where a stronger teacher model is unavailable. Here is Critic-RM, a new framework from Meta Researchers that enhances reward models using synthetic critiques, without relying on strong LLM teachers.

## Methodology

Critic-RM utilizes an instruction-finetuned large language model (LLM) as its foundation, generating multiple candidate critiques, each accompanied by a discrete score for individual responses. The process begins with a consistency-guided filtering technique that retains only those critiques whose scores align with human-annotated preference labels.

To further improve the quality of these synthetic critiques, two additional strategies — summarization and ranking — are proposed to refine the critiques used in training the reward model. The framework investigates the application of an off-the-shelf instruction-finetuned LLM, for both critique generation and reward modeling. Initially, Critic-RM generates candidate critiques for each prompt-response pair, followed by a filtering step aimed at minimizing the influence of potentially noisy rationales that could lead to incorrect predictions. This approach allows for the augmentation of preference pairs with additional critiques, ultimately enhancing the precision of reward modeling.

After generating critiques for each response, the primary challenge is developing an effective training strategy to integrate critique modeling and scalar reward prediction objectives. To address this, we propose a simple weighting strategy that balances these objectives. Initially, the model prioritizes critique modeling loss, then gradually shifts its focus toward reward prediction, utilizing both the response and the critique. This balanced approach enables the model to excel in generating high-quality critiques while maintaining accurate reward predictions.

In Critic-RM, an additional step is introduced during inference for each (prompt, response) pair. Given a (prompt, response) pair $$(x, y)$$, the model first generates a critique $$z \sim q_\phi(x, y)$$. It then predicts the reward for the response as $$r = r_\psi(x, [y, z])$$, where the reward prediction incorporates both the response and its associated critique. This process ensures a more nuanced and precise evaluation of responses.

## Results

Incorporating critiques into reward modeling has demonstrated significant benefits, particularly with the Critic-RM framework, which consistently outperforms the baselines in this study. Specifically, when trained on the same preference data, Critic-RM achieves an improvement of 3.7% to 4.7% over standard reward models. The quality of critiques plays a crucial role; comparisons reveal that other models incorporating critiques show smaller performance gains than Critic-RM when evaluated against the standard reward model. Additionally, performance enhancements are observed for both Critic-RM and its baselines when multiple critiques are generated during inference, particularly benefiting reasoning tasks.

## Summary

Researchers have developed Critic-RM, an innovative self-critiquing framework aimed at enhancing reward modeling for large language models (LLMs). This novel approach leverages the inherent capabilities of LLMs to generate and refine critiques, implementing a self-improvement mechanism that enhances both the quality of critiques and the accuracy of reward predictions. The findings from this research emphasize the efficacy of Critic-RM in improving reward modeling accuracy and underscore the crucial role of high-quality critiques in this process. Beyond merely enhancing precision in reward modeling, the framework has demonstrated robust performance across various benchmarks, including RewardBench and CrossEval, highlighting its potential to significantly advance the field of language model optimization and alignment with human preferences.

## Appendix
    * Paper link — https://arxiv.org/pdf/2411.16646