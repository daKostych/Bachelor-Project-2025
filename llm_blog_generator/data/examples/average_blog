# Data Centric AI — LLAVA

https://arxiv.org/abs/2304.08485

What are the major contributions of the paper?

[1] They are able to use bounding box or image caption information from images to ask GPT-4 to generate 3 kinds of questions and their answer [a] High-level details [b] Conversations [c] Deep level details

[2] They use this data to fine-tune an end to end model

What is the architecture of the model that they fine-tune?

It consists of the image encoder part of clip followed by a projection matrix. The matrix projects the image embeddings to the same dimension as the size of the input embedding to a LLAMA model. This is obviously somehow appended to the generated questions embeddings and then the model is used to generate answers as a decoder normally does

What do they keep frozen/what do they finetune?

There are 2 stages, stage [1] only the projection matrix for the clip is finetuned so that we can get good representations for the language model. [2] Then the image encoder is kept frozen and the projection matrix and the language model is updated

Which datasets do they use for each step?

The model contains an image encoder that is projected to the language embedding dimension. Note that this encoder is from a CLIP model. Moreover, the text decoder is a Vicuna model. First, they need to adapt this projection layer from the image embedding dimension to the text embedding dimension. For this, the parameters are kept frozen and the projection layer is updated using the C3M (or Conceptual Captions dataset). This is followed by updating the vicuna model and the projection layer, while keeping clip encoder frozen using the COCO dataset. Finally, the model performance is evaluated on the Science QA dataset.

Why use COCO for second stage finetuning and C3M for the first stage even though C3M is created by humans (online scraping)?

This is because the COCO questions and answers are augmented using the prompts mentioned in the paper and hence, the dataset is very detailed.

How is the C3M dataset collected?

Using ALT text and corresponding images. The dataset filteration is quite informative: [1] Image only: Filter images based on format, aspect ratio, image resolution [2] Text only: Missing proper nouns, determiners, conjunctions or containing several nouns or caps at improper places, SEO terms then filter out. Pornographic content [3] Image-text [4] We want to have a diverse dataset with some standardization so that the task does not become too difficult for something like VQA (e.x. generating caption that this is Justin Bieber singing is difficult as it would require a large number of pictures of justin bieber). So we use Hpernyms and replace proper nouns with hypernyms, remove place names etc. [5] Finally we identify concepts in the images using the captions and make sure that we are able to cluster them such that each concept has enough support example 100

Overall we want to make sure that we have a variety of concepts with enough coverage

How is this different from how LAION is collected?

Very similar, using different models for filtering pornographic and SMID (socio-moral image dataset) content or morally offensive content. When we are inputting to a model we want to ensure that hefty class imbalance is avoided

Why first finetune with C3M then when its so diverse?

Because the dataset generated in the LLaVa paper using prompting has much more complex discussions

How is the model evaluated?

https://llava-vl.github.io/

It mainly uses the Science QA dataset, which is collected for K-12 education. For ground truth explanation GPT-4 is provided with image information and the question and the answer and is then used to evaluate the answer generated by the LLaVa model. Moreover, there is an inherent notion of accuracy as the questions have multiple options.

How is GPT used for ensembling?

GPT is used to generate answers and is also used as the judge. When GPT cannot provide good answers LLaVa is used hence enabling ensembling

What are the dimensions of the datasets?

C3M — is 3M but only 600K are filtered COCO — is used because it has bounding boxes. Around 160K. It has conversations, complex question answering etc.

For the dataset filtration the filtering of C3M here is quite interesting and supplants beliefs. How is it filtered? Look at noun phrases (words) in each caption, remove the ones with low frequency and while adding the remaining ones makes sure that none has a fz higher than 100 so as to keep the data balanced

What are some potential applications of LLaVa?

Giving out recopies based on ingredients shown are particularly interesting as they can be extended to robotics with RL based approaches