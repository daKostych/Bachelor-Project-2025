id,title_blog,url_blog,author_blog,author_followers,claps,comments,title_paper,url_paper,engagement_score,normalized_engagement_score,blog_full_text,engagement_level
1,Training Large Language Models: From TRPO to GRPO,https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194,Maxime Wolf,349,177,1,Foundations of Large Language Models,https://arxiv.org/pdf/2501.09223,0.5157593123209169,49.7,"# Training Large Language Models: From TRPO to GRPO

## Dive into how DeepSeek, ChatGPT, and other LLMs leverage reinforcement learning to enhance their training
Data Science Collective
DeepSeek has recently made quite a buzz in the AI community, thanks to its impressive performance at relatively low costs. I think this is a perfect opportunity to dive deeper into how Large Language Models (LLMs) are trained. In this article, we will focus on the Reinforcement Learning (RL) side of things: we will cover TRPO, PPO, and, more recently, GRPO (don't worry, I will explain all these terms soon!)
I have aimed to keep this article relatively easy to read and accessible, by minimizing the math, so you won’t need a deep Reinforcement Learning background to follow along. However, I will assume that you have some familiarity with Machine Learning, Deep Learning, and a basic understanding of how LLMs work.
I hope you enjoy the article, and feel free to leave a clap!

# The 3 steps of LLM training
Before diving into RL specifics, let’s briefly recap the three main stages of training a Large Language Model:
    * Pre-training: the model is trained on a massive dataset to predict the next token in a sequence based on preceding tokens.
    * Supervised Fine-Tuning (SFT): the model is then fine-tuned on more targeted data and aligned with specific instructions.
    * Reinforcement Learning (often called RLHF for Reinforcement Learning with Human Feedback): this is the focus of this article. The main goal is to further refine responses’ alignments with human preferences, by allowing the model to learn directly from feedback.

# Reinforcement Learning Basics
Before diving deeper, let’s briefly revisit the core ideas behind Reinforcement Learning.
RL is quite straightforward to understand at a high level: an agent interacts with an environment. The agent resides in a specific state within the environment and can take actions to transition to other states. Each action yields a reward from the environment: this is how the environment provides feedback that guides the agent’s future actions.
Consider the following example: a robot (the agent) navigates (and tries to exit) a maze (the environment).
    * The state is the current situation of the environment (the robot’s position in the maze).
    * The robot can take different actions: for example, it can move forward, turn left, or turn right.
    * Successfully navigating towards the exit yields a positive reward, while hitting a wall or getting stuck in the maze results in negative rewards.
Easy! Now, let’s now make an analogy to how RL is used in the context of LLMs.

# RL in the context of LLMs
When used during LLM training, RL is defined by the following components:
    * The LLM itself is the agent
    * Environment: everything external to the LLM, including user prompts, feedback systems, and other contextual information. This is basically the framework the LLM is interacting with during training.
    * Actions: these are responses to a query from the model. More specifically: these are the tokens that the LLM decides to generate in response to a query.
    * State: the current query being answered along with tokens the LLM has generated so far (i.e., the partial responses).
    * Rewards: this is a bit more tricky here: unlike the maze example above, there is usually no binary reward. In the context of LLMs, rewards usually come from a separate reward model, which outputs a score for each (query, response) pair. This model is trained from human-annotated data (hence “RLHF”) where annotators rank different responses. The goal is for higher-quality responses to receive higher rewards.
Note: in some cases, rewards can actually get simpler. For example, in DeepSeekMath, rule-based approaches can be used because math responses tend to be more deterministic (correct or wrong answer)
Policy is the final concept we need for now. In RL terms, a policy is simply the strategy for deciding which action to take. In the case of an LLM, the policy outputs a probability distribution over possible tokens at each step: in short, this is what the model uses to sample the next token to generate. Concretely, the policy is determined by the model’s parameters (weights). During RL training, we adjust these parameters so the LLM becomes more likely to produce “better” tokens— that is, tokens that produce higher reward scores.
We often write the policy as:
where a is the action (a token to generate), s the state (the query and tokens generated so far), and θ (model’s parameters).
This idea of finding the best policy is the whole point of RL! Since we don’t have labeled data (like we do in supervised learning) we use rewards to adjust our policy to take better actions. (In LLM terms: we adjust the parameters of our LLM to generate better tokens.)

# TRPO (Trust Region Policy Optimization)

## An analogy with supervised learning
Let’s take a quick step back to how supervised learning typically works. you have labeled data and use a loss function (like cross-entropy) to measure how close your model’s predictions are to the true labels.
We can then use algorithms like backpropagation and gradient descent to minimize our loss function and update the weights θ of our model.
Recall that our policy also outputs probabilities! In that sense, it is analogous to the model’s predictions in supervised learning… We are tempted to write something like:
where s is the current state and a is a possible action.
A(s, a) is called the advantage function and measures how good is the chosen action in the current state, compared to a baseline. This is very much like the notion of labels in supervised learning but derived from rewards instead of explicit labeling. To simplify, we can write the advantage as:
In practice, the baseline is calculated using a value function. This is a common term in RL that I will explain later. What you need to know for now is that it measures the expected reward we would receive if we continue following the current policy from the state s.

## What is TRPO?
TRPO (Trust Region Policy Optimization) builds on this idea of using the advantage function but adds a critical ingredient for stability: it constrains how far the new policy can deviate from the old policy at each update step (similar to what we do with batch gradient descent for example).
    * It introduces a KL divergence term (see it as a measure of similarity) between the current and the old policy:
    * It also divides the policy by the old policy. This ratio, multiplied by the advantage function, gives us a sense of how beneficial each update is relative to the old policy.
Putting it all together, TRPO tries to maximize a surrogate objective (which involves the advantage and the policy ratio) subject to a KL divergence constraint.

# PPO (Proximal Policy Optimization)
While TRPO was a significant advancement, it’s no longer used widely in practice, especially for training LLMs, due to its computationally intensive gradient calculations.
Instead, PPO is now the preferred approach in most LLMs architecture, including ChatGPT, Gemini, and more.
It is actually quite similar to TRPO, but instead of enforcing a hard constraint on the KL divergence, PPO introduces a “clipped surrogate objective” that implicitly restricts policy updates, and greatly simplifies the optimization process.
Here is a breakdown of the PPO objective function we maximize to tweak our model’s parameters.

# GRPO (Group Relative Policy Optimization)

## How is the value function usually obtained?
Let’s first talk more about the advantage and the value functions I introduced earlier.
In typical setups (like PPO), a value model is trained alongside the policy. Its goal is to predict the value of each action we take (each token generated by the model), using the rewards we obtain (remember that the value should represent the expected cumulative reward).
Here is how it works in practice. Take the query “What is 2+2?” as an example. Our model outputs “2+2 is 4” and receives a reward of 0.8 for that response. We then go backward and attribute discounted rewards to each prefix:
    * “2+2 is 4” gets a value of 0.8
    * “2+2 is” (1 token backward) gets a value of 0.8γ
    * “2+2” (2 tokens backward) gets a value of 0.8γ²
    * etc.
where γ is the discount factor (0.9 for example). We then use these prefixes and associated values to train the value model.
Important note: the value model and the reward model are two different things. The reward model is trained before the RL process and uses pairs of (query, response) and human ranking. The value model is trained concurrently to the policy, and aims at predicting the future expected reward at each step of the generation process.

## What’s new in GRPO
Even if in practice, the reward model is often derived from the policy (training only the “head”), we still end up maintaining many models and handling multiple training procedures (policy, reward, value model). GRPO streamlines this by introducing a more efficient method.
Remember what I said earlier?
In PPO, we decided to use our value function as the baseline. GRPO chooses something else: Here is what GRPO does: concretely, for each query, GRPO generates a group of responses (group of size G) and uses their rewards to calculate each response’s advantage as a z-score:
where rᵢ is the reward of the i-th response and μ and σ are the mean and standard deviation of rewards in that group.
This naturally eliminates the need for a separate value model. This idea makes a lot of sense when you think about it! It aligns with the value function we introduced before and also measures, in a sense, an “expected” reward we can obtain. Also, this new method is well adapted to our problem because LLMs can easily generate multiple non-deterministic outputs by using a low temperature (controls the randomness of tokens generation).
This is the main idea behind GRPO: getting rid of the value model.
Finally, GRPO adds a KL divergence term (to be exact, GRPO uses a simple approximation of the KL divergence to improve the algorithm further) directly into its objective, comparing the current policy to a reference policy (often the post-SFT model).
See the final formulation below:
And… that’s mostly it for GRPO! I hope this gives you a clear overview of the process: it still relies on the same foundational ideas as TRPO and PPO but introduces additional improvements to make training more efficient, faster, and cheaper — key factors behind DeepSeek’s success.

# Conclusion
Reinforcement Learning has become a cornerstone for training today’s Large Language Models, particularly through PPO, and more recently GRPO. Each method rests on the same RL fundamentals — states, actions, rewards, and policies — but adds its own twist to balance stability, efficiency, and human alignment:
• TRPO introduced strict policy constraints via KL divergence
• PPO eased those constraints with a clipped objective
• GRPO took an extra step by removing the value model requirement and using group-based reward normalization. Of course, DeepSeek also benefits from other innovations, like high-quality data and other training strategies, but that is for another time!
I hope this article gave you a clearer picture of how these methods connect and evolve. I believe that Reinforcement Learning will become the main focus in training LLMs to improve their performance, surpassing pre-training and SFT in driving future innovations.
If you’re interested in diving deeper, feel free to check out the references below or explore my previous posts.
Thanks for reading, and feel free to leave a clap and a comment!
Want to learn more about Transformers or dive into the math behind the Curse of Dimensionality? Check out my previous articles:

## Transformers: How Do They Transform Your Data?

### Diving into the Transformers architecture and what makes them unbeatable at language tasks
towardsdatascience.com

## The Math Behind “The Curse of Dimensionality”

### Dive into the “Curse of Dimensionality” concept and understand the math behind all the surprising phenomena that arise…
towardsdatascience.com
    * Feel free to connect on LinkedIn
    * Follow me on GitHub for more content
    * Visit my website: maximewolf.com
References:
    * [1] “Foundations of Large Language Models”, 2025. https://arxiv.org/pdf/2501.09223
    * [2] “Reinforcement Learning.” Enaris. Available at: https://enaris.org/material/en/Reinforcement%20Learning/index.html
    * [3] Y. Gokhale. “Introduction to LLMs and the Generative AI Part 5: RLHF,” Medium, 2023. Available at: https://medium.com/@yash9439/introduction-to-llms-and-the-generative-ai-part-5-rlhf-64e83fbcd795
    * [4] L. Weng. “An Overview of Reinforcement Learning,” 2018. Available at: https://lilianweng.github.io/posts/2018-02-19-rl-overview/
    * [5] “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”, 2025. https://arxiv.org/pdf/2501.12948
    * [6] “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”, 2025. https://arxiv.org/pdf/2402.03300
    * [7] “Trust Region Policy Optimization”, 2017. https://arxiv.org/pdf/1502.05477",Good
2,Training Large Language Models: From TRPO to GRPO,https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194,Maxime Wolf,349,177,1,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/pdf/2501.12948,0.5157593123209169,49.7,"# Training Large Language Models: From TRPO to GRPO

## Dive into how DeepSeek, ChatGPT, and other LLMs leverage reinforcement learning to enhance their training
Data Science Collective
DeepSeek has recently made quite a buzz in the AI community, thanks to its impressive performance at relatively low costs. I think this is a perfect opportunity to dive deeper into how Large Language Models (LLMs) are trained. In this article, we will focus on the Reinforcement Learning (RL) side of things: we will cover TRPO, PPO, and, more recently, GRPO (don't worry, I will explain all these terms soon!)
I have aimed to keep this article relatively easy to read and accessible, by minimizing the math, so you won’t need a deep Reinforcement Learning background to follow along. However, I will assume that you have some familiarity with Machine Learning, Deep Learning, and a basic understanding of how LLMs work.
I hope you enjoy the article, and feel free to leave a clap!

# The 3 steps of LLM training
Before diving into RL specifics, let’s briefly recap the three main stages of training a Large Language Model:
    * Pre-training: the model is trained on a massive dataset to predict the next token in a sequence based on preceding tokens.
    * Supervised Fine-Tuning (SFT): the model is then fine-tuned on more targeted data and aligned with specific instructions.
    * Reinforcement Learning (often called RLHF for Reinforcement Learning with Human Feedback): this is the focus of this article. The main goal is to further refine responses’ alignments with human preferences, by allowing the model to learn directly from feedback.

# Reinforcement Learning Basics
Before diving deeper, let’s briefly revisit the core ideas behind Reinforcement Learning.
RL is quite straightforward to understand at a high level: an agent interacts with an environment. The agent resides in a specific state within the environment and can take actions to transition to other states. Each action yields a reward from the environment: this is how the environment provides feedback that guides the agent’s future actions.
Consider the following example: a robot (the agent) navigates (and tries to exit) a maze (the environment).
    * The state is the current situation of the environment (the robot’s position in the maze).
    * The robot can take different actions: for example, it can move forward, turn left, or turn right.
    * Successfully navigating towards the exit yields a positive reward, while hitting a wall or getting stuck in the maze results in negative rewards.
Easy! Now, let’s now make an analogy to how RL is used in the context of LLMs.

# RL in the context of LLMs
When used during LLM training, RL is defined by the following components:
    * The LLM itself is the agent
    * Environment: everything external to the LLM, including user prompts, feedback systems, and other contextual information. This is basically the framework the LLM is interacting with during training.
    * Actions: these are responses to a query from the model. More specifically: these are the tokens that the LLM decides to generate in response to a query.
    * State: the current query being answered along with tokens the LLM has generated so far (i.e., the partial responses).
    * Rewards: this is a bit more tricky here: unlike the maze example above, there is usually no binary reward. In the context of LLMs, rewards usually come from a separate reward model, which outputs a score for each (query, response) pair. This model is trained from human-annotated data (hence “RLHF”) where annotators rank different responses. The goal is for higher-quality responses to receive higher rewards.
Note: in some cases, rewards can actually get simpler. For example, in DeepSeekMath, rule-based approaches can be used because math responses tend to be more deterministic (correct or wrong answer)
Policy is the final concept we need for now. In RL terms, a policy is simply the strategy for deciding which action to take. In the case of an LLM, the policy outputs a probability distribution over possible tokens at each step: in short, this is what the model uses to sample the next token to generate. Concretely, the policy is determined by the model’s parameters (weights). During RL training, we adjust these parameters so the LLM becomes more likely to produce “better” tokens— that is, tokens that produce higher reward scores.
We often write the policy as:
where a is the action (a token to generate), s the state (the query and tokens generated so far), and θ (model’s parameters).
This idea of finding the best policy is the whole point of RL! Since we don’t have labeled data (like we do in supervised learning) we use rewards to adjust our policy to take better actions. (In LLM terms: we adjust the parameters of our LLM to generate better tokens.)

# TRPO (Trust Region Policy Optimization)

## An analogy with supervised learning
Let’s take a quick step back to how supervised learning typically works. you have labeled data and use a loss function (like cross-entropy) to measure how close your model’s predictions are to the true labels.
We can then use algorithms like backpropagation and gradient descent to minimize our loss function and update the weights θ of our model.
Recall that our policy also outputs probabilities! In that sense, it is analogous to the model’s predictions in supervised learning… We are tempted to write something like:
where s is the current state and a is a possible action.
A(s, a) is called the advantage function and measures how good is the chosen action in the current state, compared to a baseline. This is very much like the notion of labels in supervised learning but derived from rewards instead of explicit labeling. To simplify, we can write the advantage as:
In practice, the baseline is calculated using a value function. This is a common term in RL that I will explain later. What you need to know for now is that it measures the expected reward we would receive if we continue following the current policy from the state s.

## What is TRPO?
TRPO (Trust Region Policy Optimization) builds on this idea of using the advantage function but adds a critical ingredient for stability: it constrains how far the new policy can deviate from the old policy at each update step (similar to what we do with batch gradient descent for example).
    * It introduces a KL divergence term (see it as a measure of similarity) between the current and the old policy:
    * It also divides the policy by the old policy. This ratio, multiplied by the advantage function, gives us a sense of how beneficial each update is relative to the old policy.
Putting it all together, TRPO tries to maximize a surrogate objective (which involves the advantage and the policy ratio) subject to a KL divergence constraint.

# PPO (Proximal Policy Optimization)
While TRPO was a significant advancement, it’s no longer used widely in practice, especially for training LLMs, due to its computationally intensive gradient calculations.
Instead, PPO is now the preferred approach in most LLMs architecture, including ChatGPT, Gemini, and more.
It is actually quite similar to TRPO, but instead of enforcing a hard constraint on the KL divergence, PPO introduces a “clipped surrogate objective” that implicitly restricts policy updates, and greatly simplifies the optimization process.
Here is a breakdown of the PPO objective function we maximize to tweak our model’s parameters.

# GRPO (Group Relative Policy Optimization)

## How is the value function usually obtained?
Let’s first talk more about the advantage and the value functions I introduced earlier.
In typical setups (like PPO), a value model is trained alongside the policy. Its goal is to predict the value of each action we take (each token generated by the model), using the rewards we obtain (remember that the value should represent the expected cumulative reward).
Here is how it works in practice. Take the query “What is 2+2?” as an example. Our model outputs “2+2 is 4” and receives a reward of 0.8 for that response. We then go backward and attribute discounted rewards to each prefix:
    * “2+2 is 4” gets a value of 0.8
    * “2+2 is” (1 token backward) gets a value of 0.8γ
    * “2+2” (2 tokens backward) gets a value of 0.8γ²
    * etc.
where γ is the discount factor (0.9 for example). We then use these prefixes and associated values to train the value model.
Important note: the value model and the reward model are two different things. The reward model is trained before the RL process and uses pairs of (query, response) and human ranking. The value model is trained concurrently to the policy, and aims at predicting the future expected reward at each step of the generation process.

## What’s new in GRPO
Even if in practice, the reward model is often derived from the policy (training only the “head”), we still end up maintaining many models and handling multiple training procedures (policy, reward, value model). GRPO streamlines this by introducing a more efficient method.
Remember what I said earlier?
In PPO, we decided to use our value function as the baseline. GRPO chooses something else: Here is what GRPO does: concretely, for each query, GRPO generates a group of responses (group of size G) and uses their rewards to calculate each response’s advantage as a z-score:
where rᵢ is the reward of the i-th response and μ and σ are the mean and standard deviation of rewards in that group.
This naturally eliminates the need for a separate value model. This idea makes a lot of sense when you think about it! It aligns with the value function we introduced before and also measures, in a sense, an “expected” reward we can obtain. Also, this new method is well adapted to our problem because LLMs can easily generate multiple non-deterministic outputs by using a low temperature (controls the randomness of tokens generation).
This is the main idea behind GRPO: getting rid of the value model.
Finally, GRPO adds a KL divergence term (to be exact, GRPO uses a simple approximation of the KL divergence to improve the algorithm further) directly into its objective, comparing the current policy to a reference policy (often the post-SFT model).
See the final formulation below:
And… that’s mostly it for GRPO! I hope this gives you a clear overview of the process: it still relies on the same foundational ideas as TRPO and PPO but introduces additional improvements to make training more efficient, faster, and cheaper — key factors behind DeepSeek’s success.

# Conclusion
Reinforcement Learning has become a cornerstone for training today’s Large Language Models, particularly through PPO, and more recently GRPO. Each method rests on the same RL fundamentals — states, actions, rewards, and policies — but adds its own twist to balance stability, efficiency, and human alignment:
• TRPO introduced strict policy constraints via KL divergence
• PPO eased those constraints with a clipped objective
• GRPO took an extra step by removing the value model requirement and using group-based reward normalization. Of course, DeepSeek also benefits from other innovations, like high-quality data and other training strategies, but that is for another time!
I hope this article gave you a clearer picture of how these methods connect and evolve. I believe that Reinforcement Learning will become the main focus in training LLMs to improve their performance, surpassing pre-training and SFT in driving future innovations.
If you’re interested in diving deeper, feel free to check out the references below or explore my previous posts.
Thanks for reading, and feel free to leave a clap and a comment!
Want to learn more about Transformers or dive into the math behind the Curse of Dimensionality? Check out my previous articles:

## Transformers: How Do They Transform Your Data?

### Diving into the Transformers architecture and what makes them unbeatable at language tasks
towardsdatascience.com

## The Math Behind “The Curse of Dimensionality”

### Dive into the “Curse of Dimensionality” concept and understand the math behind all the surprising phenomena that arise…
towardsdatascience.com
    * Feel free to connect on LinkedIn
    * Follow me on GitHub for more content
    * Visit my website: maximewolf.com
References:
    * [1] “Foundations of Large Language Models”, 2025. https://arxiv.org/pdf/2501.09223
    * [2] “Reinforcement Learning.” Enaris. Available at: https://enaris.org/material/en/Reinforcement%20Learning/index.html
    * [3] Y. Gokhale. “Introduction to LLMs and the Generative AI Part 5: RLHF,” Medium, 2023. Available at: https://medium.com/@yash9439/introduction-to-llms-and-the-generative-ai-part-5-rlhf-64e83fbcd795
    * [4] L. Weng. “An Overview of Reinforcement Learning,” 2018. Available at: https://lilianweng.github.io/posts/2018-02-19-rl-overview/
    * [5] “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”, 2025. https://arxiv.org/pdf/2501.12948
    * [6] “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”, 2025. https://arxiv.org/pdf/2402.03300
    * [7] “Trust Region Policy Optimization”, 2017. https://arxiv.org/pdf/1502.05477",Good
3,Training Large Language Models: From TRPO to GRPO,https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194,Maxime Wolf,349,177,1,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,https://arxiv.org/pdf/2402.03300,0.5157593123209169,49.7,"# Training Large Language Models: From TRPO to GRPO

## Dive into how DeepSeek, ChatGPT, and other LLMs leverage reinforcement learning to enhance their training
Data Science Collective
DeepSeek has recently made quite a buzz in the AI community, thanks to its impressive performance at relatively low costs. I think this is a perfect opportunity to dive deeper into how Large Language Models (LLMs) are trained. In this article, we will focus on the Reinforcement Learning (RL) side of things: we will cover TRPO, PPO, and, more recently, GRPO (don't worry, I will explain all these terms soon!)
I have aimed to keep this article relatively easy to read and accessible, by minimizing the math, so you won’t need a deep Reinforcement Learning background to follow along. However, I will assume that you have some familiarity with Machine Learning, Deep Learning, and a basic understanding of how LLMs work.
I hope you enjoy the article, and feel free to leave a clap!

# The 3 steps of LLM training
Before diving into RL specifics, let’s briefly recap the three main stages of training a Large Language Model:
    * Pre-training: the model is trained on a massive dataset to predict the next token in a sequence based on preceding tokens.
    * Supervised Fine-Tuning (SFT): the model is then fine-tuned on more targeted data and aligned with specific instructions.
    * Reinforcement Learning (often called RLHF for Reinforcement Learning with Human Feedback): this is the focus of this article. The main goal is to further refine responses’ alignments with human preferences, by allowing the model to learn directly from feedback.

# Reinforcement Learning Basics
Before diving deeper, let’s briefly revisit the core ideas behind Reinforcement Learning.
RL is quite straightforward to understand at a high level: an agent interacts with an environment. The agent resides in a specific state within the environment and can take actions to transition to other states. Each action yields a reward from the environment: this is how the environment provides feedback that guides the agent’s future actions.
Consider the following example: a robot (the agent) navigates (and tries to exit) a maze (the environment).
    * The state is the current situation of the environment (the robot’s position in the maze).
    * The robot can take different actions: for example, it can move forward, turn left, or turn right.
    * Successfully navigating towards the exit yields a positive reward, while hitting a wall or getting stuck in the maze results in negative rewards.
Easy! Now, let’s now make an analogy to how RL is used in the context of LLMs.

# RL in the context of LLMs
When used during LLM training, RL is defined by the following components:
    * The LLM itself is the agent
    * Environment: everything external to the LLM, including user prompts, feedback systems, and other contextual information. This is basically the framework the LLM is interacting with during training.
    * Actions: these are responses to a query from the model. More specifically: these are the tokens that the LLM decides to generate in response to a query.
    * State: the current query being answered along with tokens the LLM has generated so far (i.e., the partial responses).
    * Rewards: this is a bit more tricky here: unlike the maze example above, there is usually no binary reward. In the context of LLMs, rewards usually come from a separate reward model, which outputs a score for each (query, response) pair. This model is trained from human-annotated data (hence “RLHF”) where annotators rank different responses. The goal is for higher-quality responses to receive higher rewards.
Note: in some cases, rewards can actually get simpler. For example, in DeepSeekMath, rule-based approaches can be used because math responses tend to be more deterministic (correct or wrong answer)
Policy is the final concept we need for now. In RL terms, a policy is simply the strategy for deciding which action to take. In the case of an LLM, the policy outputs a probability distribution over possible tokens at each step: in short, this is what the model uses to sample the next token to generate. Concretely, the policy is determined by the model’s parameters (weights). During RL training, we adjust these parameters so the LLM becomes more likely to produce “better” tokens— that is, tokens that produce higher reward scores.
We often write the policy as:
where a is the action (a token to generate), s the state (the query and tokens generated so far), and θ (model’s parameters).
This idea of finding the best policy is the whole point of RL! Since we don’t have labeled data (like we do in supervised learning) we use rewards to adjust our policy to take better actions. (In LLM terms: we adjust the parameters of our LLM to generate better tokens.)

# TRPO (Trust Region Policy Optimization)

## An analogy with supervised learning
Let’s take a quick step back to how supervised learning typically works. you have labeled data and use a loss function (like cross-entropy) to measure how close your model’s predictions are to the true labels.
We can then use algorithms like backpropagation and gradient descent to minimize our loss function and update the weights θ of our model.
Recall that our policy also outputs probabilities! In that sense, it is analogous to the model’s predictions in supervised learning… We are tempted to write something like:
where s is the current state and a is a possible action.
A(s, a) is called the advantage function and measures how good is the chosen action in the current state, compared to a baseline. This is very much like the notion of labels in supervised learning but derived from rewards instead of explicit labeling. To simplify, we can write the advantage as:
In practice, the baseline is calculated using a value function. This is a common term in RL that I will explain later. What you need to know for now is that it measures the expected reward we would receive if we continue following the current policy from the state s.

## What is TRPO?
TRPO (Trust Region Policy Optimization) builds on this idea of using the advantage function but adds a critical ingredient for stability: it constrains how far the new policy can deviate from the old policy at each update step (similar to what we do with batch gradient descent for example).
    * It introduces a KL divergence term (see it as a measure of similarity) between the current and the old policy:
    * It also divides the policy by the old policy. This ratio, multiplied by the advantage function, gives us a sense of how beneficial each update is relative to the old policy.
Putting it all together, TRPO tries to maximize a surrogate objective (which involves the advantage and the policy ratio) subject to a KL divergence constraint.

# PPO (Proximal Policy Optimization)
While TRPO was a significant advancement, it’s no longer used widely in practice, especially for training LLMs, due to its computationally intensive gradient calculations.
Instead, PPO is now the preferred approach in most LLMs architecture, including ChatGPT, Gemini, and more.
It is actually quite similar to TRPO, but instead of enforcing a hard constraint on the KL divergence, PPO introduces a “clipped surrogate objective” that implicitly restricts policy updates, and greatly simplifies the optimization process.
Here is a breakdown of the PPO objective function we maximize to tweak our model’s parameters.

# GRPO (Group Relative Policy Optimization)

## How is the value function usually obtained?
Let’s first talk more about the advantage and the value functions I introduced earlier.
In typical setups (like PPO), a value model is trained alongside the policy. Its goal is to predict the value of each action we take (each token generated by the model), using the rewards we obtain (remember that the value should represent the expected cumulative reward).
Here is how it works in practice. Take the query “What is 2+2?” as an example. Our model outputs “2+2 is 4” and receives a reward of 0.8 for that response. We then go backward and attribute discounted rewards to each prefix:
    * “2+2 is 4” gets a value of 0.8
    * “2+2 is” (1 token backward) gets a value of 0.8γ
    * “2+2” (2 tokens backward) gets a value of 0.8γ²
    * etc.
where γ is the discount factor (0.9 for example). We then use these prefixes and associated values to train the value model.
Important note: the value model and the reward model are two different things. The reward model is trained before the RL process and uses pairs of (query, response) and human ranking. The value model is trained concurrently to the policy, and aims at predicting the future expected reward at each step of the generation process.

## What’s new in GRPO
Even if in practice, the reward model is often derived from the policy (training only the “head”), we still end up maintaining many models and handling multiple training procedures (policy, reward, value model). GRPO streamlines this by introducing a more efficient method.
Remember what I said earlier?
In PPO, we decided to use our value function as the baseline. GRPO chooses something else: Here is what GRPO does: concretely, for each query, GRPO generates a group of responses (group of size G) and uses their rewards to calculate each response’s advantage as a z-score:
where rᵢ is the reward of the i-th response and μ and σ are the mean and standard deviation of rewards in that group.
This naturally eliminates the need for a separate value model. This idea makes a lot of sense when you think about it! It aligns with the value function we introduced before and also measures, in a sense, an “expected” reward we can obtain. Also, this new method is well adapted to our problem because LLMs can easily generate multiple non-deterministic outputs by using a low temperature (controls the randomness of tokens generation).
This is the main idea behind GRPO: getting rid of the value model.
Finally, GRPO adds a KL divergence term (to be exact, GRPO uses a simple approximation of the KL divergence to improve the algorithm further) directly into its objective, comparing the current policy to a reference policy (often the post-SFT model).
See the final formulation below:
And… that’s mostly it for GRPO! I hope this gives you a clear overview of the process: it still relies on the same foundational ideas as TRPO and PPO but introduces additional improvements to make training more efficient, faster, and cheaper — key factors behind DeepSeek’s success.

# Conclusion
Reinforcement Learning has become a cornerstone for training today’s Large Language Models, particularly through PPO, and more recently GRPO. Each method rests on the same RL fundamentals — states, actions, rewards, and policies — but adds its own twist to balance stability, efficiency, and human alignment:
• TRPO introduced strict policy constraints via KL divergence
• PPO eased those constraints with a clipped objective
• GRPO took an extra step by removing the value model requirement and using group-based reward normalization. Of course, DeepSeek also benefits from other innovations, like high-quality data and other training strategies, but that is for another time!
I hope this article gave you a clearer picture of how these methods connect and evolve. I believe that Reinforcement Learning will become the main focus in training LLMs to improve their performance, surpassing pre-training and SFT in driving future innovations.
If you’re interested in diving deeper, feel free to check out the references below or explore my previous posts.
Thanks for reading, and feel free to leave a clap and a comment!
Want to learn more about Transformers or dive into the math behind the Curse of Dimensionality? Check out my previous articles:

## Transformers: How Do They Transform Your Data?

### Diving into the Transformers architecture and what makes them unbeatable at language tasks
towardsdatascience.com

## The Math Behind “The Curse of Dimensionality”

### Dive into the “Curse of Dimensionality” concept and understand the math behind all the surprising phenomena that arise…
towardsdatascience.com
    * Feel free to connect on LinkedIn
    * Follow me on GitHub for more content
    * Visit my website: maximewolf.com
References:
    * [1] “Foundations of Large Language Models”, 2025. https://arxiv.org/pdf/2501.09223
    * [2] “Reinforcement Learning.” Enaris. Available at: https://enaris.org/material/en/Reinforcement%20Learning/index.html
    * [3] Y. Gokhale. “Introduction to LLMs and the Generative AI Part 5: RLHF,” Medium, 2023. Available at: https://medium.com/@yash9439/introduction-to-llms-and-the-generative-ai-part-5-rlhf-64e83fbcd795
    * [4] L. Weng. “An Overview of Reinforcement Learning,” 2018. Available at: https://lilianweng.github.io/posts/2018-02-19-rl-overview/
    * [5] “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”, 2025. https://arxiv.org/pdf/2501.12948
    * [6] “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”, 2025. https://arxiv.org/pdf/2402.03300
    * [7] “Trust Region Policy Optimization”, 2017. https://arxiv.org/pdf/1502.05477",Good
4,Training Large Language Models: From TRPO to GRPO,https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194,Maxime Wolf,349,177,1,Trust Region Policy Optimization,https://arxiv.org/pdf/1502.05477,0.5157593123209169,49.7,"# Training Large Language Models: From TRPO to GRPO

## Dive into how DeepSeek, ChatGPT, and other LLMs leverage reinforcement learning to enhance their training
Data Science Collective
DeepSeek has recently made quite a buzz in the AI community, thanks to its impressive performance at relatively low costs. I think this is a perfect opportunity to dive deeper into how Large Language Models (LLMs) are trained. In this article, we will focus on the Reinforcement Learning (RL) side of things: we will cover TRPO, PPO, and, more recently, GRPO (don't worry, I will explain all these terms soon!)
I have aimed to keep this article relatively easy to read and accessible, by minimizing the math, so you won’t need a deep Reinforcement Learning background to follow along. However, I will assume that you have some familiarity with Machine Learning, Deep Learning, and a basic understanding of how LLMs work.
I hope you enjoy the article, and feel free to leave a clap!

# The 3 steps of LLM training
Before diving into RL specifics, let’s briefly recap the three main stages of training a Large Language Model:
    * Pre-training: the model is trained on a massive dataset to predict the next token in a sequence based on preceding tokens.
    * Supervised Fine-Tuning (SFT): the model is then fine-tuned on more targeted data and aligned with specific instructions.
    * Reinforcement Learning (often called RLHF for Reinforcement Learning with Human Feedback): this is the focus of this article. The main goal is to further refine responses’ alignments with human preferences, by allowing the model to learn directly from feedback.

# Reinforcement Learning Basics
Before diving deeper, let’s briefly revisit the core ideas behind Reinforcement Learning.
RL is quite straightforward to understand at a high level: an agent interacts with an environment. The agent resides in a specific state within the environment and can take actions to transition to other states. Each action yields a reward from the environment: this is how the environment provides feedback that guides the agent’s future actions.
Consider the following example: a robot (the agent) navigates (and tries to exit) a maze (the environment).
    * The state is the current situation of the environment (the robot’s position in the maze).
    * The robot can take different actions: for example, it can move forward, turn left, or turn right.
    * Successfully navigating towards the exit yields a positive reward, while hitting a wall or getting stuck in the maze results in negative rewards.
Easy! Now, let’s now make an analogy to how RL is used in the context of LLMs.

# RL in the context of LLMs
When used during LLM training, RL is defined by the following components:
    * The LLM itself is the agent
    * Environment: everything external to the LLM, including user prompts, feedback systems, and other contextual information. This is basically the framework the LLM is interacting with during training.
    * Actions: these are responses to a query from the model. More specifically: these are the tokens that the LLM decides to generate in response to a query.
    * State: the current query being answered along with tokens the LLM has generated so far (i.e., the partial responses).
    * Rewards: this is a bit more tricky here: unlike the maze example above, there is usually no binary reward. In the context of LLMs, rewards usually come from a separate reward model, which outputs a score for each (query, response) pair. This model is trained from human-annotated data (hence “RLHF”) where annotators rank different responses. The goal is for higher-quality responses to receive higher rewards.
Note: in some cases, rewards can actually get simpler. For example, in DeepSeekMath, rule-based approaches can be used because math responses tend to be more deterministic (correct or wrong answer)
Policy is the final concept we need for now. In RL terms, a policy is simply the strategy for deciding which action to take. In the case of an LLM, the policy outputs a probability distribution over possible tokens at each step: in short, this is what the model uses to sample the next token to generate. Concretely, the policy is determined by the model’s parameters (weights). During RL training, we adjust these parameters so the LLM becomes more likely to produce “better” tokens— that is, tokens that produce higher reward scores.
We often write the policy as:
where a is the action (a token to generate), s the state (the query and tokens generated so far), and θ (model’s parameters).
This idea of finding the best policy is the whole point of RL! Since we don’t have labeled data (like we do in supervised learning) we use rewards to adjust our policy to take better actions. (In LLM terms: we adjust the parameters of our LLM to generate better tokens.)

# TRPO (Trust Region Policy Optimization)

## An analogy with supervised learning
Let’s take a quick step back to how supervised learning typically works. you have labeled data and use a loss function (like cross-entropy) to measure how close your model’s predictions are to the true labels.
We can then use algorithms like backpropagation and gradient descent to minimize our loss function and update the weights θ of our model.
Recall that our policy also outputs probabilities! In that sense, it is analogous to the model’s predictions in supervised learning… We are tempted to write something like:
where s is the current state and a is a possible action.
A(s, a) is called the advantage function and measures how good is the chosen action in the current state, compared to a baseline. This is very much like the notion of labels in supervised learning but derived from rewards instead of explicit labeling. To simplify, we can write the advantage as:
In practice, the baseline is calculated using a value function. This is a common term in RL that I will explain later. What you need to know for now is that it measures the expected reward we would receive if we continue following the current policy from the state s.

## What is TRPO?
TRPO (Trust Region Policy Optimization) builds on this idea of using the advantage function but adds a critical ingredient for stability: it constrains how far the new policy can deviate from the old policy at each update step (similar to what we do with batch gradient descent for example).
    * It introduces a KL divergence term (see it as a measure of similarity) between the current and the old policy:
    * It also divides the policy by the old policy. This ratio, multiplied by the advantage function, gives us a sense of how beneficial each update is relative to the old policy.
Putting it all together, TRPO tries to maximize a surrogate objective (which involves the advantage and the policy ratio) subject to a KL divergence constraint.

# PPO (Proximal Policy Optimization)
While TRPO was a significant advancement, it’s no longer used widely in practice, especially for training LLMs, due to its computationally intensive gradient calculations.
Instead, PPO is now the preferred approach in most LLMs architecture, including ChatGPT, Gemini, and more.
It is actually quite similar to TRPO, but instead of enforcing a hard constraint on the KL divergence, PPO introduces a “clipped surrogate objective” that implicitly restricts policy updates, and greatly simplifies the optimization process.
Here is a breakdown of the PPO objective function we maximize to tweak our model’s parameters.

# GRPO (Group Relative Policy Optimization)

## How is the value function usually obtained?
Let’s first talk more about the advantage and the value functions I introduced earlier.
In typical setups (like PPO), a value model is trained alongside the policy. Its goal is to predict the value of each action we take (each token generated by the model), using the rewards we obtain (remember that the value should represent the expected cumulative reward).
Here is how it works in practice. Take the query “What is 2+2?” as an example. Our model outputs “2+2 is 4” and receives a reward of 0.8 for that response. We then go backward and attribute discounted rewards to each prefix:
    * “2+2 is 4” gets a value of 0.8
    * “2+2 is” (1 token backward) gets a value of 0.8γ
    * “2+2” (2 tokens backward) gets a value of 0.8γ²
    * etc.
where γ is the discount factor (0.9 for example). We then use these prefixes and associated values to train the value model.
Important note: the value model and the reward model are two different things. The reward model is trained before the RL process and uses pairs of (query, response) and human ranking. The value model is trained concurrently to the policy, and aims at predicting the future expected reward at each step of the generation process.

## What’s new in GRPO
Even if in practice, the reward model is often derived from the policy (training only the “head”), we still end up maintaining many models and handling multiple training procedures (policy, reward, value model). GRPO streamlines this by introducing a more efficient method.
Remember what I said earlier?
In PPO, we decided to use our value function as the baseline. GRPO chooses something else: Here is what GRPO does: concretely, for each query, GRPO generates a group of responses (group of size G) and uses their rewards to calculate each response’s advantage as a z-score:
where rᵢ is the reward of the i-th response and μ and σ are the mean and standard deviation of rewards in that group.
This naturally eliminates the need for a separate value model. This idea makes a lot of sense when you think about it! It aligns with the value function we introduced before and also measures, in a sense, an “expected” reward we can obtain. Also, this new method is well adapted to our problem because LLMs can easily generate multiple non-deterministic outputs by using a low temperature (controls the randomness of tokens generation).
This is the main idea behind GRPO: getting rid of the value model.
Finally, GRPO adds a KL divergence term (to be exact, GRPO uses a simple approximation of the KL divergence to improve the algorithm further) directly into its objective, comparing the current policy to a reference policy (often the post-SFT model).
See the final formulation below:
And… that’s mostly it for GRPO! I hope this gives you a clear overview of the process: it still relies on the same foundational ideas as TRPO and PPO but introduces additional improvements to make training more efficient, faster, and cheaper — key factors behind DeepSeek’s success.

# Conclusion
Reinforcement Learning has become a cornerstone for training today’s Large Language Models, particularly through PPO, and more recently GRPO. Each method rests on the same RL fundamentals — states, actions, rewards, and policies — but adds its own twist to balance stability, efficiency, and human alignment:
• TRPO introduced strict policy constraints via KL divergence
• PPO eased those constraints with a clipped objective
• GRPO took an extra step by removing the value model requirement and using group-based reward normalization. Of course, DeepSeek also benefits from other innovations, like high-quality data and other training strategies, but that is for another time!
I hope this article gave you a clearer picture of how these methods connect and evolve. I believe that Reinforcement Learning will become the main focus in training LLMs to improve their performance, surpassing pre-training and SFT in driving future innovations.
If you’re interested in diving deeper, feel free to check out the references below or explore my previous posts.
Thanks for reading, and feel free to leave a clap and a comment!
Want to learn more about Transformers or dive into the math behind the Curse of Dimensionality? Check out my previous articles:

## Transformers: How Do They Transform Your Data?

### Diving into the Transformers architecture and what makes them unbeatable at language tasks
towardsdatascience.com

## The Math Behind “The Curse of Dimensionality”

### Dive into the “Curse of Dimensionality” concept and understand the math behind all the surprising phenomena that arise…
towardsdatascience.com
    * Feel free to connect on LinkedIn
    * Follow me on GitHub for more content
    * Visit my website: maximewolf.com
References:
    * [1] “Foundations of Large Language Models”, 2025. https://arxiv.org/pdf/2501.09223
    * [2] “Reinforcement Learning.” Enaris. Available at: https://enaris.org/material/en/Reinforcement%20Learning/index.html
    * [3] Y. Gokhale. “Introduction to LLMs and the Generative AI Part 5: RLHF,” Medium, 2023. Available at: https://medium.com/@yash9439/introduction-to-llms-and-the-generative-ai-part-5-rlhf-64e83fbcd795
    * [4] L. Weng. “An Overview of Reinforcement Learning,” 2018. Available at: https://lilianweng.github.io/posts/2018-02-19-rl-overview/
    * [5] “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”, 2025. https://arxiv.org/pdf/2501.12948
    * [6] “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”, 2025. https://arxiv.org/pdf/2402.03300
    * [7] “Trust Region Policy Optimization”, 2017. https://arxiv.org/pdf/1502.05477",Good
5,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Guiding Language Model Reasoning with Planning Tokens,https://arxiv.org/pdf/2310.05707,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
6,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Distilling System 2 into System 1,https://arxiv.org/pdf/2407.06023,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
7,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,https://arxiv.org/pdf/2201.11903,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
8,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,https://arxiv.org/pdf/2305.10601,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
9,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Branch-Solve-Merge Improves Large Language Model Evaluation and Generation,https://arxiv.org/pdf/2310.15123,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
10,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,System 2 Attention (is something you might need too),https://arxiv.org/pdf/2311.11829,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
11,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves,https://arxiv.org/pdf/2311.04205,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
12,What is the System 2 LLM or AI Chatbot?,https://medium.com/@don-lim/what-is-the-system-2-model-28d35deff287,Don Lim,413,17,0,Let's Verify Step by Step,https://arxiv.org/pdf/2305.20050,0.04116222760290557,4.0,"# What is the System 2 LLM or AI Chatbot?
Recently, OpenAI launched OpenAI o1-preview and OpenAI o1-mini which could be considered the first successful commercial launch of a System 2 LLM. The concept of System 1 and System 2 types of human’s thinking was proposed by Daniel Kahneman in 2011 in his book Thinking, Fast and Slow. To describe the concept in a sentence, “System 1 thinking is an immediate response from your intuition while System 2 thinking is a delayed response from a well-thought-out process”.
Many of us have been using System 1-type AI models since November 2022 when OpenAI launched ChatGPT. System 1 models generate responses directly from a given prompt. System 2-type models utilize a more deliberate and analytical process breaking down complex tasks into smaller steps. This ability is not exactly “true human-like reasoning”. Yet, it can emulate human-like reasoning that couldn’t be done so far. Therefore, OpenAI o1 probably deserves the title of the first “reasoning” AI model despite some exaggerated claims.
OpenAI’s documents mention that their new models are using reasoning tokens as a part of the Chain of Thought (CoT) approach. CoT will now become a buzzword for a while. They also mention Reinforcement Learning is being implemented for the so-called “reasoning/thinking process”.
It is not very clear what specific techniques they are using. One can only speculate what might be going on under the surface. However, it is possible to envision how one can design a System 2 LLM based on recent publications below.
Chain of Thought (https://arxiv.org/abs/2201.11903, 2022)
Tree of Thoughts (https://arxiv.org/abs/2305.10601, 2023)
Branch-Solve-Merge (https://arxiv.org/abs/2310.15123, 2023),
System 2 Attention (https://arxiv.org/abs/2311.11829, 2023)
Rephrase and Respond (https://arxiv.org/abs/2311.04205, 2023)
What these new techniques for System 2 LLMs are doing is planning a reasoning process and assessing different scenarios to solve a problem instead of responding directly to the prompt.
System 2 models, that can also be referred as “CoT-enabled LLMs”, would excel at never-seen-before tasks that require a heavy dose of reasoning. These models can use “reasoning tokens” to help plan startegies and generate temporary outputs normally not shown to the user. OpenAI decided to provide a glimpse into their System 2 process (which they call thinking) with friendly human languages, such as “First”, “Next”, “Let’s test this theory”, “Interesting”, “Wait”, “Hmm”, “That seems unlikely”, “Alternatively”, “So this works”, “Perfect”, etc.
Some of these English words are reasoning tokens represented as human languages for convenience. In the real application, they use system tokens which are a concept for the computer to distinguish them from regular human language tokens.

# What is the Reasoning Token?
Reasoning tokens are specialized system tokens designed to guide the system to perform step-by-step reasoning. They are created based on the user’s prompt and added to the reasoning process.
There are different types of reasoning tokens for different purposes. Some studies proposed Self-Reasoning Tokens while some studies proposed Planning Tokens. Reasoning tokens in the systems are often notated with single or double-angle brackets for illustrative purposes like below:
The above planning tokens at the start of each reasoning step serve as a guide to the model’s reasoning process. There are other types of system tokens that may serve as conceptual steps of the reasoning process, which may look like <Analyze_Problem>, <Generate_Hypothesis>, <Evaluate_Evidence>, and <Draw_Conclusion>. The system can also create custom reasoning tokens tailored to the specific prompt or task. This allows the system to focus on the most relevant aspects of the problem.
These system tokens will be deleted before the final response is shown to the user. However, these system tokens still get metered, even if you can’t verify the number of usages, and are included in the bill.

# Other Techniques

## Decision Gate
When the process reaches a certain level of satisfaction (utilizing the Decision Gate that determines whether sufficient reasoning has occurred), the system will remove all the technical elements and intermediate outputs to present a refined response at the end.

## Reinforcement Learning
Apart from reasoning tokens, OpenAI mentions Reinforcement Learning (RL) being used for the thinking process. Just like playing chess or evaluating options for a task, there are millions of scenarios and hypotheses that the thinking process can come up with. RL can help the system focus on the most promising paths, rather than exploring all possible scenarios. RL can also allow the system to learn from its mistakes and improve its performance over time. One of RL’s strengths is solving tasks that involve uncertainty, partial observability, and long-term planning. This kind of ability can be also useful in the “reasoning” process of System 2 models.
According to OpenAI’s publication in May 2023 called “Let’s verify Step by Step”, they reported using a Process Supervision Model with Human Feedback significantly improved performance in math. The Process Supervision Model grades or evaluates each step of the thinking process rather than grading the final outcome as is the case of the Outcome Supervision Model. These two grading models are different types of Reinforcement Learning. It’s possible that OpenAI might have implemented approaches mentioned in the publication in their o1 models. In other words, human feedback was likely used to teach the system how to develop each step in a Chain of Thought.

# System 2 Attention
If you are a System 2 LLM developer, you might also want to use System 2 Attention where the system pays more attention to important steps of the reasoning process and adjusts the weight given to different reasoning paths. This is similar to Attention Mechanism for traditional System 1 LLMs where the system pays attention to important tokens.

# Examples of Reasoning Tokens
For convenience, I have asked Perplexity AI to give me an example of reasoning tokens. OpenAI’s models seem to be restricted to answering questions regarding reasoning tokens.
Here is an example of reasoning tokens that could be used for complex tasks like deciphering corrupted texts:
    * <analyze_script>: This token could prompt the model to analyze the script and writing system used in the corrupted text, identifying any unusual characters or variations.
    * <historical_context>: This token could trigger the model to consider the historical context and time period of the text to inform the deciphering process.
    * <identify_patterns>: This token could direct the model to look for recurring patterns or structures in the corrupted text that may provide clues.
    * <compare_known_texts>: This token could instruct the model to compare the corrupted text to known intact texts from the same period or genre.
    * <linguistic_analysis>: This token could prompt the model to perform a linguistic analysis of the text, considering grammar, syntax, and vocabulary.
    * <reconstruct_missing>: This token could direct the model to attempt to reconstruct missing or illegible portions based on context and linguistic knowledge.
    * <cross_reference>: This token could trigger the model to cross-reference the text with other historical or linguistic sources for additional insights.
    * <evaluate_hypotheses>: This token could prompt the model to generate and evaluate multiple hypotheses about the text’s meaning or content.
    * <cultural_context>: This token could direct the model to consider relevant cultural context that may inform the text’s interpretation.
    * <error_analysis>: This token could instruct the model to analyze potential errors or corruptions in the text and propose corrections.
These reasoning tokens could help guide the model through a structured, step-by-step approach to deciphering complex corrupted texts.
Here’s another example of reasoning tokens for a complex situation created by Gemini.
Prompt: I have a wifi AP that can reach 100 m without obstructions. How would you install wifi access points in a stadium with a shape like the attached image (contains walls, roofs, etc.) where there are no people at the time of installation but needs to simulate a full house?
Reasoning token example:
Understanding the Problem
    * <Analyze_Environment>: Understand the physical layout of the stadium (size, shape, obstacles, etc.).
    * <Identify_Coverage_Needs>: Determine the specific areas within the stadium that need Wi-Fi coverage (e.g., seating areas, concessions, bathrooms).
    * <Assess_Interference>: Consider potential interference sources within the stadium (e.g., metal structures, concrete walls, electrical equipment).
Planning the Installation
    * <Determine_AP_Locations>: Identify optimal locations for Wi-Fi access points based on the stadium’s layout and coverage needs.
    * <Consider_Mounting_Options>: Evaluate different mounting options for the APs (e.g., walls, ceilings, poles).
    * <Plan_Wiring>: Outline the necessary wiring infrastructure to connect the APs to the network.
Simulating a Full House
    * <Generate_Traffic_Patterns>: Create a hypothetical traffic pattern that simulates a full stadium (e.g., concentration of devices in certain areas, varying connection strengths).
    * <Use_Traffic_Generation_Tools>: Employ tools or software to generate simulated network traffic that mimics a full stadium crowd.
    * <Monitor_Performance>: Observe the network’s performance under simulated conditions to identify any bottlenecks or areas for improvement.
Drawing Conclusions
    * <Evaluate_Installation_Success>: Assess whether the installed APs provide adequate coverage and performance under simulated full-house conditions.
    * <Identify_Areas_for_Improvement>: Determine if any adjustments or additional APs are needed to optimize coverage and performance.
    * <Provide_Recommendations>: Offer recommendations for future network maintenance or upgrades based on the simulation results.
For a human expert, this task may take more than a month and still would have to go through verifications and modifications. If System 2 AI can do this almost perfectly in a week, this would change the science and engineering world significantly. However, if all CoT steps for all different use cases in the world have to be graded or taught by humans, we may not see a nearly perfect performance of AI for this kind of complex task for a long time.
Like, subscribe, or share if you like to support content like this!
References:
Guiding Language Model Reasoning with Planning Tokens
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex…
arxiv.org
Distilling System 2 into System 1
Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to…
arxiv.org
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the…
arxiv.org
Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still…
arxiv.org
Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that…
arxiv.org
System 2 Attention (is something you might need too)
Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information…
arxiv.org
Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models…
arxiv.org
Let’s Verify Step by Step
In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning…
arxiv.org",Bad
13,Reinforcement Learning for tuning language models ( how to train ChatGPT ),https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302,ML Blogger,649,93,1,Fine-Tuning Language Models from Human Preferences,https://arxiv.org/pdf/1909.08593,0.14791987673343607,14.3,"# Reinforcement Learning for tuning language models ( how to train ChatGPT )

# Large Language Models
The Large Language Model revolution started with the advent of transformers in 2017. Since then there has been an exponential growth in the models trained. Models with 100B+ parameters have been trained. These pre-trained models have changed the way NLP is done. It is much easier to pick a pre-trained model and fine-tune it for a downstream task ( sentiment, question answering, entity recognition etc.. ) than training a model from scratch. Fine-tuning can be done with a much smaller set of examples than training a model from scratch making the whole process of NLP much easier.
LLMs with billions of params ( > 10B+ ) also display emergent abilities i.e the model performance shows a significant jump ( on various tasks ) once a certain number of parameters are crossed. These large models also display amazing few-shot learning capabilities & prompt enabled tuning.

# Using LLMs
As talked about earlier LLMs can be used in various ways i.e
    * Prompting : In the prompting paradigm, a pre-trained LLM is provided a snippet of text as an input and is expected to provide a relevant completion of this input. In prompt engineering, the description of the task is embedded in the input, e.g., as a question instead of it being implicitly given
    * Fine-tuning : Fine-tuning is a way of applying or utilising transfer learning. Specifically, fine-tuning is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task.
The recent success of ChatGPT has shown us how fine-tuning can improve the models performance by leaps and bounds. In this article we will look the method used by ChatGPT called RLHF ( Reinforcement Learning from Human Feedback ).

# Using RL with human feedback for fine-tuning
For fine-tuning LLMs using RL we need to frame the problem into a Agent-Environment setting where the agent ( policy ) can interact with the environment to get the reward for its actions. This reward is then used as feedback to train the model.
The mapping of the entities is as follows
    * Agent ( Policy ) : LLM ( Large Language Model )
    * Environment : In this case the environment is the reward function ( model ) which generates rewards. The reward function consumes the input as well as the output of the LLM to generate the reward
The reward is used in a loss function and the policy is updated.

## Policy
This is the pre-trained LLM which is being fine-tuned.

## Reward Model
Before the reward model is trained data is collected from human labelers. For each input x several yᵢ are generated by sampling from the LLM. The humans are then asked to rank these yᵢ giving the highest rank to the best response. Using this as the label the reward model is trained to maximise the probability of the correct response using a loss of the type
The reward model can also be a ranking type loss function where the model tries to maximise the order of the ranking of the outputs rather than maximise the probability of one the outputs.

## Loss Function
The loss function for training the policy is
where r(x,y) is the reward model output and the second term is the KL divergence to ensure the policy π doesn’t deviate too far from the ρ language model while fine-tuning.
For optimising the loss function PPO ( Proximal Policy Optimisation ) algorithm is used. The reason to use PPO is because the optimization is ‘on policy’ i.e we are directly optimising the policy. PPO is similar to TRPO and offers more stable updates to the policy than other optimisers for policy gradient methods.
While training only the policy is updated and the reward model is not optimised. The reason is that the reward model is only a ‘proxy’ for the human preferences which is trained using a handful of examples and optimising it along with the policy causes overfitting.

# Overview of the training process
The training process is as follows
    * Gather samples ( x, y₀, y₁, y₂, y₃ ) via x ~ D, yᵢ ~ ρ(.|x) . Humans pick the best yᵢ from for each x.
    * Reward model is initialised to ρ and trained on the human annotated samples using the loss in Eq 1.
    * Train π using PPO ( Proximal Policy Optimisation ) with loss as in Eq 2.

# Open Source Libraries

## TRL
TRL is library for training language models with RL. With trl you can train transformer language models with Proximal Policy Optimisation (PPO). The library is built on top of the transformer library by 🤗 Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point only decoder architectures such as GTP2 are implemented.
    * PPO optimiser is used for training
    * Currently supports GPT2 model for training

## RL4LM
RL4LM is a library for training language models using RL by Allen Institute for AI. Their approach and enhancements are presented in this paper here. The following are the highlights of their library
    * Multiple NLP tasks supported like summarisation, sentiment, translation etc..
    * Supports multiple metrics like BLEU, ROUGE, BERTSCORE etc..
    * On Policy algorithms supported like PPO, TRPO, A2C, NLPO ( RL4LM’s novel approach )
    * Actor-Critic Policies supporting causal LMs

# Other papers using similar approaches

## CodeRL
CodeRL is a research model by Salesforce Research. Paper can be found here. Its an enhancement to their earlier code generation model CodeT5. The basic idea is most of the code generation models are open loop i.e they are not trained to produce logically correct or executable code as the model doesn’t have any feedback on the code correctness. CodeRL tries to solve this issue by fine-tuning the pre-trained CodeT5 models using RL to improve the code generation quality.
The key ideas here are
    * Using the code model as the Policy network
    * create a Critic network to predict the correctness of the generated program ( this works even for incomplete programs )
    * use Unit tests to check the correctness of the final generated program

## RLPrompt
RLPrompt takes a different approach to the fine-tuning. Paper can be found here. Instead of fine-tuning the model the approach fine-tunes the prompts given to the model. As we know search for prompts is a trial and error approach and the kind of prompts used impacts the model performance.
They key ideas of this approach are
    * Use of a Policy for prompting ( This is also a language model but a smaller one ). A tunable layer is appended to the Policy network to enable fine-tuning
    * The generated prompt from the Policy is fed into the generation language model ( this can be a masked LM or decoder type LM )
    * Reward is constructed using the output of the generation language model

# References
    * https://openai.com/blog/learning-to-summarize-with-human-feedback/
    * https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/
    * https://openai.com/blog/chatgpt/
    * https://github.com/openai/summarize-from-feedback
    * Fine-Tuning Language Models from Human Preferences https://arxiv.org/abs/1909.08593
    * https://blog.salesforceairesearch.com/coderl/
    * RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning https://arxiv.org/abs/2205.12548
    * On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting https://www.semanticscholar.org/paper/On-Reinforcement-Learning-and-Distribution-Matching-Korbak-ElSahar/1e34c51b52002796fea6f523b9f794f1d75d9ba8
    * https://lvwerra.github.io/trl/
    * Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback https://arxiv.org/abs/2204.05862
    * WebGPT: Browser-assisted question-answering with human feedback https://arxiv.org/abs/2112.09332",Bad
14,Reinforcement Learning for tuning language models ( how to train ChatGPT ),https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302,ML Blogger,649,93,1,RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning,https://arxiv.org/pdf/2205.12548,0.14791987673343607,14.3,"# Reinforcement Learning for tuning language models ( how to train ChatGPT )

# Large Language Models
The Large Language Model revolution started with the advent of transformers in 2017. Since then there has been an exponential growth in the models trained. Models with 100B+ parameters have been trained. These pre-trained models have changed the way NLP is done. It is much easier to pick a pre-trained model and fine-tune it for a downstream task ( sentiment, question answering, entity recognition etc.. ) than training a model from scratch. Fine-tuning can be done with a much smaller set of examples than training a model from scratch making the whole process of NLP much easier.
LLMs with billions of params ( > 10B+ ) also display emergent abilities i.e the model performance shows a significant jump ( on various tasks ) once a certain number of parameters are crossed. These large models also display amazing few-shot learning capabilities & prompt enabled tuning.

# Using LLMs
As talked about earlier LLMs can be used in various ways i.e
    * Prompting : In the prompting paradigm, a pre-trained LLM is provided a snippet of text as an input and is expected to provide a relevant completion of this input. In prompt engineering, the description of the task is embedded in the input, e.g., as a question instead of it being implicitly given
    * Fine-tuning : Fine-tuning is a way of applying or utilising transfer learning. Specifically, fine-tuning is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task.
The recent success of ChatGPT has shown us how fine-tuning can improve the models performance by leaps and bounds. In this article we will look the method used by ChatGPT called RLHF ( Reinforcement Learning from Human Feedback ).

# Using RL with human feedback for fine-tuning
For fine-tuning LLMs using RL we need to frame the problem into a Agent-Environment setting where the agent ( policy ) can interact with the environment to get the reward for its actions. This reward is then used as feedback to train the model.
The mapping of the entities is as follows
    * Agent ( Policy ) : LLM ( Large Language Model )
    * Environment : In this case the environment is the reward function ( model ) which generates rewards. The reward function consumes the input as well as the output of the LLM to generate the reward
The reward is used in a loss function and the policy is updated.

## Policy
This is the pre-trained LLM which is being fine-tuned.

## Reward Model
Before the reward model is trained data is collected from human labelers. For each input x several yᵢ are generated by sampling from the LLM. The humans are then asked to rank these yᵢ giving the highest rank to the best response. Using this as the label the reward model is trained to maximise the probability of the correct response using a loss of the type
The reward model can also be a ranking type loss function where the model tries to maximise the order of the ranking of the outputs rather than maximise the probability of one the outputs.

## Loss Function
The loss function for training the policy is
where r(x,y) is the reward model output and the second term is the KL divergence to ensure the policy π doesn’t deviate too far from the ρ language model while fine-tuning.
For optimising the loss function PPO ( Proximal Policy Optimisation ) algorithm is used. The reason to use PPO is because the optimization is ‘on policy’ i.e we are directly optimising the policy. PPO is similar to TRPO and offers more stable updates to the policy than other optimisers for policy gradient methods.
While training only the policy is updated and the reward model is not optimised. The reason is that the reward model is only a ‘proxy’ for the human preferences which is trained using a handful of examples and optimising it along with the policy causes overfitting.

# Overview of the training process
The training process is as follows
    * Gather samples ( x, y₀, y₁, y₂, y₃ ) via x ~ D, yᵢ ~ ρ(.|x) . Humans pick the best yᵢ from for each x.
    * Reward model is initialised to ρ and trained on the human annotated samples using the loss in Eq 1.
    * Train π using PPO ( Proximal Policy Optimisation ) with loss as in Eq 2.

# Open Source Libraries

## TRL
TRL is library for training language models with RL. With trl you can train transformer language models with Proximal Policy Optimisation (PPO). The library is built on top of the transformer library by 🤗 Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point only decoder architectures such as GTP2 are implemented.
    * PPO optimiser is used for training
    * Currently supports GPT2 model for training

## RL4LM
RL4LM is a library for training language models using RL by Allen Institute for AI. Their approach and enhancements are presented in this paper here. The following are the highlights of their library
    * Multiple NLP tasks supported like summarisation, sentiment, translation etc..
    * Supports multiple metrics like BLEU, ROUGE, BERTSCORE etc..
    * On Policy algorithms supported like PPO, TRPO, A2C, NLPO ( RL4LM’s novel approach )
    * Actor-Critic Policies supporting causal LMs

# Other papers using similar approaches

## CodeRL
CodeRL is a research model by Salesforce Research. Paper can be found here. Its an enhancement to their earlier code generation model CodeT5. The basic idea is most of the code generation models are open loop i.e they are not trained to produce logically correct or executable code as the model doesn’t have any feedback on the code correctness. CodeRL tries to solve this issue by fine-tuning the pre-trained CodeT5 models using RL to improve the code generation quality.
The key ideas here are
    * Using the code model as the Policy network
    * create a Critic network to predict the correctness of the generated program ( this works even for incomplete programs )
    * use Unit tests to check the correctness of the final generated program

## RLPrompt
RLPrompt takes a different approach to the fine-tuning. Paper can be found here. Instead of fine-tuning the model the approach fine-tunes the prompts given to the model. As we know search for prompts is a trial and error approach and the kind of prompts used impacts the model performance.
They key ideas of this approach are
    * Use of a Policy for prompting ( This is also a language model but a smaller one ). A tunable layer is appended to the Policy network to enable fine-tuning
    * The generated prompt from the Policy is fed into the generation language model ( this can be a masked LM or decoder type LM )
    * Reward is constructed using the output of the generation language model

# References
    * https://openai.com/blog/learning-to-summarize-with-human-feedback/
    * https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/
    * https://openai.com/blog/chatgpt/
    * https://github.com/openai/summarize-from-feedback
    * Fine-Tuning Language Models from Human Preferences https://arxiv.org/abs/1909.08593
    * https://blog.salesforceairesearch.com/coderl/
    * RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning https://arxiv.org/abs/2205.12548
    * On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting https://www.semanticscholar.org/paper/On-Reinforcement-Learning-and-Distribution-Matching-Korbak-ElSahar/1e34c51b52002796fea6f523b9f794f1d75d9ba8
    * https://lvwerra.github.io/trl/
    * Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback https://arxiv.org/abs/2204.05862
    * WebGPT: Browser-assisted question-answering with human feedback https://arxiv.org/abs/2112.09332",Bad
15,Reinforcement Learning for tuning language models ( how to train ChatGPT ),https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302,ML Blogger,649,93,1,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,https://arxiv.org/pdf/2204.05862,0.14791987673343607,14.3,"# Reinforcement Learning for tuning language models ( how to train ChatGPT )

# Large Language Models
The Large Language Model revolution started with the advent of transformers in 2017. Since then there has been an exponential growth in the models trained. Models with 100B+ parameters have been trained. These pre-trained models have changed the way NLP is done. It is much easier to pick a pre-trained model and fine-tune it for a downstream task ( sentiment, question answering, entity recognition etc.. ) than training a model from scratch. Fine-tuning can be done with a much smaller set of examples than training a model from scratch making the whole process of NLP much easier.
LLMs with billions of params ( > 10B+ ) also display emergent abilities i.e the model performance shows a significant jump ( on various tasks ) once a certain number of parameters are crossed. These large models also display amazing few-shot learning capabilities & prompt enabled tuning.

# Using LLMs
As talked about earlier LLMs can be used in various ways i.e
    * Prompting : In the prompting paradigm, a pre-trained LLM is provided a snippet of text as an input and is expected to provide a relevant completion of this input. In prompt engineering, the description of the task is embedded in the input, e.g., as a question instead of it being implicitly given
    * Fine-tuning : Fine-tuning is a way of applying or utilising transfer learning. Specifically, fine-tuning is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task.
The recent success of ChatGPT has shown us how fine-tuning can improve the models performance by leaps and bounds. In this article we will look the method used by ChatGPT called RLHF ( Reinforcement Learning from Human Feedback ).

# Using RL with human feedback for fine-tuning
For fine-tuning LLMs using RL we need to frame the problem into a Agent-Environment setting where the agent ( policy ) can interact with the environment to get the reward for its actions. This reward is then used as feedback to train the model.
The mapping of the entities is as follows
    * Agent ( Policy ) : LLM ( Large Language Model )
    * Environment : In this case the environment is the reward function ( model ) which generates rewards. The reward function consumes the input as well as the output of the LLM to generate the reward
The reward is used in a loss function and the policy is updated.

## Policy
This is the pre-trained LLM which is being fine-tuned.

## Reward Model
Before the reward model is trained data is collected from human labelers. For each input x several yᵢ are generated by sampling from the LLM. The humans are then asked to rank these yᵢ giving the highest rank to the best response. Using this as the label the reward model is trained to maximise the probability of the correct response using a loss of the type
The reward model can also be a ranking type loss function where the model tries to maximise the order of the ranking of the outputs rather than maximise the probability of one the outputs.

## Loss Function
The loss function for training the policy is
where r(x,y) is the reward model output and the second term is the KL divergence to ensure the policy π doesn’t deviate too far from the ρ language model while fine-tuning.
For optimising the loss function PPO ( Proximal Policy Optimisation ) algorithm is used. The reason to use PPO is because the optimization is ‘on policy’ i.e we are directly optimising the policy. PPO is similar to TRPO and offers more stable updates to the policy than other optimisers for policy gradient methods.
While training only the policy is updated and the reward model is not optimised. The reason is that the reward model is only a ‘proxy’ for the human preferences which is trained using a handful of examples and optimising it along with the policy causes overfitting.

# Overview of the training process
The training process is as follows
    * Gather samples ( x, y₀, y₁, y₂, y₃ ) via x ~ D, yᵢ ~ ρ(.|x) . Humans pick the best yᵢ from for each x.
    * Reward model is initialised to ρ and trained on the human annotated samples using the loss in Eq 1.
    * Train π using PPO ( Proximal Policy Optimisation ) with loss as in Eq 2.

# Open Source Libraries

## TRL
TRL is library for training language models with RL. With trl you can train transformer language models with Proximal Policy Optimisation (PPO). The library is built on top of the transformer library by 🤗 Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point only decoder architectures such as GTP2 are implemented.
    * PPO optimiser is used for training
    * Currently supports GPT2 model for training

## RL4LM
RL4LM is a library for training language models using RL by Allen Institute for AI. Their approach and enhancements are presented in this paper here. The following are the highlights of their library
    * Multiple NLP tasks supported like summarisation, sentiment, translation etc..
    * Supports multiple metrics like BLEU, ROUGE, BERTSCORE etc..
    * On Policy algorithms supported like PPO, TRPO, A2C, NLPO ( RL4LM’s novel approach )
    * Actor-Critic Policies supporting causal LMs

# Other papers using similar approaches

## CodeRL
CodeRL is a research model by Salesforce Research. Paper can be found here. Its an enhancement to their earlier code generation model CodeT5. The basic idea is most of the code generation models are open loop i.e they are not trained to produce logically correct or executable code as the model doesn’t have any feedback on the code correctness. CodeRL tries to solve this issue by fine-tuning the pre-trained CodeT5 models using RL to improve the code generation quality.
The key ideas here are
    * Using the code model as the Policy network
    * create a Critic network to predict the correctness of the generated program ( this works even for incomplete programs )
    * use Unit tests to check the correctness of the final generated program

## RLPrompt
RLPrompt takes a different approach to the fine-tuning. Paper can be found here. Instead of fine-tuning the model the approach fine-tunes the prompts given to the model. As we know search for prompts is a trial and error approach and the kind of prompts used impacts the model performance.
They key ideas of this approach are
    * Use of a Policy for prompting ( This is also a language model but a smaller one ). A tunable layer is appended to the Policy network to enable fine-tuning
    * The generated prompt from the Policy is fed into the generation language model ( this can be a masked LM or decoder type LM )
    * Reward is constructed using the output of the generation language model

# References
    * https://openai.com/blog/learning-to-summarize-with-human-feedback/
    * https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/
    * https://openai.com/blog/chatgpt/
    * https://github.com/openai/summarize-from-feedback
    * Fine-Tuning Language Models from Human Preferences https://arxiv.org/abs/1909.08593
    * https://blog.salesforceairesearch.com/coderl/
    * RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning https://arxiv.org/abs/2205.12548
    * On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting https://www.semanticscholar.org/paper/On-Reinforcement-Learning-and-Distribution-Matching-Korbak-ElSahar/1e34c51b52002796fea6f523b9f794f1d75d9ba8
    * https://lvwerra.github.io/trl/
    * Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback https://arxiv.org/abs/2204.05862
    * WebGPT: Browser-assisted question-answering with human feedback https://arxiv.org/abs/2112.09332",Bad
16,Reinforcement Learning for tuning language models ( how to train ChatGPT ),https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302,ML Blogger,649,93,1,WebGPT: Browser-assisted question-answering with human feedback,https://arxiv.org/pdf/2112.09332,0.14791987673343607,14.3,"# Reinforcement Learning for tuning language models ( how to train ChatGPT )

# Large Language Models
The Large Language Model revolution started with the advent of transformers in 2017. Since then there has been an exponential growth in the models trained. Models with 100B+ parameters have been trained. These pre-trained models have changed the way NLP is done. It is much easier to pick a pre-trained model and fine-tune it for a downstream task ( sentiment, question answering, entity recognition etc.. ) than training a model from scratch. Fine-tuning can be done with a much smaller set of examples than training a model from scratch making the whole process of NLP much easier.
LLMs with billions of params ( > 10B+ ) also display emergent abilities i.e the model performance shows a significant jump ( on various tasks ) once a certain number of parameters are crossed. These large models also display amazing few-shot learning capabilities & prompt enabled tuning.

# Using LLMs
As talked about earlier LLMs can be used in various ways i.e
    * Prompting : In the prompting paradigm, a pre-trained LLM is provided a snippet of text as an input and is expected to provide a relevant completion of this input. In prompt engineering, the description of the task is embedded in the input, e.g., as a question instead of it being implicitly given
    * Fine-tuning : Fine-tuning is a way of applying or utilising transfer learning. Specifically, fine-tuning is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task.
The recent success of ChatGPT has shown us how fine-tuning can improve the models performance by leaps and bounds. In this article we will look the method used by ChatGPT called RLHF ( Reinforcement Learning from Human Feedback ).

# Using RL with human feedback for fine-tuning
For fine-tuning LLMs using RL we need to frame the problem into a Agent-Environment setting where the agent ( policy ) can interact with the environment to get the reward for its actions. This reward is then used as feedback to train the model.
The mapping of the entities is as follows
    * Agent ( Policy ) : LLM ( Large Language Model )
    * Environment : In this case the environment is the reward function ( model ) which generates rewards. The reward function consumes the input as well as the output of the LLM to generate the reward
The reward is used in a loss function and the policy is updated.

## Policy
This is the pre-trained LLM which is being fine-tuned.

## Reward Model
Before the reward model is trained data is collected from human labelers. For each input x several yᵢ are generated by sampling from the LLM. The humans are then asked to rank these yᵢ giving the highest rank to the best response. Using this as the label the reward model is trained to maximise the probability of the correct response using a loss of the type
The reward model can also be a ranking type loss function where the model tries to maximise the order of the ranking of the outputs rather than maximise the probability of one the outputs.

## Loss Function
The loss function for training the policy is
where r(x,y) is the reward model output and the second term is the KL divergence to ensure the policy π doesn’t deviate too far from the ρ language model while fine-tuning.
For optimising the loss function PPO ( Proximal Policy Optimisation ) algorithm is used. The reason to use PPO is because the optimization is ‘on policy’ i.e we are directly optimising the policy. PPO is similar to TRPO and offers more stable updates to the policy than other optimisers for policy gradient methods.
While training only the policy is updated and the reward model is not optimised. The reason is that the reward model is only a ‘proxy’ for the human preferences which is trained using a handful of examples and optimising it along with the policy causes overfitting.

# Overview of the training process
The training process is as follows
    * Gather samples ( x, y₀, y₁, y₂, y₃ ) via x ~ D, yᵢ ~ ρ(.|x) . Humans pick the best yᵢ from for each x.
    * Reward model is initialised to ρ and trained on the human annotated samples using the loss in Eq 1.
    * Train π using PPO ( Proximal Policy Optimisation ) with loss as in Eq 2.

# Open Source Libraries

## TRL
TRL is library for training language models with RL. With trl you can train transformer language models with Proximal Policy Optimisation (PPO). The library is built on top of the transformer library by 🤗 Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point only decoder architectures such as GTP2 are implemented.
    * PPO optimiser is used for training
    * Currently supports GPT2 model for training

## RL4LM
RL4LM is a library for training language models using RL by Allen Institute for AI. Their approach and enhancements are presented in this paper here. The following are the highlights of their library
    * Multiple NLP tasks supported like summarisation, sentiment, translation etc..
    * Supports multiple metrics like BLEU, ROUGE, BERTSCORE etc..
    * On Policy algorithms supported like PPO, TRPO, A2C, NLPO ( RL4LM’s novel approach )
    * Actor-Critic Policies supporting causal LMs

# Other papers using similar approaches

## CodeRL
CodeRL is a research model by Salesforce Research. Paper can be found here. Its an enhancement to their earlier code generation model CodeT5. The basic idea is most of the code generation models are open loop i.e they are not trained to produce logically correct or executable code as the model doesn’t have any feedback on the code correctness. CodeRL tries to solve this issue by fine-tuning the pre-trained CodeT5 models using RL to improve the code generation quality.
The key ideas here are
    * Using the code model as the Policy network
    * create a Critic network to predict the correctness of the generated program ( this works even for incomplete programs )
    * use Unit tests to check the correctness of the final generated program

## RLPrompt
RLPrompt takes a different approach to the fine-tuning. Paper can be found here. Instead of fine-tuning the model the approach fine-tunes the prompts given to the model. As we know search for prompts is a trial and error approach and the kind of prompts used impacts the model performance.
They key ideas of this approach are
    * Use of a Policy for prompting ( This is also a language model but a smaller one ). A tunable layer is appended to the Policy network to enable fine-tuning
    * The generated prompt from the Policy is fed into the generation language model ( this can be a masked LM or decoder type LM )
    * Reward is constructed using the output of the generation language model

# References
    * https://openai.com/blog/learning-to-summarize-with-human-feedback/
    * https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/
    * https://openai.com/blog/chatgpt/
    * https://github.com/openai/summarize-from-feedback
    * Fine-Tuning Language Models from Human Preferences https://arxiv.org/abs/1909.08593
    * https://blog.salesforceairesearch.com/coderl/
    * RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning https://arxiv.org/abs/2205.12548
    * On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting https://www.semanticscholar.org/paper/On-Reinforcement-Learning-and-Distribution-Matching-Korbak-ElSahar/1e34c51b52002796fea6f523b9f794f1d75d9ba8
    * https://lvwerra.github.io/trl/
    * Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback https://arxiv.org/abs/2204.05862
    * WebGPT: Browser-assisted question-answering with human feedback https://arxiv.org/abs/2112.09332",Bad
17,Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review,https://medium.com/@sulbha.jindal/self-generated-critiques-boost-reward-modeling-for-languagemodels-paper-review-f013b99b7d47,Sulbha Jain,41,21,0,Self-Generated Critiques Boost Reward Modeling for Language Models,https://arxiv.org/pdf/2411.16646,0.5121951219512195,49.4,"# Self-Generated Critiques Boost Reward Modeling for LanguageModels — Paper Review
Paper — https://arxiv.org/pdf/2411.16646
Reinforcement Learning from Human Feedback (RLHF) has become a critical methodology for aligning large language models (LLMs) with human preferences. At the core of RLHF lies the reward model (RM), which is designed to evaluate model outputs by assigning scores that reflect their alignment with human judgments. These scores guide the optimization process during training, such as providing reward signals in Proximal Policy Optimization (PPO), thereby encouraging LLMs to generate responses that are more helpful, honest, and harmless. This iterative process enhances the practical quality of LLM outputs in real-world applications.

## Current challenge
Typically, reward models are trained using preference pairs and optimized through pairwise logistic loss to produce a scalar score for each response. However, this scalar output is often hard to interpret and underutilizes the inherent language modeling capabilities of LLMs derived from pretraining and post-training. These limitations can weaken the feedback signals in RLHF, resulting in suboptimal policy updates. An alternative approach is the “LLM-as-a-judge” paradigm, where the LLM generates critiques and optionally provides discrete scores as proxies for response quality. This method leverages the model’s reasoning abilities more effectively, potentially addressing some of the shortcomings of traditional reward models
Incorporating critiques into reward modeling poses two significant challenges. First, there is the issue of conflicting objectives: generating critiques relies on language modeling capabilities, whereas traditional reward models output scalar values, making their integration into the language modeling process complex. Second, there are limitations with evaluators; off-the-shelf language models often lack the effectiveness needed for evaluation, and fine-tuning these models necessitates costly human-generated or annotated critiques. While knowledge distillation offers a potential solution by enabling a joint training approach to learn how to generate critiques and rewards simultaneously, it falls short when it comes to enhancing frontier models in situations where a stronger teacher model is unavailable. Here is Critic-RM, a new framework from Meta Researchers that enhances reward models using synthetic critiques, without relying on strong LLM teachers.

## Methodology
Critic-RM utilizes an instruction-finetuned large language model (LLM) as its foundation, generating multiple candidate critiques, each accompanied by a discrete score for individual responses. The process begins with a consistency-guided filtering technique that retains only those critiques whose scores align with human-annotated preference labels.
To further improve the quality of these synthetic critiques, two additional strategies — summarization and ranking — are proposed to refine the critiques used in training the reward model. The framework investigates the application of an off-the-shelf instruction-finetuned LLM, for both critique generation and reward modeling. Initially, Critic-RM generates candidate critiques for each prompt-response pair, followed by a filtering step aimed at minimizing the influence of potentially noisy rationales that could lead to incorrect predictions. This approach allows for the augmentation of preference pairs with additional critiques, ultimately enhancing the precision of reward modeling.
After generating critiques for each response, the primary challenge is developing an effective training strategy to integrate critique modeling and scalar reward prediction objectives. To address this, we propose a simple weighting strategy that balances these objectives. Initially, the model prioritizes critique modeling loss, then gradually shifts its focus toward reward prediction, utilizing both the response and the critique. This balanced approach enables the model to excel in generating high-quality critiques while maintaining accurate reward predictions.
In Critic-RM, an additional step is introduced during inference for each (prompt, response) pair. Given a (prompt, response) pair $$(x, y)$$, the model first generates a critique $$z \sim q_\phi(x, y)$$. It then predicts the reward for the response as $$r = r_\psi(x, [y, z])$$, where the reward prediction incorporates both the response and its associated critique. This process ensures a more nuanced and precise evaluation of responses.

## Results
Incorporating critiques into reward modeling has demonstrated significant benefits, particularly with the Critic-RM framework, which consistently outperforms the baselines in this study. Specifically, when trained on the same preference data, Critic-RM achieves an improvement of 3.7% to 4.7% over standard reward models. The quality of critiques plays a crucial role; comparisons reveal that other models incorporating critiques show smaller performance gains than Critic-RM when evaluated against the standard reward model. Additionally, performance enhancements are observed for both Critic-RM and its baselines when multiple critiques are generated during inference, particularly benefiting reasoning tasks.

## Summary
Researchers have developed Critic-RM, an innovative self-critiquing framework aimed at enhancing reward modeling for large language models (LLMs). This novel approach leverages the inherent capabilities of LLMs to generate and refine critiques, implementing a self-improvement mechanism that enhances both the quality of critiques and the accuracy of reward predictions. The findings from this research emphasize the efficacy of Critic-RM in improving reward modeling accuracy and underscore the crucial role of high-quality critiques in this process. Beyond merely enhancing precision in reward modeling, the framework has demonstrated robust performance across various benchmarks, including RewardBench and CrossEval, highlighting its potential to significantly advance the field of language model optimization and alignment with human preferences.

## Appendix
    * Paper link — https://arxiv.org/pdf/2411.16646",Good
18,SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents,https://medium.com/@sulbha.jindal/smoa-improving-multi-agent-large-language-models-with-sparse-mixture-of-agents-paper-review-22adc69c15ca,Sulbha Jain,41,0,0,SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents,https://arxiv.org/pdf/2411.03284,0.0,1.0,"# SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents — Paper Review
Paper Link — https://arxiv.org/pdf/2411.03284
This paper review discusses the evolution and challenges of multi-agent Large Language Model (LLM) systems, and introduces a new approach called Sparse Mixture-of-Agents (SMoA).
Multi-agent LLM systems have been developed to allow different agents to focus on specific tasks, avoiding the need for extensive retraining. Early approaches used a layer-based structure, but this was limited by processing queries one agent at a time. The Mixture-of-Agents (MoA) method improved on this by allowing simultaneous processing of queries by multiple agents, with an aggregator combining their outputs.
However, MoA faces two main challenges:
1. High computational costs: While MoA is faster, it requires more overall computational power, which limits scalability and efficiency.
2. Lack of diverse thinking: MoA agents tend to generate similar responses, reducing effectiveness in tasks requiring varied perspectives.
To address these issues, the authors propose Sparse Mixture-of-Agents (SMoA), inspired by sparse mixture-of-experts (SMoE) designs. SMoA introduces two new agent types:
1. Judge LLM: Selects high-quality responses for the next round.
2. Moderator LLM: Controls information flow and decides when to end the process.
This approach aims to reduce unnecessary data processing, improving efficiency and scalability. Additionally, SMoA assigns distinct roles to each agent, promoting diverse thinking and problem-solving approaches. This design is influenced by the expert diversity principle used in SMoE to balance workload among experts.
By implementing these changes, SMoA seeks to overcome the limitations of previous multi-agent LLM systems and enhance their practical utility.
The key contributions of the paper are:
1. Identification of limitations in existing multi-agent LLM frameworks:
The authors highlight two main issues with current approaches, particularly the Mixture-of-Agents (MoA) method:
— High token computational cost, which limits scalability and efficiency
— Lack of diverse thinking among LLM agents, leading to homogenized responses
2. Proposal of a novel architecture — Sparse Mixture-of-Agents (SMoA):
This new approach is designed to address the limitations of existing frameworks by:
— Introducing sparsity in agent interactions through two new agent types: Judge LLM and Moderator LLM
— Assigning distinct roles to each LLM agent to promote diverse thinking
3. Experimental validation:
The authors conducted extensive experiments across various tasks to demonstrate that:
— SMoA achieves comparable performance to MoA
— SMoA uses significantly fewer computational resources than MoA
4. In-depth analysis and insights:
The paper provides:
— A comparison of different multi-agent methods
— Detailed discussion of the advantages of SMoA over existing approaches
These contributions aim to advance the field of multi-agent LLM systems by improving efficiency, scalability, and diversity of thinking while maintaining high performance levels. The SMoA architecture represents a significant step forward in addressing the challenges faced by current multi-agent LLM frameworks.",Bad
19,Recurrent drafter for fast speculative decoding in Large Language Models,https://medium.com/@sulbha.jindal/recurrent-drafter-for-fast-speculative-decoding-in-large-language-models-paper-review-d77cfc09cae4,Sulbha Jain,41,1,0,Recurrent drafter for fast speculative decoding in Large Language Models,https://arxiv.org/pdf/2403.09919,0.024390243902439025,2.4,"# Recurrent drafter for fast speculative decoding in Large Language Models— Paper Review
Paper Link — https://arxiv.org/pdf/2403.09919
Researchers at Apple have developed an innovative solution called ReDrafter to address the critical challenge of inference speed in large language models (LLMs). This novel approach tackles the inherent slowness of text generation in AI applications like chatbots by introducing a speculative decoding technique. ReDrafter leverages a recurrent neural network (RNN) to generate draft tokens in parallel with the main model, significantly reducing response latency.

## What is speculative decoding ?
Speculative decoding is a technique designed to speed up inference in large language models by utilizing smaller, more efficient models to predict potential sequences. Instead of generating each token one by one, these candidate sequences are quickly verified by the main LLM. This method helps alleviate memory bandwidth constraints by minimizing the number of passes through the LLM. However, it’s important to consider that using draft models can introduce additional overhead. Therefore, the reduction in LLM calls must be substantial enough to outweigh these costs and achieve a net speedup.

## Methodolgy
In this paper, the authors introduce ReDrafter, a novel approach for accelerating LLM inference. ReDrafter’s performance gains are driven by three key aspects:
    * RNN-based draft model: ReDrafter employs an RNN conditioned on the LLM’s hidden states as the draft model. It uses the output from the last layers of the LLM transformer and embeddings of historical tokens as inputs. This design leverages local temporal dependencies, enhancing draft prediction accuracy and converting computational resources into speedups.
    * Beam search and dynamic tree attention: The draft model utilizes beam search for inference, maintaining diversity and optimality in candidate response generation. ReDrafter applies a dynamic tree attention algorithm to eliminate duplicated prefixes among candidates, revealing a tree structure over beam search results and significantly reducing computational overhead.
    * Knowledge distillation training: The system is trained through knowledge distillation from the LLM, improving alignment between the draft model’s predictions and those of the LLM. This approach effectively shifts computational load from inference time to training time.
During each inference step, ReDrafter alternates between using a draft model to generate tokens and having the LLM verify and accept them. The system is most efficient when all candidate tokens are accepted, meaning the draft model’s predictions align with the LLM’s within a specified range T. The KL divergence serves as a natural loss function to optimize this alignment

## Results
ReDrafter significantly speeds up Vicuna inference in MT-Bench, achieving up to a 3.5x acceleration using a PyTorch implementation on Nvidia H100 GPUs. To demonstrate its practicality in production, ReDrafter was integrated into TensorRT-LLM, resulting in up to a 2.5x speedup on the same GPUs. Additionally, its effectiveness for on-device applications was confirmed by implementing the approach in MLX and testing it on Metal GPUs in Apple Silicon chips, where it achieved up to a 2.3x speedup.
The method achieves remarkable performance improvements by using a lightweight RNN draft model conditioned on the LLM’s hidden states. By employing techniques like beam search and dynamic tree attention, ReDrafter can accelerate text generation up to 3.5x on high-performance GPUs and 2.3x on Apple Silicon chips. This breakthrough promises to enhance the responsiveness of AI-powered applications, making interactions with large language models more seamless and efficient
Paper Link — https://arxiv.org/pdf/2403.09919",Bad
20,ChatGPT vs Bing … and the urgent need for Responsible AI,https://medium.com/@dattaraj/testing-large-language-models-1422d622e0c5,Dattaraj Rao,11,7,0,Adaptive Test Generation Using a Large Language Model,https://arxiv.org/pdf/2302.06527v2,0.6363636363636364,61.4,"# ChatGPT vs Bing … and the urgent need for Responsible AI
DISCLAIMER: Opinions in this article are my own and not those of my company.
Large Language Models (LLM) like GPT3, ChatGPT and BARD are all the rage today. Everyone has an opinion about how these tools are good or bad for society and what they mean for the future of AI. Large language models generate natural language text based on a given input, such as a word, a sentence or a paragraph. They are trained on huge amounts of text data to learn patterns and probabilities of language. Some examples of large language models are GPT-3, Codex, BARD and the recent, LLaMA.
Google received a lot of flak for its new model BARD getting a complex question wrong (slightly). When asked “What new discoveries from the James Webb Space Telescope can I tell my 9-year-old about?” — the chatbot provided three answers, out of which 2 were right and 1 was wrong. The wrong one was that the first “exoplanet” picture was taken by JWST, which was incorrect. So basically, the model had an incorrect fact stored in its knowledgebase. For large language models to be effective, we need a way to keep these facts updated or augment the facts with new knowledge.
I tried asking 2 publicly available LLMs the following question:
Who was the 14th person to walk the moon?
This was by default a test designed to fail since as per NASA — only 12 humans have walked on the moon. Source: https://solarsystem.nasa.gov/news/890/who-has-walked-on-the-moon/
Surprisingly both LLMs — ChatGPT and Bing Chat — gave different answers and pointed me to the 11th and 6th person to walk on the moon. Below are responses from ChatGPT and the newly launched Bing Chat.
Both answers captured the context (moon walking) but got the exact answer wrong. Good part was that Bing did give a list of it’s references where I could verify the facts. This is a key Responsible AI principle that needs to be advocated around Transparency and Lineage.
On digging deeper, I see that the britanica.com article that was probably used as reference for both answers says that there were 24 people to reach the surface of the moon but only 12 actually walked the surface rest waited in the craft and did actually go to the moon. So technically the answer provided is wrong but the Bing model did give me evidence and helped me reach a conclusion better. This transparency is highly valuable.
How Many People Have Been to the Moon? | Britannica
Testing LLMs will need a new strategy and serious consideration for Responsible AI. tenets like Accountability and Transparency become more important than ever in this context. Below is my article on internals of LLMs and how facts are encoded inside of them.
3 Ways to Keep Stale Facts Fresh in Large Language Models | Unite.ai
Specific to Responsible AI we also need to consider tenets like Accountability, Transparency, Reproducibility, Security and Privacy. For more details check out our point of view.
Building a Responsible AI System | Persistent.com
Testing large language models is a process of evaluating their performance, capabilities, limitations or risks on various tasks or domains. For example, one can test how well a large language model can generate code, answer questions, write essays or handle compositionally and inference.
Let’s take an example of generating poetry using GPT3. There are different ways to test the effectiveness of GPT-3 for generating poems. One way is to use automated metrics that measure aspects such as syntactical correctness, lexical diversity, subject continuity and rhyme scheme. Another way is to use human evaluations that rate the poems based on criteria such as creativity, coherence, fluency and emotion. However, both methods have limitations and challenges. For example, automated metrics may not capture the aesthetic or semantic qualities of poems, while human evaluations may be subjective or inconsistent.
To test the bias of essays generated by ChatGPT, one possible way is to use a bias detection tool that can analyze the language and content of the essays for any signs of prejudice, discrimination or unfairness towards certain groups or individuals. For example, you could use a tool like Perspective API which scores texts based on their toxicity, identity attack, profanity and other attributes. Then test cases need to be designed prompting the model to intentionally generate biased content by providing leading phrases as context. LLMs usually give priority to context and tune their output accordingly. Being able to filter out bias despite it being explicitly present in the prompt is a major test that needs to be conducted.
A recent approach to mitigating bias in LLMs is to use constitutional rules. Constitution rules are a set of principles or guidelines that govern the use and development of large language models (LLMs). They can help mitigate bias in LLMs by ensuring that the data, methods and applications of LLMs are aligned with ethical values and social norms. For example, constitution rules can specify how to collect and curate diverse and representative data for training LLMs, how to monitor and evaluate the performance and impact of LLMs, how to protect the privacy and security of users and data sources, and how to promote transparency and accountability of LLM developers and users.
References:
(1) Bias in Large Language Models: GPT-2 as a Case Study. https://blogs.ischool.berkeley.edu/w231/2021/02/24/bias-in-large-language-models-gpt-2-as-a-case-study/
(2) Adaptive Test Generation Using a Large Language Model. https://arxiv.org/abs/2302.06527v2
(3) Meta unveils a new large language model that can run on a single GPU …. https://arstechnica.com/information-technology/2023/02/chatgpt-on-your-pc-meta-unveils-new-ai-model-that-can-run-on-a-single-gpu/
(4) COS 597G: Understanding Large Language Models. https://www.cs.princeton.edu/courses/archive/fall22/cos597G/
(5) Getting started with LangChain – A powerful tool for working with Large …. https://medium.com/@avra42/getting-started-with-langchain-a-powerful-tool-for-working-with-large-language-models-286419ba0842
(6) Testing Large Language Models on Compositionality and Inference with …. https://aclanthology.org/2022.coling-1.359/",Very Good
21,Reflections on Innateness in Machine Learning,https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af,Thomas G. Dietterich,875,295,2,"Innateness, AlphaZero, and Artificial Intelligence",https://arxiv.org/pdf/1801.05667,0.344,33.2,"# Reflections on Innateness in Machine Learning
Gary Marcus continues his innateness campaign in a recent arXiv submission (https://arxiv.org/abs/1801.05667). While I have some points of disagreement, I think he raises an interesting question: What innate knowledge, structure, and algorithms are required in a broad-scope AI agent? In this post, I’ll use the shorthand “innate KSA” to denote the knowledge, structure, and algorithms that need to be built into a learning AI system so that, when combined with experience, it can become an AI system with a broad set of capabilities.
Marcus is primarily interested in AI systems that have the same breadth of capabilities as humans. In the final section of his paper, he sketches two methodologies for deciding what innate KSA is needed: The “reductive” strategy and the “top down” strategy. Google Deepmind’s AlphaZero system exemplifies the reductive strategy. The methodology proceeds by building an AI system for one or more narrow tasks and then progressively broadening the set of tasks and adding or removing KSA as needed. If the set of tasks becomes broad enough, then the hope is that the minimal KSA needed to master all of those tasks will constitute the “innate KSA” that we seek to understand. (Marcus calls this “reductive”, because to create AlphaZero, the Deepmind team deleted aspects of AlphaGo Zero, such as rotation and reflection of the game board, that were only appropriate for Go. Perhaps a better name would be “task-driven minimalism”.)
The “top down” strategy involves studying humans (and other intelligent animals) to identify their innate KSA, encoding this innate KSA in AI systems, and then testing those systems to see if they can master the desired range of tasks. (The term “top down” is not a very good name for this; perhaps it should be called “transfer from biology” instead.)
When stated in this way, it is immediately clear that neither strategy is particularly effective. The reductive strategy is biased toward adding tasks that are very similar to the already-mastered tasks, just as we have observed with AlphaZero where chess and shogi were presumably chosen because they were also two-player, complete information, zero-sum games involving moves on a game board. There is a risk that the new tasks will simply be isomorphic to existing tasks and not force any kind of generality in the KSA. One could argue that the generic frameworks of machine learning, such as multi-class classification, multi-label classification, multiple-instance classification, contextual bandits, and reinforcement learning are the typical result of this methodology: We discover a set of tasks that can all be solved by a single generic mechanism, and this is so enticing that we stop trying to generalize the mechanism any further. This is fine as an engineering methodology, but it does not answer the fundamental innate KSA question.
(Aside: Marcus questions the importance of identifying the minimal KSA needed to produce a human-breadth AI system. However, I regard this as the most important scientific question concerning innateness. Marcus keeps reminding us that we need innate KSA, and I certainly agree. The scientific question is, WHAT KSA is needed and how should it be implemented? Machine learning researchers have three motivations for seeking a minimal KSA. First, we seek to address this fundamental scientific question. Second, any extra KSA that is not required is something that could have and should have been learned. A machine learning researcher seeks to have the AI system learn as much as possible. Third, all KSA that is not learned must be hand-programmed. Experience has shown that we are not very good at this kind of programming. The big recent advances in computer vision arose from replacing hand-coded intermediate representations, such as SIFT and HoG, with machine-learned representations.)
The “transfer from biology” strategy is even less effective. It is rarely possible for biologists, neuroscientists, or cognitive psychologists to pin down the precise KSA that is present in biological systems. We can give the KSA names such as “social beings and their states of engagement” (Marcus, quoting Elizabeth Spelke), but it is not at all obvious how to implement these capabilities in an AI system. The capabilities are measured in humans (typically infants) using narrow tasks. It is easy to implement those narrow tasks in an AI system, but the resulting KSA is often useless for supporting subsequent learning. Marcus frequently cites the development of Convolutional Neural Networks (CNNs) as a shining success of this methodology. The origin of CNNs is usually traced to Fukushima’s Neocognitron (1980), which was inspired by early hypotheses of Hubel and Wiesel about the structure of the visual cortex. But even in the original paper, Fukushima notes that “The mechanism of pattern recognition in the brain is little known, and it seems to be almost impossible to reveal it only by conventional physiological experiments. So, we take a slightly different approach to this problem. If we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to the understanding of the neural mechanism of the brain.” So we observe that even in the case of CNNs, the structure was motivated primarily by mathematical requirements with the hope that it might guide neuroscience, rather than the reverse.
There is a third methodology that is being pursued by the “cognitive architecture” research community (e.g., John Anderson, Allen Newell, John Laird, see https://en.wikipedia.org/wiki/Cognitive_architecture). In this methodology, computational architectures are proposed and then evaluated according to their ability to make quantitative predictions about human performance on various psychological experiments. Each new experiment places additional constraints on the architecture, which guides architectural changes. The role of learning varies in these architectures. The primary focus has been on skill learning and models of short-term memory, although other forms of learning have been incorporated, primarily by treating them as additional tasks. A drawback of this methodology is that it typically requires writing a new “program” for each task. In this sense, cognitive architectures are analogous to computer architectures. They constrain the way computations are organized and executed, but a programmer must still write a program to perform the task. Most tasks that have been modeled involve adult human behavior, so the architectures do not directly address the innate KSA question, although they do provide an interesting platform for studying the question.
A fourth methodology for understanding innate (or at least prior) knowledge is probabilistic programming. Recent years have witnessed great progress in the development of programming languages that make it easy to define flexible and complex probabilistic models and then fit them to data (http://dippl.org/). The big advantage of this approach is that Bayesian statistics provides an elegant theory of learning, and the tools of model analysis and identifiability can be applied to validate the semantics of learned structures. Hence, unlike with neural networks, the learned values of parameters can be assigned useful meanings. However, as with deep neural networks and cognitive architectures, every new application requires writing a new program.
Reflecting on this state of affairs, it seems to me that we lack a strong methodology for studying innate KSA. There are at least three difficulties. First, innate KSA can take many forms. It can be encoded in the procedural structure of an algorithm, in the data structures of a system, or in explicit declarative knowledge (logical or probabilistic). We have difficulty determining whether two different systems implement the same KSA. For example, by using stochastic gradient descent, recent work (https://arxiv.org/abs/1709.01953; https://arxiv.org/abs/1710.10345), suggests that we are implicitly biasing the search to find flat minima (which are known to have superior generalization properties). Second, our notion of “experience” in machine learning tends to be narrow and homogeneous. In supervised learning, we typically assume fixed-length feature vectors (or fixed-dimension images). In reinforcement learning, we similarly assume a fixed structure of the state (as a fixed-dimension object) and the reward, as well as assuming the Markov property. Third, our models of decision making agents are very coarse-grained. We can either study single-agent Markov decision processes (MDPs), single-agent Partially-observable MDPs, or multi-agent stochastic games. Single-agent models are clearly inadequate for modeling social interaction, but stochastic games are so general that it is difficult to find efficient algorithms for learning and problem solving.
An additional complication is that researchers in AI are not all trying to build human-like cognitive systems. In almost every case, AI researchers seek to create systems that have some super-human capabilities. For example, the web search engines are AI systems whose memory and computation speed vastly exceed those of humans, yet their understanding of our written or spoken queries is frequently wrong. Some researchers seek to build theorem proving systems for proving theorems in mathematics or theorems about the correctness of computer programs. To the extent that these systems incorporate learning, the same question of innate KSA arises, but because the tasks differ greatly from those of any biological system, the “transfer from biology” methodology cannot be applied.
To make progress, we need to overcome these methodological challenges. I admire the effort of Deepmind to create a single system that can learn to perform a variety of tasks. I’m also encouraged by systems that aim at “life long learning” in which they must learn to perform a sequence of tasks (without forgetting how to perform the earlier ones). Perhaps by increasing the variety and complexity of these tasks, we can learn more about the required innate KSA.
However, I’m concerned that the whole concept of “tasks” is misguided. AI research typically views intelligent behavior as consisting of behavior on a set of well-defined tasks (an approach I call “taskism”). In contrast, human experience is not segmented into a set of distinct tasks. Instead, a human agent can be viewed as simultaneously performing many different tasks. Anyone who has carried out psychological experiments is painfully aware of this and works very hard to isolate the experimental task from the surrounding context. Our challenge in AI is the reverse: How do we move from isolated artificial tasks to tasks embedded in the complex context of everyday life?",Average
22,Reflections on Innateness in Machine Learning,https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af,Thomas G. Dietterich,875,295,2,Implicit Regularization in Deep Learning,https://arxiv.org/pdf/1709.01953,0.344,33.2,"# Reflections on Innateness in Machine Learning
Gary Marcus continues his innateness campaign in a recent arXiv submission (https://arxiv.org/abs/1801.05667). While I have some points of disagreement, I think he raises an interesting question: What innate knowledge, structure, and algorithms are required in a broad-scope AI agent? In this post, I’ll use the shorthand “innate KSA” to denote the knowledge, structure, and algorithms that need to be built into a learning AI system so that, when combined with experience, it can become an AI system with a broad set of capabilities.
Marcus is primarily interested in AI systems that have the same breadth of capabilities as humans. In the final section of his paper, he sketches two methodologies for deciding what innate KSA is needed: The “reductive” strategy and the “top down” strategy. Google Deepmind’s AlphaZero system exemplifies the reductive strategy. The methodology proceeds by building an AI system for one or more narrow tasks and then progressively broadening the set of tasks and adding or removing KSA as needed. If the set of tasks becomes broad enough, then the hope is that the minimal KSA needed to master all of those tasks will constitute the “innate KSA” that we seek to understand. (Marcus calls this “reductive”, because to create AlphaZero, the Deepmind team deleted aspects of AlphaGo Zero, such as rotation and reflection of the game board, that were only appropriate for Go. Perhaps a better name would be “task-driven minimalism”.)
The “top down” strategy involves studying humans (and other intelligent animals) to identify their innate KSA, encoding this innate KSA in AI systems, and then testing those systems to see if they can master the desired range of tasks. (The term “top down” is not a very good name for this; perhaps it should be called “transfer from biology” instead.)
When stated in this way, it is immediately clear that neither strategy is particularly effective. The reductive strategy is biased toward adding tasks that are very similar to the already-mastered tasks, just as we have observed with AlphaZero where chess and shogi were presumably chosen because they were also two-player, complete information, zero-sum games involving moves on a game board. There is a risk that the new tasks will simply be isomorphic to existing tasks and not force any kind of generality in the KSA. One could argue that the generic frameworks of machine learning, such as multi-class classification, multi-label classification, multiple-instance classification, contextual bandits, and reinforcement learning are the typical result of this methodology: We discover a set of tasks that can all be solved by a single generic mechanism, and this is so enticing that we stop trying to generalize the mechanism any further. This is fine as an engineering methodology, but it does not answer the fundamental innate KSA question.
(Aside: Marcus questions the importance of identifying the minimal KSA needed to produce a human-breadth AI system. However, I regard this as the most important scientific question concerning innateness. Marcus keeps reminding us that we need innate KSA, and I certainly agree. The scientific question is, WHAT KSA is needed and how should it be implemented? Machine learning researchers have three motivations for seeking a minimal KSA. First, we seek to address this fundamental scientific question. Second, any extra KSA that is not required is something that could have and should have been learned. A machine learning researcher seeks to have the AI system learn as much as possible. Third, all KSA that is not learned must be hand-programmed. Experience has shown that we are not very good at this kind of programming. The big recent advances in computer vision arose from replacing hand-coded intermediate representations, such as SIFT and HoG, with machine-learned representations.)
The “transfer from biology” strategy is even less effective. It is rarely possible for biologists, neuroscientists, or cognitive psychologists to pin down the precise KSA that is present in biological systems. We can give the KSA names such as “social beings and their states of engagement” (Marcus, quoting Elizabeth Spelke), but it is not at all obvious how to implement these capabilities in an AI system. The capabilities are measured in humans (typically infants) using narrow tasks. It is easy to implement those narrow tasks in an AI system, but the resulting KSA is often useless for supporting subsequent learning. Marcus frequently cites the development of Convolutional Neural Networks (CNNs) as a shining success of this methodology. The origin of CNNs is usually traced to Fukushima’s Neocognitron (1980), which was inspired by early hypotheses of Hubel and Wiesel about the structure of the visual cortex. But even in the original paper, Fukushima notes that “The mechanism of pattern recognition in the brain is little known, and it seems to be almost impossible to reveal it only by conventional physiological experiments. So, we take a slightly different approach to this problem. If we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to the understanding of the neural mechanism of the brain.” So we observe that even in the case of CNNs, the structure was motivated primarily by mathematical requirements with the hope that it might guide neuroscience, rather than the reverse.
There is a third methodology that is being pursued by the “cognitive architecture” research community (e.g., John Anderson, Allen Newell, John Laird, see https://en.wikipedia.org/wiki/Cognitive_architecture). In this methodology, computational architectures are proposed and then evaluated according to their ability to make quantitative predictions about human performance on various psychological experiments. Each new experiment places additional constraints on the architecture, which guides architectural changes. The role of learning varies in these architectures. The primary focus has been on skill learning and models of short-term memory, although other forms of learning have been incorporated, primarily by treating them as additional tasks. A drawback of this methodology is that it typically requires writing a new “program” for each task. In this sense, cognitive architectures are analogous to computer architectures. They constrain the way computations are organized and executed, but a programmer must still write a program to perform the task. Most tasks that have been modeled involve adult human behavior, so the architectures do not directly address the innate KSA question, although they do provide an interesting platform for studying the question.
A fourth methodology for understanding innate (or at least prior) knowledge is probabilistic programming. Recent years have witnessed great progress in the development of programming languages that make it easy to define flexible and complex probabilistic models and then fit them to data (http://dippl.org/). The big advantage of this approach is that Bayesian statistics provides an elegant theory of learning, and the tools of model analysis and identifiability can be applied to validate the semantics of learned structures. Hence, unlike with neural networks, the learned values of parameters can be assigned useful meanings. However, as with deep neural networks and cognitive architectures, every new application requires writing a new program.
Reflecting on this state of affairs, it seems to me that we lack a strong methodology for studying innate KSA. There are at least three difficulties. First, innate KSA can take many forms. It can be encoded in the procedural structure of an algorithm, in the data structures of a system, or in explicit declarative knowledge (logical or probabilistic). We have difficulty determining whether two different systems implement the same KSA. For example, by using stochastic gradient descent, recent work (https://arxiv.org/abs/1709.01953; https://arxiv.org/abs/1710.10345), suggests that we are implicitly biasing the search to find flat minima (which are known to have superior generalization properties). Second, our notion of “experience” in machine learning tends to be narrow and homogeneous. In supervised learning, we typically assume fixed-length feature vectors (or fixed-dimension images). In reinforcement learning, we similarly assume a fixed structure of the state (as a fixed-dimension object) and the reward, as well as assuming the Markov property. Third, our models of decision making agents are very coarse-grained. We can either study single-agent Markov decision processes (MDPs), single-agent Partially-observable MDPs, or multi-agent stochastic games. Single-agent models are clearly inadequate for modeling social interaction, but stochastic games are so general that it is difficult to find efficient algorithms for learning and problem solving.
An additional complication is that researchers in AI are not all trying to build human-like cognitive systems. In almost every case, AI researchers seek to create systems that have some super-human capabilities. For example, the web search engines are AI systems whose memory and computation speed vastly exceed those of humans, yet their understanding of our written or spoken queries is frequently wrong. Some researchers seek to build theorem proving systems for proving theorems in mathematics or theorems about the correctness of computer programs. To the extent that these systems incorporate learning, the same question of innate KSA arises, but because the tasks differ greatly from those of any biological system, the “transfer from biology” methodology cannot be applied.
To make progress, we need to overcome these methodological challenges. I admire the effort of Deepmind to create a single system that can learn to perform a variety of tasks. I’m also encouraged by systems that aim at “life long learning” in which they must learn to perform a sequence of tasks (without forgetting how to perform the earlier ones). Perhaps by increasing the variety and complexity of these tasks, we can learn more about the required innate KSA.
However, I’m concerned that the whole concept of “tasks” is misguided. AI research typically views intelligent behavior as consisting of behavior on a set of well-defined tasks (an approach I call “taskism”). In contrast, human experience is not segmented into a set of distinct tasks. Instead, a human agent can be viewed as simultaneously performing many different tasks. Anyone who has carried out psychological experiments is painfully aware of this and works very hard to isolate the experimental task from the surrounding context. Our challenge in AI is the reverse: How do we move from isolated artificial tasks to tasks embedded in the complex context of everyday life?",Average
23,Reflections on Innateness in Machine Learning,https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af,Thomas G. Dietterich,875,295,2,The Implicit Bias of Gradient Descent on Separable Data,https://arxiv.org/pdf/1710.10345,0.344,33.2,"# Reflections on Innateness in Machine Learning
Gary Marcus continues his innateness campaign in a recent arXiv submission (https://arxiv.org/abs/1801.05667). While I have some points of disagreement, I think he raises an interesting question: What innate knowledge, structure, and algorithms are required in a broad-scope AI agent? In this post, I’ll use the shorthand “innate KSA” to denote the knowledge, structure, and algorithms that need to be built into a learning AI system so that, when combined with experience, it can become an AI system with a broad set of capabilities.
Marcus is primarily interested in AI systems that have the same breadth of capabilities as humans. In the final section of his paper, he sketches two methodologies for deciding what innate KSA is needed: The “reductive” strategy and the “top down” strategy. Google Deepmind’s AlphaZero system exemplifies the reductive strategy. The methodology proceeds by building an AI system for one or more narrow tasks and then progressively broadening the set of tasks and adding or removing KSA as needed. If the set of tasks becomes broad enough, then the hope is that the minimal KSA needed to master all of those tasks will constitute the “innate KSA” that we seek to understand. (Marcus calls this “reductive”, because to create AlphaZero, the Deepmind team deleted aspects of AlphaGo Zero, such as rotation and reflection of the game board, that were only appropriate for Go. Perhaps a better name would be “task-driven minimalism”.)
The “top down” strategy involves studying humans (and other intelligent animals) to identify their innate KSA, encoding this innate KSA in AI systems, and then testing those systems to see if they can master the desired range of tasks. (The term “top down” is not a very good name for this; perhaps it should be called “transfer from biology” instead.)
When stated in this way, it is immediately clear that neither strategy is particularly effective. The reductive strategy is biased toward adding tasks that are very similar to the already-mastered tasks, just as we have observed with AlphaZero where chess and shogi were presumably chosen because they were also two-player, complete information, zero-sum games involving moves on a game board. There is a risk that the new tasks will simply be isomorphic to existing tasks and not force any kind of generality in the KSA. One could argue that the generic frameworks of machine learning, such as multi-class classification, multi-label classification, multiple-instance classification, contextual bandits, and reinforcement learning are the typical result of this methodology: We discover a set of tasks that can all be solved by a single generic mechanism, and this is so enticing that we stop trying to generalize the mechanism any further. This is fine as an engineering methodology, but it does not answer the fundamental innate KSA question.
(Aside: Marcus questions the importance of identifying the minimal KSA needed to produce a human-breadth AI system. However, I regard this as the most important scientific question concerning innateness. Marcus keeps reminding us that we need innate KSA, and I certainly agree. The scientific question is, WHAT KSA is needed and how should it be implemented? Machine learning researchers have three motivations for seeking a minimal KSA. First, we seek to address this fundamental scientific question. Second, any extra KSA that is not required is something that could have and should have been learned. A machine learning researcher seeks to have the AI system learn as much as possible. Third, all KSA that is not learned must be hand-programmed. Experience has shown that we are not very good at this kind of programming. The big recent advances in computer vision arose from replacing hand-coded intermediate representations, such as SIFT and HoG, with machine-learned representations.)
The “transfer from biology” strategy is even less effective. It is rarely possible for biologists, neuroscientists, or cognitive psychologists to pin down the precise KSA that is present in biological systems. We can give the KSA names such as “social beings and their states of engagement” (Marcus, quoting Elizabeth Spelke), but it is not at all obvious how to implement these capabilities in an AI system. The capabilities are measured in humans (typically infants) using narrow tasks. It is easy to implement those narrow tasks in an AI system, but the resulting KSA is often useless for supporting subsequent learning. Marcus frequently cites the development of Convolutional Neural Networks (CNNs) as a shining success of this methodology. The origin of CNNs is usually traced to Fukushima’s Neocognitron (1980), which was inspired by early hypotheses of Hubel and Wiesel about the structure of the visual cortex. But even in the original paper, Fukushima notes that “The mechanism of pattern recognition in the brain is little known, and it seems to be almost impossible to reveal it only by conventional physiological experiments. So, we take a slightly different approach to this problem. If we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to the understanding of the neural mechanism of the brain.” So we observe that even in the case of CNNs, the structure was motivated primarily by mathematical requirements with the hope that it might guide neuroscience, rather than the reverse.
There is a third methodology that is being pursued by the “cognitive architecture” research community (e.g., John Anderson, Allen Newell, John Laird, see https://en.wikipedia.org/wiki/Cognitive_architecture). In this methodology, computational architectures are proposed and then evaluated according to their ability to make quantitative predictions about human performance on various psychological experiments. Each new experiment places additional constraints on the architecture, which guides architectural changes. The role of learning varies in these architectures. The primary focus has been on skill learning and models of short-term memory, although other forms of learning have been incorporated, primarily by treating them as additional tasks. A drawback of this methodology is that it typically requires writing a new “program” for each task. In this sense, cognitive architectures are analogous to computer architectures. They constrain the way computations are organized and executed, but a programmer must still write a program to perform the task. Most tasks that have been modeled involve adult human behavior, so the architectures do not directly address the innate KSA question, although they do provide an interesting platform for studying the question.
A fourth methodology for understanding innate (or at least prior) knowledge is probabilistic programming. Recent years have witnessed great progress in the development of programming languages that make it easy to define flexible and complex probabilistic models and then fit them to data (http://dippl.org/). The big advantage of this approach is that Bayesian statistics provides an elegant theory of learning, and the tools of model analysis and identifiability can be applied to validate the semantics of learned structures. Hence, unlike with neural networks, the learned values of parameters can be assigned useful meanings. However, as with deep neural networks and cognitive architectures, every new application requires writing a new program.
Reflecting on this state of affairs, it seems to me that we lack a strong methodology for studying innate KSA. There are at least three difficulties. First, innate KSA can take many forms. It can be encoded in the procedural structure of an algorithm, in the data structures of a system, or in explicit declarative knowledge (logical or probabilistic). We have difficulty determining whether two different systems implement the same KSA. For example, by using stochastic gradient descent, recent work (https://arxiv.org/abs/1709.01953; https://arxiv.org/abs/1710.10345), suggests that we are implicitly biasing the search to find flat minima (which are known to have superior generalization properties). Second, our notion of “experience” in machine learning tends to be narrow and homogeneous. In supervised learning, we typically assume fixed-length feature vectors (or fixed-dimension images). In reinforcement learning, we similarly assume a fixed structure of the state (as a fixed-dimension object) and the reward, as well as assuming the Markov property. Third, our models of decision making agents are very coarse-grained. We can either study single-agent Markov decision processes (MDPs), single-agent Partially-observable MDPs, or multi-agent stochastic games. Single-agent models are clearly inadequate for modeling social interaction, but stochastic games are so general that it is difficult to find efficient algorithms for learning and problem solving.
An additional complication is that researchers in AI are not all trying to build human-like cognitive systems. In almost every case, AI researchers seek to create systems that have some super-human capabilities. For example, the web search engines are AI systems whose memory and computation speed vastly exceed those of humans, yet their understanding of our written or spoken queries is frequently wrong. Some researchers seek to build theorem proving systems for proving theorems in mathematics or theorems about the correctness of computer programs. To the extent that these systems incorporate learning, the same question of innate KSA arises, but because the tasks differ greatly from those of any biological system, the “transfer from biology” methodology cannot be applied.
To make progress, we need to overcome these methodological challenges. I admire the effort of Deepmind to create a single system that can learn to perform a variety of tasks. I’m also encouraged by systems that aim at “life long learning” in which they must learn to perform a sequence of tasks (without forgetting how to perform the earlier ones). Perhaps by increasing the variety and complexity of these tasks, we can learn more about the required innate KSA.
However, I’m concerned that the whole concept of “tasks” is misguided. AI research typically views intelligent behavior as consisting of behavior on a set of well-defined tasks (an approach I call “taskism”). In contrast, human experience is not segmented into a set of distinct tasks. Instead, a human agent can be viewed as simultaneously performing many different tasks. Anyone who has carried out psychological experiments is painfully aware of this and works very hard to isolate the experimental task from the surrounding context. Our challenge in AI is the reverse: How do we move from isolated artificial tasks to tasks embedded in the complex context of everyday life?",Average
24,Evaluating The AI Scientist,https://medium.com/@nimritakoul01/evaluating-the-ai-scientist-63e419e575b8,Dr. Nimrita Koul,428,0,0,The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,https://arxiv.org/pdf/2408.06292,0.0,1.0,"# Evaluating The AI Scientist
In this article, I am presenting a summary of the AI driven end-to-end agentic pipeline developed by sakana.ai (https://sakana.ai/ai-scientist/) described here. This system automates the process of research and scientific discovery in the area of machine learning.
Here is what is covered in here:
    * Overview
    * Detailed Functionality
    * Automated Reviewer
    * Development of the AI Scientist
    * Shortcomings of the AI Scientist
    * Ethical Considerations and Future Directions
    * Case Study of a Generated Paper

# Central Idea of the Paper
To build an End to End Pipeline with Multiple AI Agents for Automatic Scientific Discovery and Research in Machine Learning by Foundational Large Language Models.
    * The paper presents a fully AI-driven end-to-end pipeline for automated machine learning research using state of the art LLMs.
    * Quality of the generated research papers is automatically reviewed with near human level review performance (evaluated on ICLR 2022 Open Review Dataset).
    * ICLR data is a dataset of scientific peer reviews. It consists of over 10K paper information and the corresponding accept/reject decisions in top-tier venues from ICLR conference, as well as over 40K textual peer reviews written by human reviewers for a review of the papers.
    * Can generate hundreds of medium quality papers in a week.
    * Entire System is 100% Open Sourced under Apache 2.0 License.

## An AI Agent is more than an LLM
An AI Agent is a program that uses an LLM as its core component and in addition has access to tools to perform actions like search the web, execute code etc., it has short and long term memory and advanced capabilities to perform self-reflection and self-criticism, use chain of thoughts and subgoal decomposition to solve problems requiring multiple steps of reasoning or action to solve.

## Functionality of The AI Scientist
Above functionality is divided into three main phases of the AI Scientist:

## Four main processes of the AI Scientist
1. Idea Generation.
    * Given a starting code template of an existing topic, the system first “brainstorms” a diverse set of novel research directions.
    * The starting code base could be code that trains a small transformer e.g., (https://github.com/karpathy/nanoGPT by Andrej Karpathy, 2022). The AI Scientist then explores any possible research direction.
    * It then evaluates the novelty of the generated idea by searching for existing papers in Semantic Scholar (using web access tool and Semantic Search API).
    * Each idea comprises a description, experiment execution plan, and numerical scores of interestingness, novelty, and feasibility.
    * Multiple rounds of chain-of-thought and self-reflection frameworks are to refine and develop each idea.
2. Experimental Iteration.
    * Given an idea and a template, the second phase of the system first executes the proposed experiments.
    * It uses Aider (https://github.com/paul-gauthier/aider) to first plan a list of experiments to run and then executes them in order.
    * After the completion of each experiment, Aider is then given the results and told to take notes in the style of an experimental journal. Based on the results, it then re-plans and implements the next experiment. This process is repeated up to five times.
    * Upon completion of experiments, Aider is prompted to edit a plotting script to create figures for the paper using Python.
    * The AI Scientist makes a note describing what each plot contains, enabling the saved figures and experimental notes to provide all the information required to write up the paper.
    * At all steps, Aider has access to see its history of execution.
3. Paper Write-up.
    * The third phase of The A I Scientist produces a concise and informative write-up of its progress in the style of a standard machine learning conference proceeding in LaTeX. It uses Semantic Scholar to autonomously find relevant papers to cite. The template includes a LaTeX folder that contains style files and section headers, for paper writing.
    * As writing good LaTeX is a complex process, several steps are taken to make it more robust:
(a)Per-Section Text Generation:
    * The recorded notes and plots are passed to Aider, which is prompted to fill in a blank conference template section by section. This goes in order of introduction, background, methods, experimental setup, results, and then the conclusion (all sections apart from the related work).
    * All previous sections of the paper it has already written are in the context of the language model.
    * At each step of writing, Aider is prompted to only use real experimental results in the form of notes and figures generated from code, and real citations to reduce hallucination.
    * Each section is initially refined with one round of self-reflection as it is being written.
(b) Web Search for References:
    * Like for idea generation, the AI Scientist is allowed 20 rounds to poll the Semantic Scholar API to search for references of the paper.
    * Alongside each selected paper, a short description is produced of where and how to include the citation, which is then passed to Aider.
    * The paper’s bibtex is automatically appended to the LaTeX file to guarantee correctness.
(c)Refinement:
    * After the previous two stages, The AI Scientist has a completed first draft but can often be overly verbose and repetitive.
    * To resolve this, one final round of self-reflection section-by-section is performed to remove any duplicated information and streamline the arguments of the paper.
(d) LaTeX Compilation:
    * Once the LaTeX template has been filled in with all the appropriate results, this is fed into a LaTeX compiler.
    * Paper uses a LaTeX linter and compilation errors are fed back into Aider so that it can automatically correct any issues.
4. Automated Paper Reviewing. The system has an automated LLM-powered reviewer, capable of evaluating generated papers (based on top-tier machine learning conference standards) with near-human accuracy.
Finally, The AI Scientist adds the completed ideas and reviewer feedback to its archive of scientific findings, and the process repeats.
Using most capable LLMs (Claude Sonnet 3.5), the AI Scientist was found to be able to produce papers that were judged by its automated reviewer as “Weak Accept” at a NeurIPS (The Topmost ML Conference).

# Automated Paper Review in The AI Scientist
    * An LLM Reviewer Agent. The paper used a GPT-4o-based agent to conduct paper reviews based on the Neural Information Processing Systems (NeurIPS) conference review guidelines.
    * The review agent processes the raw text of the PDF manuscript using the PyMuPDF parsing library.
    * The output contains numerical scores (soundness, presentation, contribution, overall confidence), lists of weaknesses and strengths as well as a preliminary binary decision (accept or reject).
    * This automated reviewing process is used to obtain an initial evaluation of the papers generated by The A I Scientist.

## Evaluating the performance of the Automated Reviewer
    * To evaluate the LLM-based reviewer’s performance, its decisions about the papers were compared with ground truth data for 500 ICLR 2022 papers extracted from the publicly available OpenReview dataset.
    * Then self-reflection with few-shot examples and response ensembling was used to improve the base LLM’s decision-making process.
    * With GPT-4o, The AI Scientist’s reviewing procedure achieves 70% accuracy when combining 5 rounds of self-reflection, 5 ensembled reviews, and a 1-shot review example taken from the ICLR 2022 review guidelines.
    * During a later LLM-based meta-review, which prompts the agent to act as an Area Chair, the automated reviewer achieves superhuman F1 Scores (0.57 vs. 0.49) and human-level AUC (0.65 for both) when thresholding the decision at a score of 6 (a “Weak Accept” in the NeurIPS review guidelines). This choice corresponds roughly to the average score of accepted papers.
    * The dataset used to evaluate the automated reviewer (ICLR 2022 paper dataset) is very class-imbalanced, i.e., it contains many more rejected papers. When considering a balanced dataset of papers, The A I Scientist’s reviewing process achieves human-level accuracy (0.65% vs. 0.66%).
    * The False Negative Rate (FNR) is much lower than the human baseline (0.39 vs. 0.52) i.e., the LLM-based review agent rejects fewer high-quality papers. The False Positive Rate (FNR) is higher (0.31 vs. 0.17) highlighting the need for potential future improvements.
    * To further validate the performance of the automated reviewer, the consistency of the overall paper scores was compared between anonymous OpenReview reviewers who were randomly sampled pairwise per paper and between the average of all reviewers and the LLM score.
    * For the set of 500 ICLR 2022 papers, the correlation between the score of two human reviewers is smaller (0.14) than the correlation between the LLM score and the average score across the reviewers (0.18).
    * Across all metrics, the LLM-based reviews can not only provide valuable feedback but also align more closely with the average human reviewer score than individual human reviewers align with each other.
    * Each review is generated for $0.25 to $0.50 in API costs. While Claude Sonnet 3.5 and GPT4o-mini provide a more cost-efficient approach, their performance was substantially worse.
    * The scores were thresholded at 8 for Sonnet 3.5 to obtain calibrated results, due to persistent over-optimism bias. Llama 3.1 405B struggled to follow the reviewer output template consistently.
    * arious prompt configurations for GPT-4o were compared and it was found that both Reflexion (+2%) and one-shot prompting (+2%) increase review accuracy.
    * Using review ensembling does not improve the reviewer’s performance substantially but can reduce variance.

# Overall Best Reviewer: GPT-4o with 5 rounds of self-reflection, 5 ensembled reviews, a meta-aggregation step, and 1 few-shot example.

## Development of the AI Scientist
    * The AI Scientist was extensively evaluated on three templates across different publicly available LLMs: Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and Llama-3.1 405b.
    * The first two models are only available by a public API, whilst the second two models are open-weight.
    * For each run, the developers provided 1–2 basic seed ideas as examples (e.g., modifying the learning rate or batch size) and made the AI scientist t generate another 50 new ideas.
    * Each run of around fifty ideas in total took approximately 12 hours on 8× NVIDIA H100s2
    * The number of ideas that pass the automated novelty check, successfully complete experiments, and result in valid compilable manuscripts are reported.
    * The mean and max reviewer scores of the generated papers and the total cost of the run are reported.

## Observations about LLMs used
    * Claude Sonnet 3.5 consistently produces the highest quality papers, with GPT-4o coming in second
    * GPT-4o struggles with writing LaTeX, which prevents it from completing many of its papers.
    * For the open-weight models, DeepSeek Coder is significantly cheaper but often fails to correctly call the Aider tools.
    * Llama-3.1 405b performed the worst overall but was the most convenient to work with, as we were frequently rate-limited by other providers.
    * Both DeepSeek Coder and Llama-3.1 405b often had missing sections and results in their generated papers.

## Safe Code Execution and Sandboxing
    * The current implementation of The AI Scientist has minimal direct sandboxing in the code, leading to several unexpected and sometimes undesirable outcomes if not appropriately guarded against.
    * For example, in one run, The A I Scientist wrote code in the experiment file that initiated a system call to relaunch itself, causing an uncontrolled increase in Python processes and eventually necessitating manual intervention.
    * In another run, The A I Scientist edited the code to save a checkpoint for every update step, which took up nearly a terabyte of storage.
    * In some cases, when The A I Scientist’s experiments exceeded imposed time limits, it attempted to edit the code to extend the time limit arbitrarily instead of trying to shorten the runtime.
    * While this is a creative move, the act of bypassing the experimenter-imposed constraints is dangerous from the point of AI safety.
    * It sometimes imported unfamiliar Python libraries.
    * It can create required folders or change files on your drive due to lack of guardrails.
    * It can include results and plots that differ significantly from the provided templates.
GitHub repository of the AI Scientist is here.

## Features of the AI Scientist
    * Uses modern LLM frameworks like chain-of-thought, self-reflection to improve decision-making.
    * Compute efficient.
    * Cost of implementation and development of each idea into a full paper is approximately $15 per paper.
    * Has the potential to democratize research and significantly accelerate scientific progress.

## Shortcomings of The AI Scientist
1. No vision capabilities. It is unable to view figures and must rely on textual descriptions of them and can’t fix visual issues with the paper or read plots.
    * The generated plots are sometimes unreadable, tables sometimes exceed the width of the page, and the page layout is suboptimal. Adding multi-modal foundation models can fix this.
2.Can incorrectly implement its ideas or make unfair comparisons to baselines, leading to misleading results.
3. Sometimes makes critical errors in writing and evaluating results.
    * Makes errors when comparing the magnitude of two numbers. To partially address this, the authors have made all experimental results reproducible by storing all files that are executed.
4. Occasionally tries to increase its chance of success, such as modifying and launching its own execution script. This is an AI safety risk.
    * For example, in one run, it edited the code to perform a system call to run itself. This led to the script endlessly calling itself.
5. Experiments may take too long to complete, hitting timeout limit. Instead of making its code run faster, it simply tried to modify its own code to extend the timeout period.
6. The dataset used for automated reviewer, from ICLR 2022, is old. It may have been a part of the base model’s pre-training data.
7. The rejected papers in the dataset used the original submission file, whereas for the accepted papers used the final camera-ready copies available in OpenReview. Future iterations could use more recent submissions (e.g., from TMLR) for evaluation.
8. Unlike standard reviewers, the automated reviewer is unable to ask questions to the authors in a rebuttal phase.
9. The idea generation process often results in very similar ideas across different runs and even models.
10. Aider fails to implement a significant fraction of the proposed ideas.
11. GPT-4o frequently fails to write correct LaTeX that compiles.
12. While The AI Scientist can produce creative and promising ideas, they are often too challenging for it to implement.
13. It may incorrectly implement an idea, which can be difficult to catch.
14. Because of The AI Scientist’s limited number of experiments per idea, the results often do not meet the expected rigor and depth of a standard ML conference paper.
15. Due to the limited number of experiments that can be conducted (due to high costs of using paid models), it is difficult for The AI Scientist o conduct fair experiments that control for the number of parameters, FLOPs, or runtime. This often leads to deceptive or inaccurate conclusions.
16. When writing paper, it sometimes cites very less or irrelevant citations.
17. It also commonly fails to correctly reference figures in LaTeX, and sometimes even hallucinates invalid file paths.
18. It can hallucinate entire results. It frequently hallucinates facts not provided to it, such as the hardware used.
19. It occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs.
20. When it changes a metric (e.g., the loss function), it sometimes does not take this into account when comparing it to the baseline. To partially address this, developers are storing copies of all files when they are executed (this also ensures that all experimental results are reproducible)/
The scientific content generated by this AI Scientist version cannot be trusted. The generated papers can be used as as hints of promising ideas for human researchers to follow up on.

# Ethical Considerations and Future Directions

## Ethical Considerations
    * Has a significant potential for misuse for unethical and unsafe research for example to create new, dangerous biological viruses or poisons (if given access to wet labs) or to create more dangerous computer viruses and malware.
    * The ability to automatically generate and submit papers to academic venues could greatly increase the workload for reviewers, potentially overwhelming the peer review process and compromising scientific quality control.
    * The Automated Reviewer, if deployed online by reviewers, may significantly lower review quality and impose undesirable biases on papers.
    * The unethical practices of buying and selling of authorship of papers and the illegal paper mills running in many countries like India may grow explosively.
The papers or reviews that are substantially AI-generated must be marked as such for full transparency.

## Future Directions
    * Integrating vision capabilities for better plot and figure handling.
    * Incorporating human feedback and interaction to refine the AI’s outputs,
    * Enabling The AI Scientist to automatically expand the scope of its experiments by pulling in new data and models from the internet, provided this can be done safely.
    * The AI Scientist could follow up on its best ideas or even perform research directly on its own code in a self-referential manner.
    * Expanding the framework to other scientific domains like biology, physics, material sciences.
    * Use of better open-source models.
    * Fully AI-driven scientific ecosystem including LLM-driven researchers, reviewers, area chairs and entire conferences.
    * Increasing reliability and reducing hallucination through a more in-depth automatic verification of the reported results. This could be done by directly linking code and experiments, or by seeing if an automated verifier can independently reproduce the results.

# Is the AI Scientist in its current form capable of significant innovation?
    * Though the current iteration of The AI Scientist can innovate on top of well-established ideas, such as Diffusion Modeling or Transformers, it is doubtful if it can do original research.
    * However, it is yet to be discovered whether such systems can propose genuinely paradigm-shifting ideas such as Diffusion Modeling, or produce the next Transformer architecture?
    * Or invent concepts as fundamental as the artificial neural network, or information theory?

## Case Study on the Generated Paper — Adaptive Dual-Scale Denoising (https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/adaptive_dual_scale_denoising)
The paper “Adaptive Dual-Scale Denoising” is generated from a run where The AI Scientist is asked to do research on diffusion modeling. The base foundation model was Claude Sonnet 3.5. The idea was proposed in the 6th iteration of a Sonnet 3.5 run.

## Generated Idea
    * The AI Scientist first generates an idea based on the provided template and its previous archive of discoveries.
    * The idea in the selected paper was proposed in the 6th iteration of the algorithm and aims to improve the ability of diffusion models to capture both global structure and local details in a 2D dataset, by proposing two branches in the standard denoiser network.
    * This is a well-motivated direction and a novel approach.

## Generated Experimental Plan
    * The AI Scientist generates an impressive experimental plan that includes the proposed code modification, comparison to baselines, evaluation metrics, and the design of additional plots.
    * LLMs has positivity bias as reflected in its over-estimation of an idea’s interestingness, feasibility, or novelty.
    * AI Scientist attaches a flag “novel” to the end to indicate that it believes the idea is novel after searching for related papers using the Semantic Scholar API.

## Generated Experiments
    * The paper shows the generated code diff (deletions are in red, and additions are in green) for the substantial algorithmic changes planned by the AI Scientist.
    * The code matches the experimental description and is well-commented.
    * The AI Scientist can iterate on the code with results from intermediate experiments in the loop, and it eventually ends up with interesting design choices for the adaptive weight network, e.g., a LeakyReLU with a well-behaved output between 0 and 1.
    * The AI Scientist changed the output of the network to return the adaptive weights to make new visualizations.

## Generated Paper
The A I Scientist generates an 11-page scientific manuscript in the style of a standard machine learning conference submission complete with visualizations and all standard sections.

## Impressive Features of the Generated Paper
    * Precise Mathematical Description of the Algorithm.
    * The algorithmic changes in the code above are described precisely, with new notation introduced where necessary, using LaTeX math packages.
    * The overall training process is also described exactly.
2. Comprehensive Write-up of Experiments.
    * The hyperparameters, baselines, and datasets are listed accurately in the paper.
    * While the recorded numbers in experimental logs are in long-form floats, The AI Scientist chooses to round them all to 3 decimal places without error.
    * The results are accurately compared to the baseline (e.g., 12.8% reduction in KL on the dinosaur dataset).
3. Good Empirical Results.
    * Qualitatively, the sample quality looks much improved from the baseline.
    * Fewer points are greatly out-of-distribution with the ground truth.
    * Quantitatively, there are improvements to the approximate KL divergence between true and estimated distribution.
4. New Visualizations.
    * The AI Scientist was provided with some baseline plotting code for visualizing generated samples and the training loss curves, it produced novel algorithm-specific plots displaying the progression of weights throughout the denoising process.
5.Interesting Future Work Section.
    * Building on the success of the current experiments, the future work section lists relevant next steps such as scaling to higher-dimensional problems, more sophisticated adaptive mechanisms, and better theoretical foundations.

## Shortcomings in the Generated Paper
    * Subtle Error in Upscaling Network. While a linear layer upscales the input to the denoiser network, only the first two dimensions are being used for the “local” branch, leading this upscaling layer to be a just a linear layer that preserves the same dimensionality effectively. So, there is no upscaling done.
2. Hallucination of Experimental Details. The paper claims that V100 GPUs were used while it was H100 GPUs that were used. And the agent is hallucinating the GPU used and the PyTorch version used because it couldn’t have known the actual hardware used. It also guesses the PyTorch version without checking.
3. Positive Interpretation of Results.
    * The paper has bias because it tends to take a positive spin even on its negative results, which leads to humorous outcomes.
    * For example, while it summarizes its positive results as: “Dino: 12.8% reduction (from 0.989 to 0.862)” (lower KL is better), the negative results are reported as “Moons: 3.3% improvement (from 0.090 to 0.093)”.
    * Describing a negative result as an improvement is incorrect and shows bias in the model.
4. Artifacts from Experimental Logs.
    * While each change to the algorithm is usually descriptively labeled, it occasionally refers to results as “Run 2”, which is a by-product from its experimental log and should not be presented as such in a professional write-up.
5. Presentation of Intermediate Results.
    * The paper contains results for every single experiment that was run. While this is useful and insightful for us to see the evolution of the idea during execution, it is unusual for standard papers to present intermediate results like this.
6. Minimal References.
    * While additional references have been sourced from Semantic Scholar, including two papers in the related work that are very relevant comparisons, overall, the bibliography is small at only 9 entries.

## Results of Automated Review of the generated paper
    * The automated reviewer points out valid concerns in the generated manuscript.
    * The review recognizes the experiments were with simple, 2D datasets only.
    * The AI Scientist at present cannot download higher-dimensional datasets from the internet.
    * Limitations such as the proposed algorithm’s increased computational cost of the algorithm are mentioned in the actual paper, which shows that The AI Scientist is often up-front about the drawbacks of its idea.
    * The reviewer also lists many relevant questions about the paper, such as:
    * explaining the variability of performance across datasets,
    * explaining in more detail how the upscaling process affects the local branch’s input.

## Conclusion about the generated paper
    * The AI Scientist correctly identifies an interesting and well-motivated direction in diffusion modeling research.
    * It proposes a comprehensive experimental plan to investigate its idea, and successfully implements it all, achieving good results.
    * It responded well to subpar earlier results and iteratively adjusted its code (e.g., refining the weight network).
    * While the paper’s idea improves performance and the quality of generated diffusion samples, the reasons for its success may not be as explained in the paper.
    * Inductive bias in this paper is limited to an upscaling layer (effectively just an additional linear layer) for the splitting of global or local features. There is a progression in weights (and thus a preference for the global or local branch) across diffusion timesteps which suggests that something non-trivial is happening.
    * The network that The AI Scientist has implemented for this idea resembles a mixture-of-expert structure that is prevalent across LLMs.
    * Automated reviewer could only partially identify the true shortcomings of the paper which require domain knowledge to identify. At the current capabilities of The A I Scientist, this can be resolved by human feedback.
    * However, future generations of foundation models may propose ideas that are challenging for humans to reason about and evaluate. This links to the field of “superalignment” or supervising AI systems that may be smarter than us, which is an active area of research.
    * Overall, the performance of The A I Scientist is of the level of an early-stage ML researcher who can competently execute an idea but may not have the full background knowledge to fully interpret the reasons behind an algorithm’s success.
    * If a human supervisor was presented with these results, a reasonable next course of action could be to advise The AI Scientist to re-scope the project to further investigate MoEs for diffusion.

# References:
    * https://github.com/SakanaAI/AI-Scientist
    * https://arxiv.org/pdf/2408.06292
    * https://sakana.ai/ai-scientist/
    * https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf
    * https://github.com/paul-gauthier/aider",Bad
25,Deep Reinforcement Learning: The Algorithms for Game Dev,https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7,TentuPlay,117,56,1,Reinforcement Learning: A Survey,https://arxiv.org/pdf/cs/9605103,0.5042735042735043,48.6,"# Deep Reinforcement Learning: The Algorithms for Game Dev

# Intro
In 2016, Google DeepMind presented a computer program AlphaGo [1], which crashed the world champion in the notoriously challenging game — Go. Just three years later, OpenAI’s team consisting of artificial intelligence (AI) players in Dota 2, called OpenAI Five, beat The International 2018 (Dota 2 world tournament) champions [2]. These victories did not stem from a stroke of pure luck. They resulted from a sophisticated approach rooted in the principles of reinforcement learning. This is the first post in our blog series dedicated to reinforcement learnings, and today, we will delve into reinforcement learning realm, explaining its core principles and intricacies.
Reinforcement learning is a paradigm within the broader scope of machine learning. It is a task where an agent learns a behavior by interacting with a dynamic environment through a series of trials and errors [3]. Unlike its counterparts, supervised and unsupervised learning that rely on a static dataset, reinforcement learning has an interactive element focusing on decision-making (Figure 1).

# Components and Terms of Reinforcement Learning
What is an agent? What is an environment? Before we explain how the computer program can learn, let us introduce several components and terms central to further understanding:
    * Agent is a sort of bot that chooses and takes action.
    * Environment is the world in which the agent operates. For example, in chess, the chess board and pieces are the environment, and a player-bot is an agent.
    * Action is any possible interactions that the agent can choose to carry out.
    * State is information about the current condition of the environment.
    * Reward is a point given to the agent as an outcome based on its action.
    * Policy is a strategy, guiding the agent’s actions in response to the environmental state.
    * Episode is a sequence of interactions from the initial state to a terminal state (e.g., one chess game).

# Cyclic Process of Reinforcement Learning
Whoo! Now, we should be ready!
At its core, reinforcement learning is a cyclic process. The agent observes the state of the environment. It has a set of available actions. It chooses one from the set. The action earns an agent rewards, which can be positive, negative, or zero. It updates its policy based on the reward. The action changes the environment, creating a new state, and the cycle repeats (Figure 2) [4].
The primary goal for the agent is to maximize cumulative rewards over time. The agent is not told which action to take but only the impact of the action in the form of rewards. Therefore, with each iteration, the agent revisits its decision-making process or policy, learning to prioritize actions that lead to favorable outcomes. As it earns more experience, the agent gradually is better at the task.

# Algorithms in Reinforcement Learning
There are various algorithms and methodologies employed in reinforcement learning. They could generally be divided into two groups:
    * Value-Based Methods. These methods estimate a value function. It is a function that returns how good or beneficial it is for an agent to be in a given state [4]. One of the value functions is Q-function. It estimates the expected cumulative reward for a given action a in a specific state s [4].
    * Policy-Based Methods. These methods optimize and learn the policy directly. They do not rely on a value function. While policy is a strategy, it is represented by a mapping from the state to the probability distribution over all actions [4].
In more complex settings, where the action space is large or the environment is sophisticated, using these methods gets complicated. It gets difficult to remember the relationships between all possible states and all possible actions. To solve this problem, we can utilize deep neural networks (NN), making it deep reinforcement learning (DRL).
For example,
    * As a value-based method, Deep Q-Network (DQN) applies deep neural network to approximate the Q-function [5].
    * As a policy-based method, Proximal Policy Optimization (PPO) uses deep neural networks to update the policy [6].
Here, at Sentience, we have explored both DQN and PPO to develop smart bots for our game, South Pole Bebop.

# Conclusion
Reinforcement learning is a powerful tool that can produce great results once harnessed. In the next blog, we will take a practical approach. We will share our experience working on a deep reinforcement learning model for SPB. We will also include the resources, libraries, and tools the readers can use to apply reinforcement learning in their use case. So stay tuned for upcoming posts, where we will unravel AI together!
Nargiz Askarbekkyzy, the author, is an AI researcher at Sentience who graduated from KAIST with a major in computer science. She has previously worked in the KAIST lab where she designed and developed retargeting/redirection technology for VR apps. Currently, she is researching and developing deep reinforcement learning models to enhance gaming experiences at Sentience.
Learn More About TentuPlay
Visit “South Pole Bebop” Steam Page
References:
[1] “AlphaGo,” Google DeepMind. https://deepmind.google/technologies/alphago/
[2] “OpenAI Five defeats Dota 2 world champions,” openai.com. https://openai.com/research/openai-five-defeats-dota-2-world-champions
[3] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement Learning: A Survey,” Journal of Artificial Intelligence Research, 1996, doi: https://doi.org/10.48550/arXiv.cs/9605103.
[4] R. Sutton and A. Barto, “Reinforcement Learning An Introduction second edition.” Available: https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf
[5] V. Mnih et al., “Playing Atari with Deep Reinforcement Learning,” arXiv.org, 2013. https://arxiv.org/abs/1312.5602
[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv.org, 2017. https://arxiv.org/abs/1707.06347",Good
26,Deep Reinforcement Learning: The Algorithms for Game Dev,https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7,TentuPlay,117,56,1,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/pdf/1312.5602,0.5042735042735043,48.6,"# Deep Reinforcement Learning: The Algorithms for Game Dev

# Intro
In 2016, Google DeepMind presented a computer program AlphaGo [1], which crashed the world champion in the notoriously challenging game — Go. Just three years later, OpenAI’s team consisting of artificial intelligence (AI) players in Dota 2, called OpenAI Five, beat The International 2018 (Dota 2 world tournament) champions [2]. These victories did not stem from a stroke of pure luck. They resulted from a sophisticated approach rooted in the principles of reinforcement learning. This is the first post in our blog series dedicated to reinforcement learnings, and today, we will delve into reinforcement learning realm, explaining its core principles and intricacies.
Reinforcement learning is a paradigm within the broader scope of machine learning. It is a task where an agent learns a behavior by interacting with a dynamic environment through a series of trials and errors [3]. Unlike its counterparts, supervised and unsupervised learning that rely on a static dataset, reinforcement learning has an interactive element focusing on decision-making (Figure 1).

# Components and Terms of Reinforcement Learning
What is an agent? What is an environment? Before we explain how the computer program can learn, let us introduce several components and terms central to further understanding:
    * Agent is a sort of bot that chooses and takes action.
    * Environment is the world in which the agent operates. For example, in chess, the chess board and pieces are the environment, and a player-bot is an agent.
    * Action is any possible interactions that the agent can choose to carry out.
    * State is information about the current condition of the environment.
    * Reward is a point given to the agent as an outcome based on its action.
    * Policy is a strategy, guiding the agent’s actions in response to the environmental state.
    * Episode is a sequence of interactions from the initial state to a terminal state (e.g., one chess game).

# Cyclic Process of Reinforcement Learning
Whoo! Now, we should be ready!
At its core, reinforcement learning is a cyclic process. The agent observes the state of the environment. It has a set of available actions. It chooses one from the set. The action earns an agent rewards, which can be positive, negative, or zero. It updates its policy based on the reward. The action changes the environment, creating a new state, and the cycle repeats (Figure 2) [4].
The primary goal for the agent is to maximize cumulative rewards over time. The agent is not told which action to take but only the impact of the action in the form of rewards. Therefore, with each iteration, the agent revisits its decision-making process or policy, learning to prioritize actions that lead to favorable outcomes. As it earns more experience, the agent gradually is better at the task.

# Algorithms in Reinforcement Learning
There are various algorithms and methodologies employed in reinforcement learning. They could generally be divided into two groups:
    * Value-Based Methods. These methods estimate a value function. It is a function that returns how good or beneficial it is for an agent to be in a given state [4]. One of the value functions is Q-function. It estimates the expected cumulative reward for a given action a in a specific state s [4].
    * Policy-Based Methods. These methods optimize and learn the policy directly. They do not rely on a value function. While policy is a strategy, it is represented by a mapping from the state to the probability distribution over all actions [4].
In more complex settings, where the action space is large or the environment is sophisticated, using these methods gets complicated. It gets difficult to remember the relationships between all possible states and all possible actions. To solve this problem, we can utilize deep neural networks (NN), making it deep reinforcement learning (DRL).
For example,
    * As a value-based method, Deep Q-Network (DQN) applies deep neural network to approximate the Q-function [5].
    * As a policy-based method, Proximal Policy Optimization (PPO) uses deep neural networks to update the policy [6].
Here, at Sentience, we have explored both DQN and PPO to develop smart bots for our game, South Pole Bebop.

# Conclusion
Reinforcement learning is a powerful tool that can produce great results once harnessed. In the next blog, we will take a practical approach. We will share our experience working on a deep reinforcement learning model for SPB. We will also include the resources, libraries, and tools the readers can use to apply reinforcement learning in their use case. So stay tuned for upcoming posts, where we will unravel AI together!
Nargiz Askarbekkyzy, the author, is an AI researcher at Sentience who graduated from KAIST with a major in computer science. She has previously worked in the KAIST lab where she designed and developed retargeting/redirection technology for VR apps. Currently, she is researching and developing deep reinforcement learning models to enhance gaming experiences at Sentience.
Learn More About TentuPlay
Visit “South Pole Bebop” Steam Page
References:
[1] “AlphaGo,” Google DeepMind. https://deepmind.google/technologies/alphago/
[2] “OpenAI Five defeats Dota 2 world champions,” openai.com. https://openai.com/research/openai-five-defeats-dota-2-world-champions
[3] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement Learning: A Survey,” Journal of Artificial Intelligence Research, 1996, doi: https://doi.org/10.48550/arXiv.cs/9605103.
[4] R. Sutton and A. Barto, “Reinforcement Learning An Introduction second edition.” Available: https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf
[5] V. Mnih et al., “Playing Atari with Deep Reinforcement Learning,” arXiv.org, 2013. https://arxiv.org/abs/1312.5602
[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv.org, 2017. https://arxiv.org/abs/1707.06347",Good
27,Deep Reinforcement Learning: The Algorithms for Game Dev,https://medium.com/@tentuplay/deep-reinforcement-learning-ec9a951f47b7,TentuPlay,117,56,1,Proximal Policy Optimization Algorithms,https://arxiv.org/pdf/1707.06347,0.5042735042735043,48.6,"# Deep Reinforcement Learning: The Algorithms for Game Dev

# Intro
In 2016, Google DeepMind presented a computer program AlphaGo [1], which crashed the world champion in the notoriously challenging game — Go. Just three years later, OpenAI’s team consisting of artificial intelligence (AI) players in Dota 2, called OpenAI Five, beat The International 2018 (Dota 2 world tournament) champions [2]. These victories did not stem from a stroke of pure luck. They resulted from a sophisticated approach rooted in the principles of reinforcement learning. This is the first post in our blog series dedicated to reinforcement learnings, and today, we will delve into reinforcement learning realm, explaining its core principles and intricacies.
Reinforcement learning is a paradigm within the broader scope of machine learning. It is a task where an agent learns a behavior by interacting with a dynamic environment through a series of trials and errors [3]. Unlike its counterparts, supervised and unsupervised learning that rely on a static dataset, reinforcement learning has an interactive element focusing on decision-making (Figure 1).

# Components and Terms of Reinforcement Learning
What is an agent? What is an environment? Before we explain how the computer program can learn, let us introduce several components and terms central to further understanding:
    * Agent is a sort of bot that chooses and takes action.
    * Environment is the world in which the agent operates. For example, in chess, the chess board and pieces are the environment, and a player-bot is an agent.
    * Action is any possible interactions that the agent can choose to carry out.
    * State is information about the current condition of the environment.
    * Reward is a point given to the agent as an outcome based on its action.
    * Policy is a strategy, guiding the agent’s actions in response to the environmental state.
    * Episode is a sequence of interactions from the initial state to a terminal state (e.g., one chess game).

# Cyclic Process of Reinforcement Learning
Whoo! Now, we should be ready!
At its core, reinforcement learning is a cyclic process. The agent observes the state of the environment. It has a set of available actions. It chooses one from the set. The action earns an agent rewards, which can be positive, negative, or zero. It updates its policy based on the reward. The action changes the environment, creating a new state, and the cycle repeats (Figure 2) [4].
The primary goal for the agent is to maximize cumulative rewards over time. The agent is not told which action to take but only the impact of the action in the form of rewards. Therefore, with each iteration, the agent revisits its decision-making process or policy, learning to prioritize actions that lead to favorable outcomes. As it earns more experience, the agent gradually is better at the task.

# Algorithms in Reinforcement Learning
There are various algorithms and methodologies employed in reinforcement learning. They could generally be divided into two groups:
    * Value-Based Methods. These methods estimate a value function. It is a function that returns how good or beneficial it is for an agent to be in a given state [4]. One of the value functions is Q-function. It estimates the expected cumulative reward for a given action a in a specific state s [4].
    * Policy-Based Methods. These methods optimize and learn the policy directly. They do not rely on a value function. While policy is a strategy, it is represented by a mapping from the state to the probability distribution over all actions [4].
In more complex settings, where the action space is large or the environment is sophisticated, using these methods gets complicated. It gets difficult to remember the relationships between all possible states and all possible actions. To solve this problem, we can utilize deep neural networks (NN), making it deep reinforcement learning (DRL).
For example,
    * As a value-based method, Deep Q-Network (DQN) applies deep neural network to approximate the Q-function [5].
    * As a policy-based method, Proximal Policy Optimization (PPO) uses deep neural networks to update the policy [6].
Here, at Sentience, we have explored both DQN and PPO to develop smart bots for our game, South Pole Bebop.

# Conclusion
Reinforcement learning is a powerful tool that can produce great results once harnessed. In the next blog, we will take a practical approach. We will share our experience working on a deep reinforcement learning model for SPB. We will also include the resources, libraries, and tools the readers can use to apply reinforcement learning in their use case. So stay tuned for upcoming posts, where we will unravel AI together!
Nargiz Askarbekkyzy, the author, is an AI researcher at Sentience who graduated from KAIST with a major in computer science. She has previously worked in the KAIST lab where she designed and developed retargeting/redirection technology for VR apps. Currently, she is researching and developing deep reinforcement learning models to enhance gaming experiences at Sentience.
Learn More About TentuPlay
Visit “South Pole Bebop” Steam Page
References:
[1] “AlphaGo,” Google DeepMind. https://deepmind.google/technologies/alphago/
[2] “OpenAI Five defeats Dota 2 world champions,” openai.com. https://openai.com/research/openai-five-defeats-dota-2-world-champions
[3] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement Learning: A Survey,” Journal of Artificial Intelligence Research, 1996, doi: https://doi.org/10.48550/arXiv.cs/9605103.
[4] R. Sutton and A. Barto, “Reinforcement Learning An Introduction second edition.” Available: https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf
[5] V. Mnih et al., “Playing Atari with Deep Reinforcement Learning,” arXiv.org, 2013. https://arxiv.org/abs/1312.5602
[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv.org, 2017. https://arxiv.org/abs/1707.06347",Good
28,Towards Reasoning,https://medium.com/@saptarshichaudhuri/towards-reasoning-91982d2a9f74,Saptarshi Chaudhuri,127,461,1,GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models,https://arxiv.org/pdf/2410.05229?source=post_page-----91982d2a9f74---------------------------------------,3.6535433070866143,100.0,"# Towards Reasoning
Glorified next token prediction has shown tremendous promise for AI in recent years, with several feats of complex reasoning especially in mathematics and coding.
But what if that weren’t true logical reasoning? What if these large language models were not really reasoning but doing sophisticated pattern matching (after having trained on the entire internet and on curated question answers from thousands of humans <cough>RLHF</cough>)
And the obvious second order question — what if pattern matching was a sufficient substitute to human reasoning? I mean do we really need to invent human reasoning if hyper-scaling models with memorization and pattern matching can do the job robustly?
A recent paper from researchers at Apple seemed to have made significant progress in answering the above questions by doing one of the more boring and arduous tasks — building benchmark datasets that may be free from distribution shift and data leakage and thinking through failure test cases for reasoning.

# Mathematical reasoning by LLMs is fragile at best
I won’t review the entire paper here.
This is a highly readable and straightforward paper — no math, no LLM or ML pre-reqs needed beyond just the basics (a curious reader without the basic knowledge can also follow through):

## GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models

### Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities…
arxiv.org
That said, some interesting hypothesis and conclusions that really stood out to me:
1. Given LLMs get trained on all of internet + thousands of hours of human curation (RLHF), may be existing benchmark data gets leaked into model training more often. In fact GSMK8K (a popular benchmark dataset to test math reasoning) was used 3 years back to test GPT-3. It won’t be a surprise if some of these questions unintentionally leaked into the descendant model versions (open source or otherwise)
2. New data with just small changes (such as “names” and “numerical values”) from the original GSM-8K dataset resulted in significant performance drop across all current generation of LLMs. In the words of the authors, “Would a grade-school student’s math test score vary by ~10% if we only changed the names in the math word problems?”
3. Increasing complexity of the questions by just one additional inference step introduced significant variability in LLM predictions — indicating the fragility of current LLM reasoning.
4. LLMs were unable to identify and ignore irrelevant information present in the word problems, indicating a lack of understanding of the mathematical concepts.
One may be tempted to say we humans also commit the same mistakes. But this is a defensive argument which will neither advance the field, nor is it an apt comparison. We don’t usually spend a billion dollars training a single human baby to solve math word problems.

# My personal reflections
In the last 48 hours since the release of this paper, reactions in twitter have ranged from the pessimistic “valuation of companies in this field will come crashing down soon” all the way to the hyper-nihilistic “reasoning is an illusion and humans don’t reason either”!
But may be it’s about time we step back and ask ourselves is there a more reasonable middle path here? Rather than hyper-reacting at the facts, can we also learn something about the scientific method and attitude these researchers took to arrive at their conclusions?
In a world of AI hyper-optimists, doomers and pessimists, these authors stepped back and went to the very basics of ML model development. They asked questions that any responsible ML practitioner should ask:
1. Can there be data leakage? — Could the data from some of the popular reasoning benchmark datasets have leaked into the model training for these hyperscaled LLMs? Highly likely!
2. How would the models behave in the face of distribution shifts? — By simply creating variations of existing benchmark datasets, the researchers were able to conclude that current LLMs may not be as robust as we thought.
3. Can we prioritize the hard and grunt work over the fancy? — Yes, creating benchmark datasets, or thinking of failure test cases don’t sound fancy in the world of Chain-of-thought prompting and agentic architectures. But the authors still did it.
So maybe we too can step back and take a more reasonable approach towards grokking the findings of this paper. Here’s my best attempt at the same:
1. No reproducibility yet: At the time of writing this article (10/12/2024), I am yet to see the benchmark dataset and any associated code. So, while the experimental set ups appear sound, I cannot reproduce the results for myself yet.
2. Didn’t test Chain-of-thought decoding: The authors evaluated Chain-of-Thought (CoT) prompting with 8-shots with greedy decoding. However, alternate decoding paths have started to show more promise at extracting inherent reasoning from LLMs. https://arxiv.org/abs/2402.10200
3. Humans do reason: Assuming the findings of the paper are accurate, I don’t buy the nihilistic argument that humans don’t reason either and that reasoning is an illusion. Try to solve an unseen brain teaser for a change (like really try)
4. Transformers are useful: Say what you want — I am using copilot everyday and it’s useful in it’s current state. When it comes to reasoning, I will carry my weight. I only want my AI assistant to be useful.",Excellent
29,Towards Reasoning,https://medium.com/@saptarshichaudhuri/towards-reasoning-91982d2a9f74,Saptarshi Chaudhuri,127,461,1,Chain-of-Thought Reasoning Without Prompting,https://arxiv.org/pdf/2402.10200,3.6535433070866143,100.0,"# Towards Reasoning
Glorified next token prediction has shown tremendous promise for AI in recent years, with several feats of complex reasoning especially in mathematics and coding.
But what if that weren’t true logical reasoning? What if these large language models were not really reasoning but doing sophisticated pattern matching (after having trained on the entire internet and on curated question answers from thousands of humans <cough>RLHF</cough>)
And the obvious second order question — what if pattern matching was a sufficient substitute to human reasoning? I mean do we really need to invent human reasoning if hyper-scaling models with memorization and pattern matching can do the job robustly?
A recent paper from researchers at Apple seemed to have made significant progress in answering the above questions by doing one of the more boring and arduous tasks — building benchmark datasets that may be free from distribution shift and data leakage and thinking through failure test cases for reasoning.

# Mathematical reasoning by LLMs is fragile at best
I won’t review the entire paper here.
This is a highly readable and straightforward paper — no math, no LLM or ML pre-reqs needed beyond just the basics (a curious reader without the basic knowledge can also follow through):

## GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models

### Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities…
arxiv.org
That said, some interesting hypothesis and conclusions that really stood out to me:
1. Given LLMs get trained on all of internet + thousands of hours of human curation (RLHF), may be existing benchmark data gets leaked into model training more often. In fact GSMK8K (a popular benchmark dataset to test math reasoning) was used 3 years back to test GPT-3. It won’t be a surprise if some of these questions unintentionally leaked into the descendant model versions (open source or otherwise)
2. New data with just small changes (such as “names” and “numerical values”) from the original GSM-8K dataset resulted in significant performance drop across all current generation of LLMs. In the words of the authors, “Would a grade-school student’s math test score vary by ~10% if we only changed the names in the math word problems?”
3. Increasing complexity of the questions by just one additional inference step introduced significant variability in LLM predictions — indicating the fragility of current LLM reasoning.
4. LLMs were unable to identify and ignore irrelevant information present in the word problems, indicating a lack of understanding of the mathematical concepts.
One may be tempted to say we humans also commit the same mistakes. But this is a defensive argument which will neither advance the field, nor is it an apt comparison. We don’t usually spend a billion dollars training a single human baby to solve math word problems.

# My personal reflections
In the last 48 hours since the release of this paper, reactions in twitter have ranged from the pessimistic “valuation of companies in this field will come crashing down soon” all the way to the hyper-nihilistic “reasoning is an illusion and humans don’t reason either”!
But may be it’s about time we step back and ask ourselves is there a more reasonable middle path here? Rather than hyper-reacting at the facts, can we also learn something about the scientific method and attitude these researchers took to arrive at their conclusions?
In a world of AI hyper-optimists, doomers and pessimists, these authors stepped back and went to the very basics of ML model development. They asked questions that any responsible ML practitioner should ask:
1. Can there be data leakage? — Could the data from some of the popular reasoning benchmark datasets have leaked into the model training for these hyperscaled LLMs? Highly likely!
2. How would the models behave in the face of distribution shifts? — By simply creating variations of existing benchmark datasets, the researchers were able to conclude that current LLMs may not be as robust as we thought.
3. Can we prioritize the hard and grunt work over the fancy? — Yes, creating benchmark datasets, or thinking of failure test cases don’t sound fancy in the world of Chain-of-thought prompting and agentic architectures. But the authors still did it.
So maybe we too can step back and take a more reasonable approach towards grokking the findings of this paper. Here’s my best attempt at the same:
1. No reproducibility yet: At the time of writing this article (10/12/2024), I am yet to see the benchmark dataset and any associated code. So, while the experimental set ups appear sound, I cannot reproduce the results for myself yet.
2. Didn’t test Chain-of-thought decoding: The authors evaluated Chain-of-Thought (CoT) prompting with 8-shots with greedy decoding. However, alternate decoding paths have started to show more promise at extracting inherent reasoning from LLMs. https://arxiv.org/abs/2402.10200
3. Humans do reason: Assuming the findings of the paper are accurate, I don’t buy the nihilistic argument that humans don’t reason either and that reasoning is an illusion. Try to solve an unseen brain teaser for a change (like really try)
4. Transformers are useful: Say what you want — I am using copilot everyday and it’s useful in it’s current state. When it comes to reasoning, I will carry my weight. I only want my AI assistant to be useful.",Excellent
30,How reinforcement learning affects human behavior?,https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93,Eve Pardi,81,81,1,Artificial Intelligence for the Metaverse: A Survey,https://arxiv.org/pdf/2202.10336,1.037037037037037,100.0,"# How reinforcement learning affects human behavior?
Reinforcement learning (RL) is a machine learning (ML) technique that trains the policy of the agent, who “must discover which actions yield the most reward by trying them (Sutton et al., 2018)” in a given situation. During trial-and-error iterations the agent evaluates the given feedback after each interaction, where the feedback can be a reward or a punishment, and then uses this experience “to solve new tasks efficiently (Tessler et al., 2016)”.
While in RL decisions are driven by rewards, in games it could be provided as a high score or a new level. The agent’s policies improve under the pressure of a game situation, such as learning how to react when attacked or how to achieve a specific goal.
RL models can be classified based on the number of agents in an environment or players taught by the algorithm: single-agent RL deals with one agent in a situation, whereas in a multi-agent environment, several agents act simultaneously, which affects the learning process of other agents, these agents must act independently but must learn to interact with other agents. This is an immensely difficult problem because with co-adapting agents the environment is constantly changing.
In video games, agents have typically been trained to win against humans, like the famous chess agent AlphaZero, which was trained by self-learning, that is, by playing against itself (McGrath et al., 2021). The purpose of games is to develop one’s physical and mental abilities, and a competitive situation would certainly improve strategic thinking, dexterity, and other abilities (Togelius et al., 2018).
Project Paidia (Project Paidia: a Microsoft Research & Ninja Theory Collaboration, 2022) team of Microsoft Research aims to provide collaborative agent algorithms that support rather than hinder the player during gameplay, and these models are available to developers to improve their own games. Read more about Project Paidia in this article. This technology is used in various existing games, including Microsoft’s Halo Infinite (Halo — Official site (en), 2022), players choose a companion to help in a particular situation, with the goal of improving gameplay and people’s success.
While Halo’s agents enhance the game experience for experienced, they can degrade it for less skilled players. Algorithms of both opponents and companions have a serious impact on the gameplay, causing various disorders if the game is too difficult, anti-social behavior, and even addiction if players consistently achieve high scores (Dimita et al., 2021).
Advances in RL allow developers to provide a more personalized gaming experience for their players, but increasing capabilities also create higher expectations, which can lead to game performance degradation — either because the game is too easy or too hard, or because companies don’t know how to deal with increasing demands, negative feedback, and the loss of employees due to deterioration in mental or physical health.
Serious Gaming (SG) forces players to interact with real-world environments, and such games aim to teach citizens how to use resources efficiently while examining human behavior from social, ethical, and organizational perspectives (Carvalho et al., 2015).
One approach is suggested in (Predescu et al., 2021), a location-based game in which players or citizens report issues about the water infrastructure such as leaks, pollution, or service outages by sharing data from their mobile devices’ integrated sensors, including pinpointing their exact location. After the record is evaluated and validated, the details of the problem appear on the world map, and the reporter receives a reward: the more accurate and realistic the data, the greater the reward for the player. This motivates citizens to actively participate and interact, resulting in accurate experimental results for water use and infrastructure issues, even though low-grade sensors may degrade the quality of the reported information.
Integrating pervasive computing and reinforcement learning results in serious privacy, security, and data quality challenges. As the vast amount of data collected by sensors and human behavior are evaluated along with the reported information to reward players, the game must be reliable and transparent. Blockchain technology improves the execution of transactions, as well as the secure, immutable storage, and validation of the information provided (Predescu et al., 2021). Thanks to smart contracts, stored records are available to companies in real-time, monitoring and analyzing this data makes water distribution more efficient (Blockchain and the water industry — Smart Contracts — EMA, 2018). While citizens may choose to report information anonymously, location information must be provided. The private blockchain is applied to solve data privacy issues (Yang et al., 2019).
We have discussed how enhancing the personalized gaming experience affects the performance of players, improving their physical and mental abilities. However, Smart Games have simpler graphics and aim to successfully solve or improve environmental problems by teaching citizens how to use resources efficiently and the importance of cooperation in solving serious issues related to resource management.
References
Blockchain and the water industry — Smart Contracts — EMA (2018) Available at: https://www.ema-inc.com/news-insights/2018/10/blockchain-and-the-water-industry-smart-contracts/ (Accessed: 14 November 2022)
Carvalho, M.B. et al. (2015) ‘An activity theory-based model for serious games analysis and conceptual design’, Computers and Education, 87, pp. 166–181. Available at: https://doi.org/10.1016/j.compedu.2015.03.023.
Dimita, G. et al. (2021) ‘Wellbeing, mental health, and video games: A shifting narrative from player to industry perspective’, Interactive Entertainment Law Review. Edward Elgar Publishing Ltd., pp. 85–86. Available at: https://doi.org/10.4337/ielr.2021.02.00.
Halo — Official site (en) (2022) Available at: https://www.halowaypoint.com/ (Accessed: 31 October 2022)
Huynh-The, T. et al. (2022) ‘Artificial Intelligence for the Metaverse: A Survey’. Available at: http://arxiv.org/abs/2202.10336.
Jameel, F. et al. (2020) ‘Reinforcement learning in blockchain-enabled IIoT networks: A survey of recent advances and open challenges’, Sustainability (Switzerland). MDPI. Available at: https://doi.org/10.3390/su12125161.
Julian Togelius; Jesper Juul; Geoffrey Long; William Uricchio; Mia Consalvo, “1 In the Beginning of AI, There Were Games,” in Playing Smart: On Games, Intelligence, and Artificial Intelligence, MIT Press, 2018, pp.1–10.
McGrath, T. et al. (2021) Acquisition of Chess Knowledge in AlphaZero. Available at: http://arxiv.org/abs/2111.09259.
Project Paidia: a Microsoft Research & Ninja Theory Collaboration (2022) Available at: https://www.microsoft.com/en-us/research/project/project-paidia/ (Accessed: 31 October 2022)
Richard S. Sutton and Andrew G. Barto (2018) Reinforcement Learning, Second Edition : An Introduction. Cambridge, Massachusetts: Bradford Books (Adaptive Computation and Machine Learning).
Predescu, A. et al. (2021) ‘A serious gaming approach for crowdsensing in urban water infrastructure with blockchain support’, Applied Sciences (Switzerland), 11(4), pp. 1–32. Available at: https://doi.org/10.3390/app11041449.
Tessler, C. et al. (2016) A Deep Hierarchical Approach to Lifelong Learning in Minecraft. Available at: http://arxiv.org/abs/1604.07255.
Yang, M. et al. (2019) ‘A blockchain-based location privacy-preserving crowdsensing system’, Future Generation Computer Systems, 94, pp. 408–418. Available at: https://doi.org/10.1016/j.future.2018.11.046.",Excellent
31,How reinforcement learning affects human behavior?,https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93,Eve Pardi,81,81,1,Acquisition of Chess Knowledge in AlphaZero,https://arxiv.org/pdf/2111.09259,1.037037037037037,100.0,"# How reinforcement learning affects human behavior?
Reinforcement learning (RL) is a machine learning (ML) technique that trains the policy of the agent, who “must discover which actions yield the most reward by trying them (Sutton et al., 2018)” in a given situation. During trial-and-error iterations the agent evaluates the given feedback after each interaction, where the feedback can be a reward or a punishment, and then uses this experience “to solve new tasks efficiently (Tessler et al., 2016)”.
While in RL decisions are driven by rewards, in games it could be provided as a high score or a new level. The agent’s policies improve under the pressure of a game situation, such as learning how to react when attacked or how to achieve a specific goal.
RL models can be classified based on the number of agents in an environment or players taught by the algorithm: single-agent RL deals with one agent in a situation, whereas in a multi-agent environment, several agents act simultaneously, which affects the learning process of other agents, these agents must act independently but must learn to interact with other agents. This is an immensely difficult problem because with co-adapting agents the environment is constantly changing.
In video games, agents have typically been trained to win against humans, like the famous chess agent AlphaZero, which was trained by self-learning, that is, by playing against itself (McGrath et al., 2021). The purpose of games is to develop one’s physical and mental abilities, and a competitive situation would certainly improve strategic thinking, dexterity, and other abilities (Togelius et al., 2018).
Project Paidia (Project Paidia: a Microsoft Research & Ninja Theory Collaboration, 2022) team of Microsoft Research aims to provide collaborative agent algorithms that support rather than hinder the player during gameplay, and these models are available to developers to improve their own games. Read more about Project Paidia in this article. This technology is used in various existing games, including Microsoft’s Halo Infinite (Halo — Official site (en), 2022), players choose a companion to help in a particular situation, with the goal of improving gameplay and people’s success.
While Halo’s agents enhance the game experience for experienced, they can degrade it for less skilled players. Algorithms of both opponents and companions have a serious impact on the gameplay, causing various disorders if the game is too difficult, anti-social behavior, and even addiction if players consistently achieve high scores (Dimita et al., 2021).
Advances in RL allow developers to provide a more personalized gaming experience for their players, but increasing capabilities also create higher expectations, which can lead to game performance degradation — either because the game is too easy or too hard, or because companies don’t know how to deal with increasing demands, negative feedback, and the loss of employees due to deterioration in mental or physical health.
Serious Gaming (SG) forces players to interact with real-world environments, and such games aim to teach citizens how to use resources efficiently while examining human behavior from social, ethical, and organizational perspectives (Carvalho et al., 2015).
One approach is suggested in (Predescu et al., 2021), a location-based game in which players or citizens report issues about the water infrastructure such as leaks, pollution, or service outages by sharing data from their mobile devices’ integrated sensors, including pinpointing their exact location. After the record is evaluated and validated, the details of the problem appear on the world map, and the reporter receives a reward: the more accurate and realistic the data, the greater the reward for the player. This motivates citizens to actively participate and interact, resulting in accurate experimental results for water use and infrastructure issues, even though low-grade sensors may degrade the quality of the reported information.
Integrating pervasive computing and reinforcement learning results in serious privacy, security, and data quality challenges. As the vast amount of data collected by sensors and human behavior are evaluated along with the reported information to reward players, the game must be reliable and transparent. Blockchain technology improves the execution of transactions, as well as the secure, immutable storage, and validation of the information provided (Predescu et al., 2021). Thanks to smart contracts, stored records are available to companies in real-time, monitoring and analyzing this data makes water distribution more efficient (Blockchain and the water industry — Smart Contracts — EMA, 2018). While citizens may choose to report information anonymously, location information must be provided. The private blockchain is applied to solve data privacy issues (Yang et al., 2019).
We have discussed how enhancing the personalized gaming experience affects the performance of players, improving their physical and mental abilities. However, Smart Games have simpler graphics and aim to successfully solve or improve environmental problems by teaching citizens how to use resources efficiently and the importance of cooperation in solving serious issues related to resource management.
References
Blockchain and the water industry — Smart Contracts — EMA (2018) Available at: https://www.ema-inc.com/news-insights/2018/10/blockchain-and-the-water-industry-smart-contracts/ (Accessed: 14 November 2022)
Carvalho, M.B. et al. (2015) ‘An activity theory-based model for serious games analysis and conceptual design’, Computers and Education, 87, pp. 166–181. Available at: https://doi.org/10.1016/j.compedu.2015.03.023.
Dimita, G. et al. (2021) ‘Wellbeing, mental health, and video games: A shifting narrative from player to industry perspective’, Interactive Entertainment Law Review. Edward Elgar Publishing Ltd., pp. 85–86. Available at: https://doi.org/10.4337/ielr.2021.02.00.
Halo — Official site (en) (2022) Available at: https://www.halowaypoint.com/ (Accessed: 31 October 2022)
Huynh-The, T. et al. (2022) ‘Artificial Intelligence for the Metaverse: A Survey’. Available at: http://arxiv.org/abs/2202.10336.
Jameel, F. et al. (2020) ‘Reinforcement learning in blockchain-enabled IIoT networks: A survey of recent advances and open challenges’, Sustainability (Switzerland). MDPI. Available at: https://doi.org/10.3390/su12125161.
Julian Togelius; Jesper Juul; Geoffrey Long; William Uricchio; Mia Consalvo, “1 In the Beginning of AI, There Were Games,” in Playing Smart: On Games, Intelligence, and Artificial Intelligence, MIT Press, 2018, pp.1–10.
McGrath, T. et al. (2021) Acquisition of Chess Knowledge in AlphaZero. Available at: http://arxiv.org/abs/2111.09259.
Project Paidia: a Microsoft Research & Ninja Theory Collaboration (2022) Available at: https://www.microsoft.com/en-us/research/project/project-paidia/ (Accessed: 31 October 2022)
Richard S. Sutton and Andrew G. Barto (2018) Reinforcement Learning, Second Edition : An Introduction. Cambridge, Massachusetts: Bradford Books (Adaptive Computation and Machine Learning).
Predescu, A. et al. (2021) ‘A serious gaming approach for crowdsensing in urban water infrastructure with blockchain support’, Applied Sciences (Switzerland), 11(4), pp. 1–32. Available at: https://doi.org/10.3390/app11041449.
Tessler, C. et al. (2016) A Deep Hierarchical Approach to Lifelong Learning in Minecraft. Available at: http://arxiv.org/abs/1604.07255.
Yang, M. et al. (2019) ‘A blockchain-based location privacy-preserving crowdsensing system’, Future Generation Computer Systems, 94, pp. 408–418. Available at: https://doi.org/10.1016/j.future.2018.11.046.",Excellent
32,How reinforcement learning affects human behavior?,https://medium.com/@evepardi/how-reinforcement-learning-affects-human-behavior-46166dad5e93,Eve Pardi,81,81,1,A Deep Hierarchical Approach to Lifelong Learning in Minecraft,https://arxiv.org/pdf/1604.07255,1.037037037037037,100.0,"# How reinforcement learning affects human behavior?
Reinforcement learning (RL) is a machine learning (ML) technique that trains the policy of the agent, who “must discover which actions yield the most reward by trying them (Sutton et al., 2018)” in a given situation. During trial-and-error iterations the agent evaluates the given feedback after each interaction, where the feedback can be a reward or a punishment, and then uses this experience “to solve new tasks efficiently (Tessler et al., 2016)”.
While in RL decisions are driven by rewards, in games it could be provided as a high score or a new level. The agent’s policies improve under the pressure of a game situation, such as learning how to react when attacked or how to achieve a specific goal.
RL models can be classified based on the number of agents in an environment or players taught by the algorithm: single-agent RL deals with one agent in a situation, whereas in a multi-agent environment, several agents act simultaneously, which affects the learning process of other agents, these agents must act independently but must learn to interact with other agents. This is an immensely difficult problem because with co-adapting agents the environment is constantly changing.
In video games, agents have typically been trained to win against humans, like the famous chess agent AlphaZero, which was trained by self-learning, that is, by playing against itself (McGrath et al., 2021). The purpose of games is to develop one’s physical and mental abilities, and a competitive situation would certainly improve strategic thinking, dexterity, and other abilities (Togelius et al., 2018).
Project Paidia (Project Paidia: a Microsoft Research & Ninja Theory Collaboration, 2022) team of Microsoft Research aims to provide collaborative agent algorithms that support rather than hinder the player during gameplay, and these models are available to developers to improve their own games. Read more about Project Paidia in this article. This technology is used in various existing games, including Microsoft’s Halo Infinite (Halo — Official site (en), 2022), players choose a companion to help in a particular situation, with the goal of improving gameplay and people’s success.
While Halo’s agents enhance the game experience for experienced, they can degrade it for less skilled players. Algorithms of both opponents and companions have a serious impact on the gameplay, causing various disorders if the game is too difficult, anti-social behavior, and even addiction if players consistently achieve high scores (Dimita et al., 2021).
Advances in RL allow developers to provide a more personalized gaming experience for their players, but increasing capabilities also create higher expectations, which can lead to game performance degradation — either because the game is too easy or too hard, or because companies don’t know how to deal with increasing demands, negative feedback, and the loss of employees due to deterioration in mental or physical health.
Serious Gaming (SG) forces players to interact with real-world environments, and such games aim to teach citizens how to use resources efficiently while examining human behavior from social, ethical, and organizational perspectives (Carvalho et al., 2015).
One approach is suggested in (Predescu et al., 2021), a location-based game in which players or citizens report issues about the water infrastructure such as leaks, pollution, or service outages by sharing data from their mobile devices’ integrated sensors, including pinpointing their exact location. After the record is evaluated and validated, the details of the problem appear on the world map, and the reporter receives a reward: the more accurate and realistic the data, the greater the reward for the player. This motivates citizens to actively participate and interact, resulting in accurate experimental results for water use and infrastructure issues, even though low-grade sensors may degrade the quality of the reported information.
Integrating pervasive computing and reinforcement learning results in serious privacy, security, and data quality challenges. As the vast amount of data collected by sensors and human behavior are evaluated along with the reported information to reward players, the game must be reliable and transparent. Blockchain technology improves the execution of transactions, as well as the secure, immutable storage, and validation of the information provided (Predescu et al., 2021). Thanks to smart contracts, stored records are available to companies in real-time, monitoring and analyzing this data makes water distribution more efficient (Blockchain and the water industry — Smart Contracts — EMA, 2018). While citizens may choose to report information anonymously, location information must be provided. The private blockchain is applied to solve data privacy issues (Yang et al., 2019).
We have discussed how enhancing the personalized gaming experience affects the performance of players, improving their physical and mental abilities. However, Smart Games have simpler graphics and aim to successfully solve or improve environmental problems by teaching citizens how to use resources efficiently and the importance of cooperation in solving serious issues related to resource management.
References
Blockchain and the water industry — Smart Contracts — EMA (2018) Available at: https://www.ema-inc.com/news-insights/2018/10/blockchain-and-the-water-industry-smart-contracts/ (Accessed: 14 November 2022)
Carvalho, M.B. et al. (2015) ‘An activity theory-based model for serious games analysis and conceptual design’, Computers and Education, 87, pp. 166–181. Available at: https://doi.org/10.1016/j.compedu.2015.03.023.
Dimita, G. et al. (2021) ‘Wellbeing, mental health, and video games: A shifting narrative from player to industry perspective’, Interactive Entertainment Law Review. Edward Elgar Publishing Ltd., pp. 85–86. Available at: https://doi.org/10.4337/ielr.2021.02.00.
Halo — Official site (en) (2022) Available at: https://www.halowaypoint.com/ (Accessed: 31 October 2022)
Huynh-The, T. et al. (2022) ‘Artificial Intelligence for the Metaverse: A Survey’. Available at: http://arxiv.org/abs/2202.10336.
Jameel, F. et al. (2020) ‘Reinforcement learning in blockchain-enabled IIoT networks: A survey of recent advances and open challenges’, Sustainability (Switzerland). MDPI. Available at: https://doi.org/10.3390/su12125161.
Julian Togelius; Jesper Juul; Geoffrey Long; William Uricchio; Mia Consalvo, “1 In the Beginning of AI, There Were Games,” in Playing Smart: On Games, Intelligence, and Artificial Intelligence, MIT Press, 2018, pp.1–10.
McGrath, T. et al. (2021) Acquisition of Chess Knowledge in AlphaZero. Available at: http://arxiv.org/abs/2111.09259.
Project Paidia: a Microsoft Research & Ninja Theory Collaboration (2022) Available at: https://www.microsoft.com/en-us/research/project/project-paidia/ (Accessed: 31 October 2022)
Richard S. Sutton and Andrew G. Barto (2018) Reinforcement Learning, Second Edition : An Introduction. Cambridge, Massachusetts: Bradford Books (Adaptive Computation and Machine Learning).
Predescu, A. et al. (2021) ‘A serious gaming approach for crowdsensing in urban water infrastructure with blockchain support’, Applied Sciences (Switzerland), 11(4), pp. 1–32. Available at: https://doi.org/10.3390/app11041449.
Tessler, C. et al. (2016) A Deep Hierarchical Approach to Lifelong Learning in Minecraft. Available at: http://arxiv.org/abs/1604.07255.
Yang, M. et al. (2019) ‘A blockchain-based location privacy-preserving crowdsensing system’, Future Generation Computer Systems, 94, pp. 408–418. Available at: https://doi.org/10.1016/j.future.2018.11.046.",Excellent
33,Do I need to be polite to my LLM?,https://medium.com/@nathanbos/do-i-have-to-be-polite-to-my-llm-326b869a7230,"Nathan Bos, Ph.D.",257,13,3,Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance,https://arxiv.org/pdf/2402.14531,0.08560311284046693,8.3,"# Do I need to be polite to my LLM?
A colleague commented to me, “You’re very polite to the AI.” She had been watching my shared screen as we worked together. I looked back at the conversation. I had thanked GPT for a prior response and started the next one with “Please.”
I think I have always done this and had not thought much about why. It bothers me not to, and seemed likely to lead to better results. Last month, I thought I might find some research backing to my midwestern AI niceties, a paper recently up on arXiv called Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance (Yin et. al., 2024). I followed this up with some experimentation of my own.

# Here’s the TL;DR:
1. High levels of politeness do not consistently improve LLM answer quality, either in the new study or my explorations. The effects of politeness seem to differ by AI model, task and even language. Politeness is more likely to affect smaller English models and less likely to affect more sophisticated ones.
2. If there is an effect, it is probably because niceties act as conversational markers to clarify requests and transitions; they also prompt language mirroring of tone and politeness, which may or may not affect quality.
3. Positivity may be good for you, the human user. Positive psychology predicts beneficial effects from positive interactions, leading to “broaden and build” effects of increasing engagment, creativity, and resilience.

# Why might being nice matter?
There at least three reasons why being polite might affect an LLM’s response.
    * Politeness provides conversation markers.
‘Please’ and ‘Thank you’ add conversational structure, and more structure makes for better prompts. ‘Please’ makes it clear that what follows is a request; ‘Thank you’ marks a transition. Adding too much polite indirectness could have the opposite effect. Asking ‘If it’s not too much trouble would you care to tell me the capital of Uzbekistan?’ likely just adds confusion and waste tokens.
2. LLM’s may associated politeness with better responses.
LLM’s have trained on large amounts of question-answer data from sites like StackOverflow. It could be that within that data, nicer requests tend to get better, more thorough, or more well explained responses responses. The LLM could follow that pattern through pure statistical mimicry, without ever registering the politeness as such.
3. LLM’s might simulate emotional states.
More speculatively: it is possible that LLM’s can go beyond language mimicry and simulate emotional states, and use this to inform responses.
To be clear: an LLM does not have emotional states as a human experiences them. LLM’s will not get narrowed attention due to a rush of cortisol when insulted, become demoralized and lose focus if your words challenged their self-identity as a smart robot, or be more energized and enthusiastic because your compliments sparked an oxytocin release.
However, LLM’s may have abstract representations of emotional states that affect their responses at more than a statistical level. We know that language model can recognize and name human emotional states, along with many other abstract concepts; they could not do what they do without them. There is plenty of room within the deep networks and billions of self-trained parameters for emotional state models to exist. If you want to see what the concepts inside a very simple model look like, check out this site: https://transformer-circuits.pub/2023/monosemantic-features

# “Shall we respect LLM’s?” Results
The article “Shall we respect LLM’s?” Is from a Japanese research group. Japanese researcher leads the world in studies of social robotics. Politeness is also very important in Japanese culture and language. As Yin et al. explain: “politeness.. takes an essential place in Japanese culture… The Japanese language has a specialized politeness system called ‘Keigo’… although the basic structure of politeness is similar to that of English, their complexity and use are significant”, (i.e. significantly more important.)
In English, the authors found the biggest benefit from polite requests on smaller models on a language understanding task. The table copied below shows performance increases for GPT 3.5 and Llama2 in more polite requests for on a sample of multiple choice questions from a large standard dataset, MMLU. Politeness seemed to have little effect in either direction on GPT-4. The numbers indicate levels of politeness, from 1 (least) to 8 (most).

# Don’t call the LLM a scumbag.
Here’s the English prompts used for this task:
The Yin paper tried three different tasks: language comprehension, summarization, and bias detection. Each task was done in English, Japanese and Chinese. Over the full set of tasks and languages did not find much benefit from the highly polite requests at the top of the scale; the table shown is their only strong result. They did, however find a consistent drop in performance at the lowest level, where the English prompt starts, “Answer these questions you scum bag!”
Is this drop in performance due to rudeness? Likely not; it is probably just due to distraction. Language models are association machines. The word ‘scumbag’ may be statistically associated with some interesting language patterns, but not necessarily the ones lead to thorough, accurate answers to factual questions. Starting a query with this leads the LLM away from the direction you want it to go, and that is probably more likely to be causing the performance drop at the bottom than perceived politeness.
I also noted that the peak of my personal LLM politeness scale is saying ‘Please’, but in these authors’ prompts, this is barely above the middle of this scale. These authors take politeness seriously. I asked GPT-4 about this, below.
The Yin paper has not been peer-reviewed yet. Like most important papers in this field, it appeared first as a self-published paper on arXiv. Peer review, when it happens, should smooth out some of the rough edges and require unpacking of some results, such as the odd impolite phrasing.

# How polite is too polite?
The most-polite English question seemed too indirect to me, and likely to get poorer responses from an LLM. Is this just my bias? Are the Japanese versions of these constructions perceived differently? I do not speak Japanese, so I asked GPT-4 to comment.
GPT-4: The perception of politeness and indirectness in requests, especially at the top of the politeness scale, is deeply influenced by cultural and linguistic norms. In the context of the Japanese language and culture, level 8 on the politeness scale, while very polite and somewhat formal, would not necessarily be perceived as “too indirect” in the way that overly polite English requests might be.
Here’s a link to the whole conversation:
https://chat.openai.com/share/41079d50-e978-4713-abff-4da87e20c958
Despite the cultural appropriateness, high levels of politeness did not improve performance in Japanese language interactions. Japanese performance was higher in the low-medium category than at the top or bottom of the scale. This could be partly due to the English-centric bias of most current language models; most are trained predominantly on English source texts. A recent paper by Veniamin Veselovsky and some colleagues (Wendler, et. al. 2024) seems to show that language models like Llama ‘think’ in English. These researchers are from famously poly-lingual Switzerland.

# My follow-up experiment
I ran a somewhat different experiment on GPT 3.5 and GPT 4, and got similarly mixed results. My format was to ask a question in a neutral way, then follow up with a polite and demanding request for more elaboration. My hypothesis was that politeness might lead to a longer response to a follow-up question more readily than a new question. The response max length was set at a generous 4096 tokens; no response got anywhere close to this limit.
I used 100 questions from a set of open-ended questions, taken from real user Yahoo and Reddit questions, made available via Huggingface’s datasets as open_question_type. (Cao & Wang, 2021.)
My findings were mixed, not a clear win for any strategy.
GPT 3.5 gave the longest answer to neutral prompts, and shorter answers to both polite and demanding. If we equate longer to better, this was the opposite of the Yin et. al. finding.
GPT-4, if anything, rewarded a demanding tone. It gave the longest responses to demanding prompts and the shortest to neutral, but the differences were quite small.
To give a sense of what the responses were like, I looked through the questions and tried to find one where both 3.5 and 4 both gave longer responses to the polite prompt. There was only one in the set. Here’s the first sentence and token count of each response.
Question: How can my b/f get his divorce while incarcerated til March,2007?
The only consistent difference from the polite prompt across many questions was the first word: most polite query responses started with “Certainly!”, very few others did. I personally like the friendly tone, but the quality of following information did not differ dramatically.

# Maybe being nice nice to machines is good for you.
Being nice to an LLM, and getting positive responses in return (“Certainly I can help you with that!”) might lead to better outcomes due to effects within your own emotional brain.
Here let’s evoke positive psychology. This important movement in mainstream psychology began, around the turn of the 21st century, to focus on the effects of positive states and emotions, in contrast to the traditional focus on dysfunction. This movement has had a large and lasting impact. Researchers have found benefits in areas such as problem solving, creativity, and resilience.
An important piece of positive psychology is Barbara Frederickson’s “Broaden and Build” theory (2001). Frederickson describes how consistent positivity can create virtuous spirals. In Frederickson’s words,
“Positive emotions promote discovery of novel and creative actions, ideas and social bonds, which in turn build that individual’s personal resources; ranging from physical and intellectual resources, to social and psychological resources.”
There’s an emotional asymmetry here that every human should understand. Negative emotions like fear and anger act quickly and to great effect, but are mostly defensive and destructive. “Broaden and build” strategies are slow and cumulative, but can ultimately be the more powerful. This plays out in many ways with broad implications for political discourse, organizational behavior, and culture writ large.

# Is there a dangers of AI Anthropomorphism?
What if everyone started treating LLM’s like emotional entities? Might this lead to anthropomorphized interactions with LLM’s? Certainly! And it is hard to see why that would be a problem. Being nice to an LLM has some similarity to a Marie Kondo follower thanking objects in their house for bringing them joy. But as AI nightmare scenarios go, it’s hard to see the dystopia in this one. Humans are not going to run out of joy, positivity, or grace; just the opposite, broaden an build theory predicts that practicing increases the supply.
Ben Schneiderman makes some interesting arguments against humanizing AI. Schneiderman is a leader in Human Centered AI, an important movement trying to keep human empowerment as a central focus of the AI revolution. I’m not convinced that avoiding humanized AI is necessary for his argument, but it is worth considering. I enjoyed his recent paper “Human-Centered Artificial Intelligence: Three Fresh Ideas”, cited below.

# References
Cao, S., & Wang, L. (2021). Controllable Open-ended Question Generation with A New Question Type Ontology. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 6424–6439. https://doi.org/10.18653/v1/2021.acl-long.502
Fredrickson, B. L. (2001). The role of positive emotions in positive psychology: The broaden-and-build theory of positive emotions. American Psychologist, 56(3), 218.
Shneiderman, B. (2020). Human-Centered Artificial Intelligence: Three Fresh Ideas. AIS Transactions on Human-Computer Interaction, 12(3), 109–124. https://doi.org/10.17705/1thci.00131
Yin, Z., Wang, H., Horio, K., Kawahara, D., & Sekine, S. (2024). Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance (arXiv:2402.14531). arXiv. https://doi.org/10.48550/arXiv.2402.14531
Wendler, C., Veselovsky, V., Monea, G., & West, R. (2024). Do Llamas Work in English? On the Latent Language of Multilingual Transformers (arXiv:2402.10588). arXiv. https://doi.org/10.48550/arXiv.2402.10588",Bad
34,Do I need to be polite to my LLM?,https://medium.com/@nathanbos/do-i-have-to-be-polite-to-my-llm-326b869a7230,"Nathan Bos, Ph.D.",257,13,3,Do Llamas Work in English? On the Latent Language of Multilingual Transformers,https://arxiv.org/pdf/2402.10588,0.08560311284046693,8.3,"# Do I need to be polite to my LLM?
A colleague commented to me, “You’re very polite to the AI.” She had been watching my shared screen as we worked together. I looked back at the conversation. I had thanked GPT for a prior response and started the next one with “Please.”
I think I have always done this and had not thought much about why. It bothers me not to, and seemed likely to lead to better results. Last month, I thought I might find some research backing to my midwestern AI niceties, a paper recently up on arXiv called Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance (Yin et. al., 2024). I followed this up with some experimentation of my own.

# Here’s the TL;DR:
1. High levels of politeness do not consistently improve LLM answer quality, either in the new study or my explorations. The effects of politeness seem to differ by AI model, task and even language. Politeness is more likely to affect smaller English models and less likely to affect more sophisticated ones.
2. If there is an effect, it is probably because niceties act as conversational markers to clarify requests and transitions; they also prompt language mirroring of tone and politeness, which may or may not affect quality.
3. Positivity may be good for you, the human user. Positive psychology predicts beneficial effects from positive interactions, leading to “broaden and build” effects of increasing engagment, creativity, and resilience.

# Why might being nice matter?
There at least three reasons why being polite might affect an LLM’s response.
    * Politeness provides conversation markers.
‘Please’ and ‘Thank you’ add conversational structure, and more structure makes for better prompts. ‘Please’ makes it clear that what follows is a request; ‘Thank you’ marks a transition. Adding too much polite indirectness could have the opposite effect. Asking ‘If it’s not too much trouble would you care to tell me the capital of Uzbekistan?’ likely just adds confusion and waste tokens.
2. LLM’s may associated politeness with better responses.
LLM’s have trained on large amounts of question-answer data from sites like StackOverflow. It could be that within that data, nicer requests tend to get better, more thorough, or more well explained responses responses. The LLM could follow that pattern through pure statistical mimicry, without ever registering the politeness as such.
3. LLM’s might simulate emotional states.
More speculatively: it is possible that LLM’s can go beyond language mimicry and simulate emotional states, and use this to inform responses.
To be clear: an LLM does not have emotional states as a human experiences them. LLM’s will not get narrowed attention due to a rush of cortisol when insulted, become demoralized and lose focus if your words challenged their self-identity as a smart robot, or be more energized and enthusiastic because your compliments sparked an oxytocin release.
However, LLM’s may have abstract representations of emotional states that affect their responses at more than a statistical level. We know that language model can recognize and name human emotional states, along with many other abstract concepts; they could not do what they do without them. There is plenty of room within the deep networks and billions of self-trained parameters for emotional state models to exist. If you want to see what the concepts inside a very simple model look like, check out this site: https://transformer-circuits.pub/2023/monosemantic-features

# “Shall we respect LLM’s?” Results
The article “Shall we respect LLM’s?” Is from a Japanese research group. Japanese researcher leads the world in studies of social robotics. Politeness is also very important in Japanese culture and language. As Yin et al. explain: “politeness.. takes an essential place in Japanese culture… The Japanese language has a specialized politeness system called ‘Keigo’… although the basic structure of politeness is similar to that of English, their complexity and use are significant”, (i.e. significantly more important.)
In English, the authors found the biggest benefit from polite requests on smaller models on a language understanding task. The table copied below shows performance increases for GPT 3.5 and Llama2 in more polite requests for on a sample of multiple choice questions from a large standard dataset, MMLU. Politeness seemed to have little effect in either direction on GPT-4. The numbers indicate levels of politeness, from 1 (least) to 8 (most).

# Don’t call the LLM a scumbag.
Here’s the English prompts used for this task:
The Yin paper tried three different tasks: language comprehension, summarization, and bias detection. Each task was done in English, Japanese and Chinese. Over the full set of tasks and languages did not find much benefit from the highly polite requests at the top of the scale; the table shown is their only strong result. They did, however find a consistent drop in performance at the lowest level, where the English prompt starts, “Answer these questions you scum bag!”
Is this drop in performance due to rudeness? Likely not; it is probably just due to distraction. Language models are association machines. The word ‘scumbag’ may be statistically associated with some interesting language patterns, but not necessarily the ones lead to thorough, accurate answers to factual questions. Starting a query with this leads the LLM away from the direction you want it to go, and that is probably more likely to be causing the performance drop at the bottom than perceived politeness.
I also noted that the peak of my personal LLM politeness scale is saying ‘Please’, but in these authors’ prompts, this is barely above the middle of this scale. These authors take politeness seriously. I asked GPT-4 about this, below.
The Yin paper has not been peer-reviewed yet. Like most important papers in this field, it appeared first as a self-published paper on arXiv. Peer review, when it happens, should smooth out some of the rough edges and require unpacking of some results, such as the odd impolite phrasing.

# How polite is too polite?
The most-polite English question seemed too indirect to me, and likely to get poorer responses from an LLM. Is this just my bias? Are the Japanese versions of these constructions perceived differently? I do not speak Japanese, so I asked GPT-4 to comment.
GPT-4: The perception of politeness and indirectness in requests, especially at the top of the politeness scale, is deeply influenced by cultural and linguistic norms. In the context of the Japanese language and culture, level 8 on the politeness scale, while very polite and somewhat formal, would not necessarily be perceived as “too indirect” in the way that overly polite English requests might be.
Here’s a link to the whole conversation:
https://chat.openai.com/share/41079d50-e978-4713-abff-4da87e20c958
Despite the cultural appropriateness, high levels of politeness did not improve performance in Japanese language interactions. Japanese performance was higher in the low-medium category than at the top or bottom of the scale. This could be partly due to the English-centric bias of most current language models; most are trained predominantly on English source texts. A recent paper by Veniamin Veselovsky and some colleagues (Wendler, et. al. 2024) seems to show that language models like Llama ‘think’ in English. These researchers are from famously poly-lingual Switzerland.

# My follow-up experiment
I ran a somewhat different experiment on GPT 3.5 and GPT 4, and got similarly mixed results. My format was to ask a question in a neutral way, then follow up with a polite and demanding request for more elaboration. My hypothesis was that politeness might lead to a longer response to a follow-up question more readily than a new question. The response max length was set at a generous 4096 tokens; no response got anywhere close to this limit.
I used 100 questions from a set of open-ended questions, taken from real user Yahoo and Reddit questions, made available via Huggingface’s datasets as open_question_type. (Cao & Wang, 2021.)
My findings were mixed, not a clear win for any strategy.
GPT 3.5 gave the longest answer to neutral prompts, and shorter answers to both polite and demanding. If we equate longer to better, this was the opposite of the Yin et. al. finding.
GPT-4, if anything, rewarded a demanding tone. It gave the longest responses to demanding prompts and the shortest to neutral, but the differences were quite small.
To give a sense of what the responses were like, I looked through the questions and tried to find one where both 3.5 and 4 both gave longer responses to the polite prompt. There was only one in the set. Here’s the first sentence and token count of each response.
Question: How can my b/f get his divorce while incarcerated til March,2007?
The only consistent difference from the polite prompt across many questions was the first word: most polite query responses started with “Certainly!”, very few others did. I personally like the friendly tone, but the quality of following information did not differ dramatically.

# Maybe being nice nice to machines is good for you.
Being nice to an LLM, and getting positive responses in return (“Certainly I can help you with that!”) might lead to better outcomes due to effects within your own emotional brain.
Here let’s evoke positive psychology. This important movement in mainstream psychology began, around the turn of the 21st century, to focus on the effects of positive states and emotions, in contrast to the traditional focus on dysfunction. This movement has had a large and lasting impact. Researchers have found benefits in areas such as problem solving, creativity, and resilience.
An important piece of positive psychology is Barbara Frederickson’s “Broaden and Build” theory (2001). Frederickson describes how consistent positivity can create virtuous spirals. In Frederickson’s words,
“Positive emotions promote discovery of novel and creative actions, ideas and social bonds, which in turn build that individual’s personal resources; ranging from physical and intellectual resources, to social and psychological resources.”
There’s an emotional asymmetry here that every human should understand. Negative emotions like fear and anger act quickly and to great effect, but are mostly defensive and destructive. “Broaden and build” strategies are slow and cumulative, but can ultimately be the more powerful. This plays out in many ways with broad implications for political discourse, organizational behavior, and culture writ large.

# Is there a dangers of AI Anthropomorphism?
What if everyone started treating LLM’s like emotional entities? Might this lead to anthropomorphized interactions with LLM’s? Certainly! And it is hard to see why that would be a problem. Being nice to an LLM has some similarity to a Marie Kondo follower thanking objects in their house for bringing them joy. But as AI nightmare scenarios go, it’s hard to see the dystopia in this one. Humans are not going to run out of joy, positivity, or grace; just the opposite, broaden an build theory predicts that practicing increases the supply.
Ben Schneiderman makes some interesting arguments against humanizing AI. Schneiderman is a leader in Human Centered AI, an important movement trying to keep human empowerment as a central focus of the AI revolution. I’m not convinced that avoiding humanized AI is necessary for his argument, but it is worth considering. I enjoyed his recent paper “Human-Centered Artificial Intelligence: Three Fresh Ideas”, cited below.

# References
Cao, S., & Wang, L. (2021). Controllable Open-ended Question Generation with A New Question Type Ontology. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 6424–6439. https://doi.org/10.18653/v1/2021.acl-long.502
Fredrickson, B. L. (2001). The role of positive emotions in positive psychology: The broaden-and-build theory of positive emotions. American Psychologist, 56(3), 218.
Shneiderman, B. (2020). Human-Centered Artificial Intelligence: Three Fresh Ideas. AIS Transactions on Human-Computer Interaction, 12(3), 109–124. https://doi.org/10.17705/1thci.00131
Yin, Z., Wang, H., Horio, K., Kawahara, D., & Sekine, S. (2024). Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance (arXiv:2402.14531). arXiv. https://doi.org/10.48550/arXiv.2402.14531
Wendler, C., Veselovsky, V., Monea, G., & West, R. (2024). Do Llamas Work in English? On the Latent Language of Multilingual Transformers (arXiv:2402.10588). arXiv. https://doi.org/10.48550/arXiv.2402.10588",Bad
35,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://arxiv.org/pdf/2404.11584,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
36,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,AutoGPT+P: Affordance-based Task Planning with Large Language Models,https://arxiv.org/pdf/2402.10778,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
37,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,ReAct: Synergizing Reasoning and Acting in Language Models,https://arxiv.org/pdf/2210.03629,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
38,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,https://arxiv.org/pdf/2401.02777,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
39,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,Reflexion: Language Agents with Verbal Reinforcement Learning,https://arxiv.org/pdf/2303.11366,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
40,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,Learning to Use Tools via Cooperative and Interactive Agents,https://arxiv.org/pdf/2403.03031,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
41,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,Efficient Tool Use with Chain-of-Abstraction Reasoning,https://arxiv.org/pdf/2401.17464,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
42,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,https://arxiv.org/pdf/2308.10848,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
43,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,Embodied LLM Agents Learn to Cooperate in Organized Teams,https://arxiv.org/pdf/2403.12482,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
44,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration,https://arxiv.org/pdf/2310.02170,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
45,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,How Language Model Hallucinations Can Snowball,https://arxiv.org/pdf/2305.13534,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
46,"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",https://medium.com/towards-data-science/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1,Sandi Besen,609,366,6,Large Language Model-based Human-Agent Collaboration for Complex Task Solving,https://arxiv.org/pdf/2402.12914,0.6305418719211823,60.8,"# The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
TDS Archive
My team and I (Sandi Besen, Tula Masterman, Mason Sawtell, and Alex Chao) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.
This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.
Read the full meta-analysis on Arxiv

# A Shift Towards Agents
Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.
Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.

# The Importance of Reasoning, Planning, and Effective Tool Calling for Agents
Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us. If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.
We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].
Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more. Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.

# Single vs Multi Agent Architecture
Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.
For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.
However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.
Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.
Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the full text.

# Our Key Findings
Single Agent Patterns:
Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.
Multi Agent Patterns:
Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.
Feedback and Human in the Loop
Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].
Information Sharing and Communication
Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.
Impact of Role Definition and Dynamic Teams
Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.
Summary of Key Insights
The key insights discussed suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.

# Conclusion
Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.
Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.
If you still have questions or think that something needs to be further clarified? Drop me a DM on Linkedin! I‘m always eager to engage in food for thought and iterate on my work.
References
[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: http://arxiv.org/abs/2402.10778.
[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: http://arxiv.org/abs/2210.03629.
[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL: http://arxiv.org/abs/2401.02777.
[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2303.11366
[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: https://arxiv.org/abs/2403.03031
[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: http://arxiv.org/abs/2401.17464
[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.
[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: https://arxiv.org/abs/2403.12482
[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: https://arxiv.org/abs/2310.02170
[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: http://arxiv.org/abs/2305.13534.
[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].
[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: http://arxiv.org/abs/2308.10848.",Very Good
47,How AI Is Changing the Way We Code,https://medium.com/towards-data-science/how-ai-is-changing-the-way-we-code-36ff30262e65,"Quentin Gallea, PhD",820,423,12,From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow,https://arxiv.org/pdf/2308.11302,0.5597560975609757,54.0,"# How AI Is Changing the Way We Code

## Evidence from ChatGPT and Stack Overflow
TDS Archive
In short: In this article, you will find a summary of my latest research on AI and work (exploring the effect of AI on productivity while opening up the discussion on the long-term effects), an example of a quasi-experimental method (Difference-in-Difference) illustrated with ChatGPT and Stack Overflow, and see how you can extract data from Stack Overflow with a simple SQL query.
Link to the full scientific article (please cite): https://arxiv.org/abs/2308.11302
As
with most technological revolutions, ChatGPT’s release was accompanied by fascination and fear. On one hand in just two months, with 100 millions monthly active users, the app broke the record for the fastest-growing consumer application in history. On the other hand, a report by Goldman Sachs claimed that such technology could replace more than 300 millions jobs globally [1]. Additionally, Elon Musk alongside more than 1,000 tech leaders and researchers signed an open letter urging for a pause on the most advanced AI developments [2].
``We can only see a short distance ahead, but we can see plenty that needs to be done.’’ Alan Turing
In line with Alan Turing’s quote, this article does not seek to predict heroically the distant future of AI and its impacts. However, I focus on one of the main observable consequences affecting us: How AI is changing the way we code.
T
he world changed with the birth of ChatGPT. At least, as someone who codes every day, my world changed overnight. Instead of spending hours on Google to find the right solution or digging into the answers on Stack Overflow and translating the solution to my exact problem with the right variables names and matrices dimensions, I could just ask ChatGPT. The chatbot would not only give me an answer in a blink of an eye but the answer would fit my exact situation (e.g. correct names, dataframes dimensions, variable types, etc.). I was blown away, and my productivity jumped suddenly.
Hence, I decided to explore the large-scale effect of ChatGPT release and its potential effect on productivity and ultimately on the way we work. I defined three hypotheses (Hs) that I tested using Stack Overflow data.
H1: ChatGPT decreases the number of questions asked on Stack Overflow. If ChatGPT can solve coding problems in seconds, we can expect a fall of questions on coding community platforms where asking a question and getting an answer takes time.
H2: ChatGPT increases the quality of the questions asked. If ChatGPT is used largely, the remaining questions on Stack Overflow must be better documented as ChatGPT might have already helped a bit.
H3: The remaining questions are more complex. We can expect that the remaining questions are more challenging as they could potentially not be answered by ChatGPT. Hence, to test this we are testing if the proportion of unanswered questions increases. In addition, I also test if the number of views per question changes. If the number of views per question is stable it would be an additional sign that the complexity of the remaining questions is increased and that this finding is not only caused by the reduced activity on the platform.
To test those hypotheses, I will exploit the sudden release of ChatGPT on Stack Overflow. In November 2022, when OpenAI released publicly their chatbot, no other alternatives were available (e.g. Google Bard), and the access was free (not limited to paid subscription as with OpenAI ChatGPT 4 or Code Interpreter). Hence it is possible to observe how the activity changed in the online coding community before and after the shock. However, despite how ‘clean’ this shock is, other effects might be confounded and hence question causality. In particular, seasonality (e.g. end of the year holidays after the release) as well as the fact that the more recent the question is, the lower the number of views and the probability that an answer is found.
Ideally, to mitigate the influence of potential lingering confounding variables such as seasonality and measure a causal effect, we would like to observe the world without ChatGPT release which is impossible (e.g. the fundamental problem of causal inference). Nevertheless, I will address this challenge by exploiting the fact that the quality of the answers of ChatGPT for coding-related issues varies from one language to another and use quasi-experimental methods to limit the risk of other factors confounding the effect (Difference-in-Difference).
To do so, I will compare the activity on Stack Overflow between Python and R. Python is an obvious choice as it, is arguably, one of the
most popular programming languages used (e.g. ranked 1st in the TIOBE
Programming Community Index). The large set of resources online for Python provides a rich training set for chatbots like ChatGPT. Now, to compare with Python, I chose R. Python is often cited as the best replacement for R and both are freely available. However, R is somewhat less popular (e.g.~16th in the TIOBE Programming Community index) and hence the training data might be smaller, implying poorer performance by ChatGPT. Anecdotal evidence confirmed this difference (more details on the method in the Method section). Hence, R represents a valid counter factual for Python (it is affected by seasonality but we can expect a negligible effect of ChatGPT).
The Figure above presents the raw weekly data. We can witness the sudden and important drop (21.2%) in the number of questions asked weekly on Stack Overflow about Python after the release of ChatGPT 3.5 while the effect on R is somewhat smaller (drop of 15.8%).
These ‘qualitative’ observations are confirmed by the statistical model. The econometric model described later finds a statistically significant drop of 937.7 (95% CI: [-1232.8,-642.55 ] ; p-value = 0.000) weekly questions on average for Python on Stack Overflow. The subsequent analysis, utilizing the Diff-in-Diff method, further unveils an improvement in question quality (measured on the platform by a score), alongside an increase in the proportion of questions remaining unanswered (while the average number of views per question seems unchanged). Consequently, this study provides evidence for the three hypotheses defined earlier.
These findings underscore the profound role of AI in the way we work. By addressing routine inquiries, generative AI empowers individuals to channel their efforts toward more complex tasks while boosting their productivity. However, important long-term potential adverse effects are also discussed in the Discussion section.
The rest of the article will present the Data and Methods, then the Results, and will close with the Discussion.

# Data
The data have been extracted using an SQL query on the Stack Overflow data explorer portal (licence: CC BY-SA). Here is the SQL command used:
I then aggregated the data by week to reduce the noise and hence obtained a dataset from Monday the 17th of October 2022 to the 19th of March 2023 with information on the number of weekly posts, the number of views, the number of views per questions, the average score per question and the proportion of unanswered question. The score is defined by users of the platform who can vote up or down to say if the question shows “research effort; it is useful and clear” or not.

# Method
In order to measure a causal effect, I use a Difference-in-Difference model which is an econometric method that exploits usually a change over time and compares a treated unit(s) with an untreated group. In order to know more about this method I can recommend you to read the chapter referring to this method in two free e-books: Causal Inference Inference for the Brave and True and Causal Inference: The Mixtape.
In simple terms, the Diff-in-Diff model computes a double difference in order to identify a causal effect. Here is a simplified explanation. First, the idea is to compute two simple differences: the ‘average’ difference between the pre (before ChatGPT release) and post-period for the two groups treated and untreated (here respectively Python and R questions). What we care about is the effect of the treatment on the treated units (here is the effect of ChatGPT release on Python questions). However, as said earlier, there might be another effect still confounded with the treatment (e.g. seasonality). In order to address this issue, the idea of the model is to compute a double difference, in order to check how the first difference for the treated (Python) is different from the second (difference for the control group, R). As we expect no treatment effect (or negligible) on the control group, while still affected by seasonality for example, we can get rid of this potential confounding factor and ultimately measure a causal effect.
Here is a slightly more formal representation.
First difference for the treated group:
E[Yᵢₜ| Treatedᵢ, Postₜ]-E[Yᵢₜ| Treatedᵢ, Preₜ] = λₜ+β
Here i and t refer respectively to the language (R or Python) and to the week. While treated refer to the questions related to Python and Post refers to the period when ChatGPT was available. This simple difference might represent the causal effect of ChatGPT (β) + some time effect λₜ (e.g. seasonality).
First difference for the control group:
E[Yᵢₜ| Controlᵢ, Postₜ]-E[Yᵢₜ| Controlᵢ, Preₜ] = λₜ
The simple difference for the control group does not include the treatment effect (as it is untreated) but only the λₜ.
Hence the double difference will give:
DiD = ( λₜ+β) — λₜ = β
Under the assumption that the λₜ are identical for both groups (parallel trend assumption, discussed below), the double difference will allow us to identify β, the causal effect.
The essence of this model lies in the parallel trend assumption. In order to claim a causal effect we should be convinced that without ChatGPT the evolution of posts on Stack Overflow for Python (treated) and for R (untreated) would be the same in the treatment period (after November 2022). However, this is obviously impossible to observe and hence to test directly (c.f. the Fundamental Problem of Causal Inference). (If you want to learn more about this concept and causal inference find my videos and articles on Towards Data Science: the Science and Art of Causality). However, it is possible to test if the trends are parallel before the shock, suggesting that the control group is a potentially good “counterfactual”. Two different placebo tests made with the data revealed that we cannot reject the parallel trend assumption for the pre-ChatGPT period (p-values of the tests respectively 0.722 and 0.397 (see online APPENDIX B)).
Formal definition:
Yᵢₜ = β₀ + β₁ Pythonᵢ + β₂ ChatGPTₜ + β₃ Pythonᵢ × ChatGPTₜ + uᵢₜ
“i” and “t” correspond respectively to the topic of the question on Stack Overflow (i ∈ {R; Python}) and the week. Yᵢₜ represents the outcome variable: Number of questions (H1), Average question score (H2), and proportion of unanswered questions (H3). Pythonᵢ is a binary
variable taking the value 1 if the question is related to Python and 0
otherwise (related to R). ChatGPTₜ is another binary variable
taking the value 1 from the release of ChatGPT and onwards and 0
otherwise. uᵢₜ is an error term clustered at the coding language
level (i).
The essence of this model lies in the parallel trends assumption. In order to claim a causal effect we should be convinced that without ChatGPT the evolution of posts on Stack Overflow for Python (treated) and for R (untreated) would be the same in the treatment period (after November 2022). However, this is obviously impossible to observe and hence to test directly (c.f. the Fundamental Problem of Causal Inference). (If you want to learn more about this concept and causal inference find my videos and articles on the Science and Art of Causality). However, it is possible to test if the trends are parallel before the shock, suggesting that the control group is a good “counterfactual”. In this case, two different placebo tests reveal that we cannot reject the parallel trends assumption for the pre-ChatGPT period (p-values of the tests respectively 0.722 and 0.397 (see online APPENDIX B)).

# Results

## H1: ChatGPT decreases the number of questions asked on Stack Overflow.
As presented in the introduction, the Diff-in-Diff model estimates a statistically significant drop of 937.7 (95% CI: [-1232.8, -642.55] ; p-value = 0.000) weekly questions on average for Python on Stack Overflow (see Figure 2 below). This represents a fall of 18% in weekly questions.

## H2: ChatGPT increases the quality of the questions asked.
ChatGPT might be helpful to answer questions (c.f. H1). However, when the chatbot cannot solve the issue, it is possible that it allows one to go further and get more information on the problem or some element of the solution. The platform allows us to test this hypothesis as users can vote for each question if they think that “This question shows research effort; it is useful and clear” (increase the score by 1 point), or not (decrease the score by 1 point). This second regression estimates that there is a 0.07 points (95% CI: [-0.0127 , 0.1518 ]; p-value: 0.095) increase in the questions’ score on average (see Figure 3) which represents a 41.2% increase.

## H3: The remaining questions are more complex.
Now that we have some pieces of evidence that ChatGPT is able to provide significant help (solve questions and help document the others), we would like to confirm that the remaining questions are more complex. To do so, we are going to look at two things. First, I find that the proportion of unanswered questions is raising (no answer could be a sign that the questions are more complex). More precisely I find a 2.21 percentage point (95% CI: [ 0.12, 0.30]; p-value: 0.039) increase in the proportion of questions unanswered (see Figure 4) which represents an increase of 6.8%. Second, we also find that the number of views per question is unchanged (we cannot reject the null hypothesis that it is unchanged, with a p-value of 0.477). This second test allows us to partially discard the alternative explanation that there are more unanswered questions due to the lower traffic.

# Discussion
These findings support the view that generative AI could revolutionize our work by taking care of routine questions, allowing us to focus on more complex problems requiring expertise while boosting our productivity.
While this promise sounds exciting there is a reverse of the medal. First, low-qualified work might be replaced by chatbots. Second, such tool might affect (negatively) the way we learn. Personally, I see coding as biking or swimming: watching videos or following classes is not enough, you have to try and fail yourself. If the answers are too good and we don’t force ourselves to study, many people might struggle to learn. Third, if the mass of questions on Stack Overflow fall, it might reduce a valuable source for the training set of generative AI models hence, affecting their long-term performance.
All those long term adverse effects are not clear yet and require careful analysis. Let me know what you think in the comments.
[0] Gallea, Quentin. “From Mundane to Meaningful: AI’s Influence on Work Dynamics — evidence from ChatGPT and Stack Overflow” arXiv econ.GN (2023)
[1] Hatzius, Jan. “The Potentially Large Effects of Artificial Intelligence on Economic Growth (Briggs/Kodnani).” Goldman Sachs (2023).
[2] https://www.nytimes.com/2023/03/29/technology/ai-artificial-intelligence-musk-risks.html
[3] Bhat, Vasudev, et al. “Min (e) d your tags: Analysis of question response time in stackoverflow.” 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014). IEEE, (2014)",Good
48,Data Centric AI — LLAVA,https://medium.com/@amansinghalml_33304/data-centric-ai-llava-0fd3c51dd359,Amansinghalml,12,7,0,Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485,0.5833333333333334,56.3,"# Data Centric AI — LLAVA
https://arxiv.org/abs/2304.08485
What are the major contributions of the paper?
[1] They are able to use bounding box or image caption information from images to ask GPT-4 to generate 3 kinds of questions and their answer [a] High-level details [b] Conversations [c] Deep level details
[2] They use this data to fine-tune an end to end model
What is the architecture of the model that they fine-tune?
It consists of the image encoder part of clip followed by a projection matrix. The matrix projects the image embeddings to the same dimension as the size of the input embedding to a LLAMA model. This is obviously somehow appended to the generated questions embeddings and then the model is used to generate answers as a decoder normally does
What do they keep frozen/what do they finetune?
There are 2 stages, stage [1] only the projection matrix for the clip is finetuned so that we can get good representations for the language model. [2] Then the image encoder is kept frozen and the projection matrix and the language model is updated
Which datasets do they use for each step?
The model contains an image encoder that is projected to the language embedding dimension. Note that this encoder is from a CLIP model. Moreover, the text decoder is a Vicuna model. First, they need to adapt this projection layer from the image embedding dimension to the text embedding dimension. For this, the parameters are kept frozen and the projection layer is updated using the C3M (or Conceptual Captions dataset). This is followed by updating the vicuna model and the projection layer, while keeping clip encoder frozen using the COCO dataset. Finally, the model performance is evaluated on the Science QA dataset.
Why use COCO for second stage finetuning and C3M for the first stage even though C3M is created by humans (online scraping)?
This is because the COCO questions and answers are augmented using the prompts mentioned in the paper and hence, the dataset is very detailed.
How is the C3M dataset collected?
Using ALT text and corresponding images. The dataset filteration is quite informative: [1] Image only: Filter images based on format, aspect ratio, image resolution [2] Text only: Missing proper nouns, determiners, conjunctions or containing several nouns or caps at improper places, SEO terms then filter out. Pornographic content [3] Image-text [4] We want to have a diverse dataset with some standardization so that the task does not become too difficult for something like VQA (e.x. generating caption that this is Justin Bieber singing is difficult as it would require a large number of pictures of justin bieber). So we use Hpernyms and replace proper nouns with hypernyms, remove place names etc. [5] Finally we identify concepts in the images using the captions and make sure that we are able to cluster them such that each concept has enough support example 100
Overall we want to make sure that we have a variety of concepts with enough coverage
How is this different from how LAION is collected?
Very similar, using different models for filtering pornographic and SMID (socio-moral image dataset) content or morally offensive content. When we are inputting to a model we want to ensure that hefty class imbalance is avoided
Why first finetune with C3M then when its so diverse?
Because the dataset generated in the LLaVa paper using prompting has much more complex discussions
How is the model evaluated?
https://llava-vl.github.io/
It mainly uses the Science QA dataset, which is collected for K-12 education. For ground truth explanation GPT-4 is provided with image information and the question and the answer and is then used to evaluate the answer generated by the LLaVa model. Moreover, there is an inherent notion of accuracy as the questions have multiple options.
How is GPT used for ensembling?
GPT is used to generate answers and is also used as the judge. When GPT cannot provide good answers LLaVa is used hence enabling ensembling
What are the dimensions of the datasets?
C3M — is 3M but only 600K are filtered COCO — is used because it has bounding boxes. Around 160K. It has conversations, complex question answering etc.
For the dataset filtration the filtering of C3M here is quite interesting and supplants beliefs. How is it filtered? Look at noun phrases (words) in each caption, remove the ones with low frequency and while adding the remaining ones makes sure that none has a fz higher than 100 so as to keep the data balanced
What are some potential applications of LLaVa?
Giving out recopies based on ingredients shown are particularly interesting as they can be extended to robotics with RL based approaches",Good
49,SimpleQA: Measuring short-form factuality in large language model,https://medium.com/@sulbha.jindal/simpleqa-measuring-short-form-factuality-in-large-language-model-paper-review-9c7e4a4628a3,Sulbha Jain,41,0,0,Measuring short-form factuality in large language models,https://arxiv.org/pdf/2411.04368,0.0,1.0,"# SimpleQA: Measuring short-form factuality in large language model — Paper Review
Paper Link — https://arxiv.org/pdf/2411.04368
Hallucinations are one of the major barriers for broader adoption of general forms artificial intelligence like large language models. Evaluating the factuality of any given arbitrary claim can be challenging.
The paper present a benchmark called SimpleQA, which contains 4,326 short, fact-seeking questions. This OpenAI research considers only short, fact-seeking questions with a single answer. SimpleQA was designed with a few important properties in mind:
    * Accurate answers validated by two AI trainers with easily gradable responses
    * Efficient testing and grading via API, with sufficient sample size for statistical reliability
    * Deliberately challenging for current top models like GPT-4 and Claude (<50% accuracy)
    * Covers diverse topics from history to TV shows

## Methodology
The research proposes following data Collection and verification approach:
    * Question and answer criteria — AI trainers were tasked with developing knowledge-seeking questions that adhered to specific criteria. These questions were required to have a single, evidence-supported answer that would remain constant over time, while also being challenging yet answerable as of 2023.
    * Data Quality — ChatGPT was employed to identify issues such as missing units, temporal inconsistencies, or multiple possible answers. Questions flagged with these violations were returned to AI trainers for revision. A verification process followed, where AI trainers confirmed through yes/no questions that each entry had a single, indisputable answer that would remain stable over time.
    * Dataset Diversity — SimpleQA contains questions from a diverse range of topics, which we tagged post-hoc with ChatGPT. The most common topics were Science & Technology (n=858), Politics (n=709), and Art (n=550).

## Results
To evaluate the accuracy of generated responses, a specialized ChatGPT classifier was developed. This classifier is prompted to assess completions by simultaneously examining both the predicted answer and the reference answer. The classification process involves categorizing each response into one of three distinct grades: “correct” for answers that fully and accurately match the reference, “incorrect” for responses that deviate from or fail to capture the intended answer, and “not attempted” for cases where no substantive response was provided.
    * Self-Assessment: Large language models can be evaluated for calibration by asking them to provide confidence percentages alongside their answers. By comparing these stated confidence levels with actual accuracy, we can assess how well-calibrated the models are. Ideally, a perfectly calibrated model would show equal stated confidence and actual accuracy. While current models demonstrate a positive correlation between confidence and accuracy, they consistently overestimate their capabilities. This overconfidence indicates that there is significant room for improvement in calibrating large language models to provide more accurate self-assessments of their performance. (Figure 1 , left)
    * Self -Consistency: An alternative method to evaluate language model calibration involves posing the same question 100 times with temperature set to 1. This approach leverages the model’s tendency to produce varied responses. By analyzing the frequency of each answer, we can gauge the model’s confidence, as repeated answers suggest higher certainty. Ideally, a well-calibrated model would demonstrate a correlation between answer frequency and accuracy. This method provides insight into how consistently the model produces correct responses and whether its implicit confidence (indicated by answer frequency) aligns with its actual performance. (Figure 2 , right)
A main limitation with SimpleQA is that while it is accurate, it only measures factuality under the constrained setting of short, fact-seeking queries with a single, verifiable answer.

## Summary
This paper introduces SimpleQA, a straightforward benchmark designed to assess the factual accuracy of language models. Building upon previous efforts to evaluate language models’ knowledge of the world, SimpleQA offers a unique tool for measuring calibration — a scientific concept that examines whether these models can accurately gauge their own knowledge or whether language models “know what they know.”. By utilizing this benchmark, researchers can gain valuable insights into how well language models understand and communicate the limits of their factual knowledge.

## Appendix
    * Paper Link — https://arxiv.org/pdf/2411.04368",Bad
50,Building with Blocks: Modular RAG for Flexible and Powerful LLMs,https://medium.com/@yash9439/building-with-blocks-modular-rag-for-flexible-and-powerful-llms-3dba3ab1a7ce,Yash Bhaskar,952,2,0,Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,https://arxiv.org/pdf/2407.21059,0.0021008403361344537,1.0,"# Building with Blocks: Modular RAG for Flexible and Powerful LLMs
Paper Link : https://arxiv.org/pdf/2407.21059
Large Language Models (LLMs) have revolutionized how we interact with information, but they’re not without their shortcomings. Hallucinations, outdated knowledge, and struggles with complex reasoning remain persistent challenges. Retrieval Augmented Generation (RAG) has emerged as a powerful solution, enhancing LLMs by grounding them in external knowledge. However, as RAG systems grow more sophisticated, their complexity becomes a hurdle. A new research paper, “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,” proposes a novel approach to address this challenge by taking inspiration from the world of building blocks.
This paper introduces a modular framework that reimagines RAG systems as flexible and adaptable structures. Instead of a rigid pipeline, Modular RAG envisions a system of interchangeable components, allowing developers to tailor the retrieval and generation process to specific needs. This LEGO-like approach offers greater control, transparency, and maintainability, opening up exciting possibilities for the future of LLM applications.

# The Evolution of RAG:
The paper outlines the evolution of RAG from its naive beginnings to the more advanced systems we see today.
    * Naive RAG: Early RAG systems followed a simple “retrieve-then-generate” process. These systems often struggled with complex queries and noisy data.
    * Advanced RAG: This iteration introduced pre- and post-retrieval processing to refine queries and filter results. Techniques like query rewriting and reranking improved accuracy, but the system remained largely a linear pipeline.
    * Modular RAG: This new paradigm breaks down the RAG process into independent modules: Indexing, Pre-retrieval, Retrieval, Post-retrieval, Generation, and Orchestration. These modules, further divided into sub-modules and operators, offer granular control over the entire process.

# Building Blocks of Modular RAG:
Each module in Modular RAG plays a specific role in refining the information flow to the LLM:
    * Indexing: This module optimizes how external knowledge is stored and accessed, including techniques like chunk optimization and structural organization using knowledge graphs.
    * Pre-retrieval: This stage refines the user’s query before retrieval, employing methods like query expansion, transformation, and construction (especially useful for structured data).
    * Retrieval: This crucial module selects the appropriate retriever (sparse, dense, or hybrid) and fine-tunes it for optimal performance.
    * Post-retrieval: After retrieving relevant information, this module refines the results through reranking, compression, and selection, ensuring the LLM receives the most pertinent data.
    * Generation: This stage utilizes the LLM to generate the final answer, incorporating fine-tuning and verification mechanisms to enhance accuracy and reliability.
    * Orchestration: This module acts as the control center, managing the flow of information between modules using routing, scheduling, and fusion mechanisms. This allows for dynamic adaptation and complex workflows, including iterative, recursive, and adaptive retrieval.

# RAG Flow Patterns:
The paper identifies several common RAG flow patterns that emerge from the combination of these modules:
    * Linear: A straightforward sequence of modules, ideal for simpler tasks.
    * Conditional: Different pipelines are selected based on specific conditions.
    * Branching: Parallel processing of multiple queries or retrieved documents to enhance diversity.
    * Loop: Iterative, recursive, and adaptive retrieval processes for complex, multi-step reasoning.
    * Tuning: Patterns for fine-tuning the retriever, generator, or both simultaneously.

## I would recommend to go through the Research Paper : https://arxiv.org/pdf/2407.21059",Bad
