title_blog,url_blog,author_blog,author_followers,claps,comments,publisher_followers,title_paper,url_paper,publisher_blog,blog_full_text,engagement_score,normalized_engagement_score,engagement_level
Recurrent drafter for fast speculative decoding in Large Language Models,https://medium.com/@sulbha.jindal/recurrent-drafter-for-fast-speculative-decoding-in-large-language-models-paper-review-d77cfc09cae4,Sulbha Jain,41,1,0,0.0,Recurrent drafter for fast speculative decoding in Large Language Models,https://arxiv.org/pdf/2403.09919,Sulbha Jain,"# Recurrent drafter for fast speculative decoding in Large Language Models— Paper Review

Paper Link — https://arxiv.org/pdf/2403.09919

Researchers at Apple have developed an innovative solution called ReDrafter to address the critical challenge of inference speed in large language models (LLMs). This novel approach tackles the inherent slowness of text generation in AI applications like chatbots by introducing a speculative decoding technique. ReDrafter leverages a recurrent neural network (RNN) to generate draft tokens in parallel with the main model, significantly reducing response latency.

## What is speculative decoding ?

Speculative decoding is a technique designed to speed up inference in large language models by utilizing smaller, more efficient models to predict potential sequences. Instead of generating each token one by one, these candidate sequences are quickly verified by the main LLM. This method helps alleviate memory bandwidth constraints by minimizing the number of passes through the LLM. However, it’s important to consider that using draft models can introduce additional overhead. Therefore, the reduction in LLM calls must be substantial enough to outweigh these costs and achieve a net speedup.

## Methodolgy

In this paper, the authors introduce ReDrafter, a novel approach for accelerating LLM inference. ReDrafter’s performance gains are driven by three key aspects:
    * RNN-based draft model: ReDrafter employs an RNN conditioned on the LLM’s hidden states as the draft model. It uses the output from the last layers of the LLM transformer and embeddings of historical tokens as inputs. This design leverages local temporal dependencies, enhancing draft prediction accuracy and converting computational resources into speedups.
    * Beam search and dynamic tree attention: The draft model utilizes beam search for inference, maintaining diversity and optimality in candidate response generation. ReDrafter applies a dynamic tree attention algorithm to eliminate duplicated prefixes among candidates, revealing a tree structure over beam search results and significantly reducing computational overhead.
    * Knowledge distillation training: The system is trained through knowledge distillation from the LLM, improving alignment between the draft model’s predictions and those of the LLM. This approach effectively shifts computational load from inference time to training time.

During each inference step, ReDrafter alternates between using a draft model to generate tokens and having the LLM verify and accept them. The system is most efficient when all candidate tokens are accepted, meaning the draft model’s predictions align with the LLM’s within a specified range T. The KL divergence serves as a natural loss function to optimize this alignment

## Results

ReDrafter significantly speeds up Vicuna inference in MT-Bench, achieving up to a 3.5x acceleration using a PyTorch implementation on Nvidia H100 GPUs. To demonstrate its practicality in production, ReDrafter was integrated into TensorRT-LLM, resulting in up to a 2.5x speedup on the same GPUs. Additionally, its effectiveness for on-device applications was confirmed by implementing the approach in MLX and testing it on Metal GPUs in Apple Silicon chips, where it achieved up to a 2.3x speedup.

The method achieves remarkable performance improvements by using a lightweight RNN draft model conditioned on the LLM’s hidden states. By employing techniques like beam search and dynamic tree attention, ReDrafter can accelerate text generation up to 3.5x on high-performance GPUs and 2.3x on Apple Silicon chips. This breakthrough promises to enhance the responsiveness of AI-powered applications, making interactions with large language models more seamless and efficient

Paper Link — https://arxiv.org/pdf/2403.09919",0.23708305259387408,6.0,Bad
How AI Is Changing the Way We Code,https://medium.com/towards-data-science/how-ai-is-changing-the-way-we-code-36ff30262e65,"Quentin Gallea, PhD",820,423,12,816000.0,From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow,https://arxiv.org/pdf/2308.11302,TDS Archive,"# How AI Is Changing the Way We Code

## Evidence from ChatGPT and Stack Overflow

In short: In this article, you will find a summary of my latest research on AI and work (exploring the effect of AI on productivity while opening up the discussion on the long-term effects), an example of a quasi-experimental method (Difference-in-Difference) illustrated with ChatGPT and Stack Overflow, and see how you can extract data from Stack Overflow with a simple SQL query.

Link to the full scientific article (please cite): https://arxiv.org/abs/2308.11302

As
with most technological revolutions, ChatGPT’s release was accompanied by fascination and fear. On one hand in just two months, with 100 millions monthly active users, the app broke the record for the fastest-growing consumer application in history. On the other hand, a report by Goldman Sachs claimed that such technology could replace more than 300 millions jobs globally [1]. Additionally, Elon Musk alongside more than 1,000 tech leaders and researchers signed an open letter urging for a pause on the most advanced AI developments [2].

``We can only see a short distance ahead, but we can see plenty that needs to be done.’’ Alan Turing

In line with Alan Turing’s quote, this article does not seek to predict heroically the distant future of AI and its impacts. However, I focus on one of the main observable consequences affecting us: How AI is changing the way we code.

T
he world changed with the birth of ChatGPT. At least, as someone who codes every day, my world changed overnight. Instead of spending hours on Google to find the right solution or digging into the answers on Stack Overflow and translating the solution to my exact problem with the right variables names and matrices dimensions, I could just ask ChatGPT. The chatbot would not only give me an answer in a blink of an eye but the answer would fit my exact situation (e.g. correct names, dataframes dimensions, variable types, etc.). I was blown away, and my productivity jumped suddenly.

Hence, I decided to explore the large-scale effect of ChatGPT release and its potential effect on productivity and ultimately on the way we work. I defined three hypotheses (Hs) that I tested using Stack Overflow data.

H1: ChatGPT decreases the number of questions asked on Stack Overflow. If ChatGPT can solve coding problems in seconds, we can expect a fall of questions on coding community platforms where asking a question and getting an answer takes time.

H2: ChatGPT increases the quality of the questions asked. If ChatGPT is used largely, the remaining questions on Stack Overflow must be better documented as ChatGPT might have already helped a bit.

H3: The remaining questions are more complex. We can expect that the remaining questions are more challenging as they could potentially not be answered by ChatGPT. Hence, to test this we are testing if the proportion of unanswered questions increases. In addition, I also test if the number of views per question changes. If the number of views per question is stable it would be an additional sign that the complexity of the remaining questions is increased and that this finding is not only caused by the reduced activity on the platform.

To test those hypotheses, I will exploit the sudden release of ChatGPT on Stack Overflow. In November 2022, when OpenAI released publicly their chatbot, no other alternatives were available (e.g. Google Bard), and the access was free (not limited to paid subscription as with OpenAI ChatGPT 4 or Code Interpreter). Hence it is possible to observe how the activity changed in the online coding community before and after the shock. However, despite how ‘clean’ this shock is, other effects might be confounded and hence question causality. In particular, seasonality (e.g. end of the year holidays after the release) as well as the fact that the more recent the question is, the lower the number of views and the probability that an answer is found.

Ideally, to mitigate the influence of potential lingering confounding variables such as seasonality and measure a causal effect, we would like to observe the world without ChatGPT release which is impossible (e.g. the fundamental problem of causal inference). Nevertheless, I will address this challenge by exploiting the fact that the quality of the answers of ChatGPT for coding-related issues varies from one language to another and use quasi-experimental methods to limit the risk of other factors confounding the effect (Difference-in-Difference).

To do so, I will compare the activity on Stack Overflow between Python and R. Python is an obvious choice as it, is arguably, one of the
most popular programming languages used (e.g. ranked 1st in the TIOBE
Programming Community Index). The large set of resources online for Python provides a rich training set for chatbots like ChatGPT. Now, to compare with Python, I chose R. Python is often cited as the best replacement for R and both are freely available. However, R is somewhat less popular (e.g.~16th in the TIOBE Programming Community index) and hence the training data might be smaller, implying poorer performance by ChatGPT. Anecdotal evidence confirmed this difference (more details on the method in the Method section). Hence, R represents a valid counter factual for Python (it is affected by seasonality but we can expect a negligible effect of ChatGPT).

The Figure above presents the raw weekly data. We can witness the sudden and important drop (21.2%) in the number of questions asked weekly on Stack Overflow about Python after the release of ChatGPT 3.5 while the effect on R is somewhat smaller (drop of 15.8%).

These ‘qualitative’ observations are confirmed by the statistical model. The econometric model described later finds a statistically significant drop of 937.7 (95% CI: [-1232.8,-642.55 ] ; p-value = 0.000) weekly questions on average for Python on Stack Overflow. The subsequent analysis, utilizing the Diff-in-Diff method, further unveils an improvement in question quality (measured on the platform by a score), alongside an increase in the proportion of questions remaining unanswered (while the average number of views per question seems unchanged). Consequently, this study provides evidence for the three hypotheses defined earlier.

These findings underscore the profound role of AI in the way we work. By addressing routine inquiries, generative AI empowers individuals to channel their efforts toward more complex tasks while boosting their productivity. However, important long-term potential adverse effects are also discussed in the Discussion section.

The rest of the article will present the Data and Methods, then the Results, and will close with the Discussion.

# Data

The data have been extracted using an SQL query on the Stack Overflow data explorer portal (licence: CC BY-SA). Here is the SQL command used:

I then aggregated the data by week to reduce the noise and hence obtained a dataset from Monday the 17th of October 2022 to the 19th of March 2023 with information on the number of weekly posts, the number of views, the number of views per questions, the average score per question and the proportion of unanswered question. The score is defined by users of the platform who can vote up or down to say if the question shows “research effort; it is useful and clear” or not.

# Method

In order to measure a causal effect, I use a Difference-in-Difference model which is an econometric method that exploits usually a change over time and compares a treated unit(s) with an untreated group. In order to know more about this method I can recommend you to read the chapter referring to this method in two free e-books: Causal Inference Inference for the Brave and True and Causal Inference: The Mixtape.

In simple terms, the Diff-in-Diff model computes a double difference in order to identify a causal effect. Here is a simplified explanation. First, the idea is to compute two simple differences: the ‘average’ difference between the pre (before ChatGPT release) and post-period for the two groups treated and untreated (here respectively Python and R questions). What we care about is the effect of the treatment on the treated units (here is the effect of ChatGPT release on Python questions). However, as said earlier, there might be another effect still confounded with the treatment (e.g. seasonality). In order to address this issue, the idea of the model is to compute a double difference, in order to check how the first difference for the treated (Python) is different from the second (difference for the control group, R). As we expect no treatment effect (or negligible) on the control group, while still affected by seasonality for example, we can get rid of this potential confounding factor and ultimately measure a causal effect.

Here is a slightly more formal representation.

First difference for the treated group:

E[Yᵢₜ| Treatedᵢ, Postₜ]-E[Yᵢₜ| Treatedᵢ, Preₜ] = λₜ+β

Here i and t refer respectively to the language (R or Python) and to the week. While treated refer to the questions related to Python and Post refers to the period when ChatGPT was available. This simple difference might represent the causal effect of ChatGPT (β) + some time effect λₜ (e.g. seasonality).

First difference for the control group:

E[Yᵢₜ| Controlᵢ, Postₜ]-E[Yᵢₜ| Controlᵢ, Preₜ] = λₜ

The simple difference for the control group does not include the treatment effect (as it is untreated) but only the λₜ.

Hence the double difference will give:

DiD = ( λₜ+β) — λₜ = β

Under the assumption that the λₜ are identical for both groups (parallel trend assumption, discussed below), the double difference will allow us to identify β, the causal effect.

The essence of this model lies in the parallel trend assumption. In order to claim a causal effect we should be convinced that without ChatGPT the evolution of posts on Stack Overflow for Python (treated) and for R (untreated) would be the same in the treatment period (after November 2022). However, this is obviously impossible to observe and hence to test directly (c.f. the Fundamental Problem of Causal Inference). (If you want to learn more about this concept and causal inference find my videos and articles on Towards Data Science: the Science and Art of Causality). However, it is possible to test if the trends are parallel before the shock, suggesting that the control group is a potentially good “counterfactual”. Two different placebo tests made with the data revealed that we cannot reject the parallel trend assumption for the pre-ChatGPT period (p-values of the tests respectively 0.722 and 0.397 (see online APPENDIX B)).

Formal definition:

Yᵢₜ = β₀ + β₁ Pythonᵢ + β₂ ChatGPTₜ + β₃ Pythonᵢ × ChatGPTₜ + uᵢₜ

“i” and “t” correspond respectively to the topic of the question on Stack Overflow (i ∈ {R; Python}) and the week. Yᵢₜ represents the outcome variable: Number of questions (H1), Average question score (H2), and proportion of unanswered questions (H3). Pythonᵢ is a binary
variable taking the value 1 if the question is related to Python and 0
otherwise (related to R). ChatGPTₜ is another binary variable
taking the value 1 from the release of ChatGPT and onwards and 0
otherwise. uᵢₜ is an error term clustered at the coding language
level (i).

The essence of this model lies in the parallel trends assumption. In order to claim a causal effect we should be convinced that without ChatGPT the evolution of posts on Stack Overflow for Python (treated) and for R (untreated) would be the same in the treatment period (after November 2022). However, this is obviously impossible to observe and hence to test directly (c.f. the Fundamental Problem of Causal Inference). (If you want to learn more about this concept and causal inference find my videos and articles on the Science and Art of Causality). However, it is possible to test if the trends are parallel before the shock, suggesting that the control group is a good “counterfactual”. In this case, two different placebo tests reveal that we cannot reject the parallel trends assumption for the pre-ChatGPT period (p-values of the tests respectively 0.722 and 0.397 (see online APPENDIX B)).

# Results

## H1: ChatGPT decreases the number of questions asked on Stack Overflow.

As presented in the introduction, the Diff-in-Diff model estimates a statistically significant drop of 937.7 (95% CI: [-1232.8, -642.55] ; p-value = 0.000) weekly questions on average for Python on Stack Overflow (see Figure 2 below). This represents a fall of 18% in weekly questions.

## H2: ChatGPT increases the quality of the questions asked.

ChatGPT might be helpful to answer questions (c.f. H1). However, when the chatbot cannot solve the issue, it is possible that it allows one to go further and get more information on the problem or some element of the solution. The platform allows us to test this hypothesis as users can vote for each question if they think that “This question shows research effort; it is useful and clear” (increase the score by 1 point), or not (decrease the score by 1 point). This second regression estimates that there is a 0.07 points (95% CI: [-0.0127 , 0.1518 ]; p-value: 0.095) increase in the questions’ score on average (see Figure 3) which represents a 41.2% increase.

## H3: The remaining questions are more complex.

Now that we have some pieces of evidence that ChatGPT is able to provide significant help (solve questions and help document the others), we would like to confirm that the remaining questions are more complex. To do so, we are going to look at two things. First, I find that the proportion of unanswered questions is raising (no answer could be a sign that the questions are more complex). More precisely I find a 2.21 percentage point (95% CI: [ 0.12, 0.30]; p-value: 0.039) increase in the proportion of questions unanswered (see Figure 4) which represents an increase of 6.8%. Second, we also find that the number of views per question is unchanged (we cannot reject the null hypothesis that it is unchanged, with a p-value of 0.477). This second test allows us to partially discard the alternative explanation that there are more unanswered questions due to the lower traffic.

# Discussion

These findings support the view that generative AI could revolutionize our work by taking care of routine questions, allowing us to focus on more complex problems requiring expertise while boosting our productivity.

While this promise sounds exciting there is a reverse of the medal. First, low-qualified work might be replaced by chatbots. Second, such tool might affect (negatively) the way we learn. Personally, I see coding as biking or swimming: watching videos or following classes is not enough, you have to try and fail yourself. If the answers are too good and we don’t force ourselves to study, many people might struggle to learn. Third, if the mass of questions on Stack Overflow fall, it might reduce a valuable source for the training set of generative AI models hence, affecting their long-term performance.

All those long term adverse effects are not clear yet and require careful analysis. Let me know what you think in the comments.

[0] Gallea, Quentin. “From Mundane to Meaningful: AI’s Influence on Work Dynamics — evidence from ChatGPT and Stack Overflow” arXiv econ.GN (2023)

[1] Hatzius, Jan. “The Potentially Large Effects of Artificial Intelligence on Economic Growth (Briggs/Kodnani).” Goldman Sachs (2023).

[2] https://www.nytimes.com/2023/03/29/technology/ai-artificial-intelligence-musk-risks.html

[3] Bhat, Vasudev, et al. “Min (e) d your tags: Analysis of question response time in stackoverflow.” 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014). IEEE, (2014)",3.56104654683719,86.0,Excellent
Data Centric AI — LLAVA,https://medium.com/@amansinghalml_33304/data-centric-ai-llava-0fd3c51dd359,Amansinghalml,12,7,0,0.0,Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485,Amansinghalml,"# Data Centric AI — LLAVA

https://arxiv.org/abs/2304.08485

What are the major contributions of the paper?

[1] They are able to use bounding box or image caption information from images to ask GPT-4 to generate 3 kinds of questions and their answer [a] High-level details [b] Conversations [c] Deep level details

[2] They use this data to fine-tune an end to end model

What is the architecture of the model that they fine-tune?

It consists of the image encoder part of clip followed by a projection matrix. The matrix projects the image embeddings to the same dimension as the size of the input embedding to a LLAMA model. This is obviously somehow appended to the generated questions embeddings and then the model is used to generate answers as a decoder normally does

What do they keep frozen/what do they finetune?

There are 2 stages, stage [1] only the projection matrix for the clip is finetuned so that we can get good representations for the language model. [2] Then the image encoder is kept frozen and the projection matrix and the language model is updated

Which datasets do they use for each step?

The model contains an image encoder that is projected to the language embedding dimension. Note that this encoder is from a CLIP model. Moreover, the text decoder is a Vicuna model. First, they need to adapt this projection layer from the image embedding dimension to the text embedding dimension. For this, the parameters are kept frozen and the projection layer is updated using the C3M (or Conceptual Captions dataset). This is followed by updating the vicuna model and the projection layer, while keeping clip encoder frozen using the COCO dataset. Finally, the model performance is evaluated on the Science QA dataset.

Why use COCO for second stage finetuning and C3M for the first stage even though C3M is created by humans (online scraping)?

This is because the COCO questions and answers are augmented using the prompts mentioned in the paper and hence, the dataset is very detailed.

How is the C3M dataset collected?

Using ALT text and corresponding images. The dataset filteration is quite informative: [1] Image only: Filter images based on format, aspect ratio, image resolution [2] Text only: Missing proper nouns, determiners, conjunctions or containing several nouns or caps at improper places, SEO terms then filter out. Pornographic content [3] Image-text [4] We want to have a diverse dataset with some standardization so that the task does not become too difficult for something like VQA (e.x. generating caption that this is Justin Bieber singing is difficult as it would require a large number of pictures of justin bieber). So we use Hpernyms and replace proper nouns with hypernyms, remove place names etc. [5] Finally we identify concepts in the images using the captions and make sure that we are able to cluster them such that each concept has enough support example 100

Overall we want to make sure that we have a variety of concepts with enough coverage

How is this different from how LAION is collected?

Very similar, using different models for filtering pornographic and SMID (socio-moral image dataset) content or morally offensive content. When we are inputting to a model we want to ensure that hefty class imbalance is avoided

Why first finetune with C3M then when its so diverse?

Because the dataset generated in the LLaVa paper using prompting has much more complex discussions

How is the model evaluated?

https://llava-vl.github.io/

It mainly uses the Science QA dataset, which is collected for K-12 education. For ground truth explanation GPT-4 is provided with image information and the question and the answer and is then used to evaluate the answer generated by the LLaVa model. Moreover, there is an inherent notion of accuracy as the questions have multiple options.

How is GPT used for ensembling?

GPT is used to generate answers and is also used as the judge. When GPT cannot provide good answers LLaVa is used hence enabling ensembling

What are the dimensions of the datasets?

C3M — is 3M but only 600K are filtered COCO — is used because it has bounding boxes. Around 160K. It has conversations, complex question answering etc.

For the dataset filtration the filtering of C3M here is quite interesting and supplants beliefs. How is it filtered? Look at noun phrases (words) in each caption, remove the ones with low frequency and while adding the remaining ones makes sure that none has a fz higher than 100 so as to keep the data balanced

What are some potential applications of LLaVa?

Giving out recopies based on ingredients shown are particularly interesting as they can be extended to robotics with RL based approaches",1.3161665735625547,32.0,Average
Building with Blocks: Modular RAG for Flexible and Powerful LLMs,https://medium.com/@yash9439/building-with-blocks-modular-rag-for-flexible-and-powerful-llms-3dba3ab1a7ce,Yash Bhaskar,952,2,0,0.0,Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,https://arxiv.org/pdf/2407.21059,Yash Bhaskar,"# Building with Blocks: Modular RAG for Flexible and Powerful LLMs

Paper Link : https://arxiv.org/pdf/2407.21059

Large Language Models (LLMs) have revolutionized how we interact with information, but they’re not without their shortcomings. Hallucinations, outdated knowledge, and struggles with complex reasoning remain persistent challenges. Retrieval Augmented Generation (RAG) has emerged as a powerful solution, enhancing LLMs by grounding them in external knowledge. However, as RAG systems grow more sophisticated, their complexity becomes a hurdle. A new research paper, “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,” proposes a novel approach to address this challenge by taking inspiration from the world of building blocks.

This paper introduces a modular framework that reimagines RAG systems as flexible and adaptable structures. Instead of a rigid pipeline, Modular RAG envisions a system of interchangeable components, allowing developers to tailor the retrieval and generation process to specific needs. This LEGO-like approach offers greater control, transparency, and maintainability, opening up exciting possibilities for the future of LLM applications.

# The Evolution of RAG:

The paper outlines the evolution of RAG from its naive beginnings to the more advanced systems we see today.
    * Naive RAG: Early RAG systems followed a simple “retrieve-then-generate” process. These systems often struggled with complex queries and noisy data.
    * Advanced RAG: This iteration introduced pre- and post-retrieval processing to refine queries and filter results. Techniques like query rewriting and reranking improved accuracy, but the system remained largely a linear pipeline.
    * Modular RAG: This new paradigm breaks down the RAG process into independent modules: Indexing, Pre-retrieval, Retrieval, Post-retrieval, Generation, and Orchestration. These modules, further divided into sub-modules and operators, offer granular control over the entire process.

# Building Blocks of Modular RAG:

Each module in Modular RAG plays a specific role in refining the information flow to the LLM:
    * Indexing: This module optimizes how external knowledge is stored and accessed, including techniques like chunk optimization and structural organization using knowledge graphs.
    * Pre-retrieval: This stage refines the user’s query before retrieval, employing methods like query expansion, transformation, and construction (especially useful for structured data).
    * Retrieval: This crucial module selects the appropriate retriever (sparse, dense, or hybrid) and fine-tunes it for optimal performance.
    * Post-retrieval: After retrieving relevant information, this module refines the results through reranking, compression, and selection, ensuring the LLM receives the most pertinent data.
    * Generation: This stage utilizes the LLM to generate the final answer, incorporating fine-tuning and verification mechanisms to enhance accuracy and reliability.
    * Orchestration: This module acts as the control center, managing the flow of information between modules using routing, scheduling, and fusion mechanisms. This allows for dynamic adaptation and complex workflows, including iterative, recursive, and adaptive retrieval.

# RAG Flow Patterns:

The paper identifies several common RAG flow patterns that emerge from the combination of these modules:
    * Linear: A straightforward sequence of modules, ideal for simpler tasks.
    * Conditional: Different pipelines are selected based on specific conditions.
    * Branching: Parallel processing of multiple queries or retrieved documents to enhance diversity.
    * Loop: Iterative, recursive, and adaptive retrieval processes for complex, multi-step reasoning.
    * Tuning: Patterns for fine-tuning the retriever, generator, or both simultaneously.

## I would recommend to go through the Research Paper : https://arxiv.org/pdf/2407.21059",0.25585199540500264,6.0,Bad
Advancements in Reinforcement Learning Research Using Soccer Data,https://medium.com/@keisuke198619/advancements-in-reinforcement-learning-research-using-soccer-data-131952d3e8a0,Keisuke Fujii,25,3,1,0.0,Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning,https://arxiv.org/pdf/2305.17886,Keisuke Fujii,"# Advancements in Reinforcement Learning Research Using Soccer Data

Last month, we published a paper based on deep reinforcement learning that simultaneously evaluates the actions of multiple soccer players.

As an associate professor at Nagoya University, I focus on research in machine learning for sports. For those new to my work, please refer to this link.

Reinforcement learning is a framework where agents are modeled to earn rewards. They learn policies to output actions from states by interacting with their environment, including other agents. This approach has famously surpassed human skills in board games like chess and go.

A landmark in soccer-related reinforcement learning research was the 2020 release of Google Research Football (GFootball). It’s a platform that allows easy experimentation with reinforcement learning algorithms in Python, leading to numerous studies being presented at top international conferences in machine learning like NeurIPS, ICLR, and ICML. Unlike board games, having 22 freely moving agents in a continuous space makes 11 vs. 11 scenarios challenging. However, setting up smaller problems like 3 vs. 1 or 4 vs. 2, and defining discrete actions similar to video games, has made problem-solving more accessible and likely contributed to its popularity.

Our research team, traditionally focused on sports data analysis, pondered if such agent models could evaluate real players’ actions. Our study, using the GFootball model, is the first to assess actual soccer players’ actions.

Previous studies using reinforcement learning to evaluate players in team sports existed, like estimating a Q-function to value actions like shooting or passing. These studies often considered the team as a single agent, unable to evaluate off-ball players at every time step. For details, see the ‘Related work’ section of our paper.

Our approach involved a simple deep learning model to estimate Q-values for each action (see also the figure below). Inputs included the positions and velocities of all players and the ball. Actions, mimicking GFootball, included eight-directional movements and actions like shooting (sh) or passing (p), with a neural network outputting Q-values for each action. For more details, see the ‘Method’ section of our paper.

As an example, consider the opening figure where player A has the ball and is passing to player B. We calculated the Q-values for each action, with the pass (p) having the highest Q-value, indicating it as the most effective action.

We averaged these Q-values for individual players and examined their correlation with season scores for prominent Yokohama F. Marinos players (excluding goalkeepers) in the 2019 J1 League. A negative correlation was observed between the average Q-values and season scores. It’s challenging to determine an exact correlation due to players scoring zero or one goal, but the trend seemed to favor evaluating DF and MF players over FWs. This might be due to our parameter settings considering all passes and off-ball movements related to scoring. For more details, see the ‘Results’ section of our paper.

This research, a first attempt with simple parameters, suggests that more accurate modeling could provide more precise evaluations. Additionally, another paper about creating a reinforcement learning-based simulator that generates actions similar to real soccer players was accepted at the ICAART 2024, an international conference on agent research. More details will be available soon, and I look forward to discussing this further.",1.0443553292697192,25.0,Average
Beyond the Black Box: Inside the workings of LLMs,https://medium.com/@vishalmisra/beyond-the-black-box-inside-the-workings-of-llms-435bc82da7e9,Vishal Misra,358,74,0,0.0,Beyond the Black Box: A Statistical Model for LLM Reasoning and Inference,https://arxiv.org/pdf/2402.03175,Vishal Misra,"# Beyond the Black Box: Inside the workings of LLMs

# Exploring the Inner Workings of Large Language Models: A Statistical Perspective

For those interested in a more accessible, audio-based explanation of our work, I’m linking a podcast that discusses the key concepts of our paper. This podcast offers a conversational deep dive into the ideas presented here, providing additional context and examples that may be helpful for non-technical audiences.

You can find the (short, 13 minute) podcast here. It’s an excellent companion piece to this blog post, offering a perspective on our research and its implications for the field of AI. The best part, the podcast itself is completely AI generated, using the NotebookLM service of Google, and I must say the result is incredible!

# Our Paper

We have been investigating the mechanisms behind Large Language Models (LLMs) like GPT-3 and ChatGPT. In our recent paper, “Beyond the Black Box: A Statistical Model for LLM Reasoning and Inference,” my colleague prof. Siddhartha Dalal and I propose a framework for understanding how these models might function. We’d like to share some of our key findings and their potential implications.

# The Concept of a Probability Matrix

At the core of our model is the idea of an abstract probability matrix. Imagine a vast table where:
    * Each row represents a possible input (or “prompt”) to the LLM
    * Each column represents a word in the model’s vocabulary
    * Each cell contains the probability of that word being the next one in the sequence

This matrix, if it could exist, would essentially contain all possible language knowledge. In practice, it’s far too large to be explicitly represented. With a vocabulary of 50,000 tokens and prompts of 8,000 tokens, we’re dealing with 50,000⁸⁰⁰⁰ rows — a number larger than the atoms in the observable universe. LLMs, therefore, are essentially approximating this matrix based on their training data.

# Embeddings and Continuity

Our work explores the relationship between embeddings (vector representations of words or phrases) and probability distributions over the vocabulary. We prove a continuity theorem which suggests that similar inputs should produce similar probability distributions for the next word.

Specifically, if the mapping between embeddings and multinomial distributions preserves convexity, it must be continuous. This property helps explain how LLMs can generalize to novel inputs, as similar prompts in the embedding space will lead to similar next-token predictions.

# A Bayesian Learning Perspective

We propose that LLM text generation aligns with principles of Bayesian learning:
    * The pre-trained model provides a prior distribution.
    * The input prompt serves as new evidence.
    * The model computes a posterior distribution for next token prediction.

This framework offers an explanation for in-context learning, where LLMs adapt to new tasks given just a few examples. Our model suggests this could be viewed as rapid Bayesian updating. When an LLM is given examples of a new task within the prompt, it’s essentially performing quick probability updates based on this new information.

# Visualizing LLM Behavior

To test our hypotheses, we modified an open-source LLM based on the Llama architecture to visualize its next-token probabilities during inference. These visualizations offer insights into how the model’s confidence changes as it processes inputs and adapts to new tasks.

# Understanding the Visualizations

Our visualization technique provides a color-coded representation of the model’s confidence in its next-token predictions:
    * Red indicates low confidence: The model is highly uncertain about the next token.
    * Orange suggests moderate confidence: The model has some idea of what might come next, but isn’t strongly committed.
    * Green represents high confidence: The model is very sure about its prediction for the next token.

This color scheme allows us to visually track how the model’s certainty evolves as it processes a prompt. For instance, when presented with a new task or unfamiliar content, we often see a predominance of red and orange, indicating uncertainty. As the model encounters more context or in-context examples, we typically observe a shift towards green, signaling increased confidence.

In our experiments with domain-specific languages, this shift was particularly pronounced. Initially, the completions were dominated by red and orange hues. However, after processing just a few examples, subsequent completions showed a marked increase in green, demonstrating the model’s rapid adaptation to the new syntax and semantics.

These visualizations not only support our theoretical framework but also offer a new tool for understanding and potentially debugging LLM behavior. They provide a window into the model’s internal decision-making process, helping us to better understand how these complex systems arrive at their outputs.

# A Concrete Example: Learning a Cricket Domain Specific Language

To illustrate how our model explains in-context learning, let’s walk through a detailed example from our experiments, and using Figure 2 as the running example. We’ll show how an LLM learns to translate natural language queries about cricket statistics into a specialized Domain Specific Language (DSL).

# The Task

Our goal is to have the LLM translate natural language questions about cricket statistics into a structured DSL format. This DSL is entirely unknown to the model before we start providing examples.

# The Process

We’ll provide the model with a few examples of natural language queries and their corresponding DSL representations. Then, we’ll ask it to translate a new query.

# Example 1: Setting the Stage

Natural Language Query: “Tournament0 team with best win loss record after losing the toss”

DSL Representation:

{

‘orderby’: [‘win_loss_ratio’],

‘toss’: [‘lost’],

‘tournament’: [‘Tournament0’],

‘type’: [‘team’]

}

At this point, the model has seen one example. Its probability distribution for the next tokens is likely still quite uncertain, as reflected in our visualizations by predominantly red and orange colors.

# Example 2: Building Context

Natural Language Query: “lowest team total”

DSL Representation:

{

‘groupby’: [‘innings’],

‘orderby’: [‘runs’],

‘sortorder’: [‘reverse’],

‘type’: [‘team’]

}

After this second example, the model starts to recognize patterns. It’s learning that queries are translated into JSON-like structures with specific keys like ‘orderby’, ‘type’, etc. Our visualizations would likely show a mix of orange and green, indicating increasing confidence in certain tokens.

# Example 3: Reinforcing the Pattern

Natural Language Query: “biggest Tournament0 total in defeat”

DSL Representation:

{

‘groupby’: [‘innings’],

‘orderby’: [‘runs’],

‘result’: [‘loss’],

‘tournament’: [‘Tournament0’],

‘type’: [‘team’]

}

This third example further reinforces the pattern. The model is now quite familiar with the structure of the DSL and how different natural language phrases map to specific DSL elements. Our visualizations at this point would show predominantly green for the structure of the JSON and the common keys.

# The Test: A New Query

Now, let’s present the model with a new query it hasn’t seen before:

Natural Language Query: “highest losing team total in Tournament0”

# The Model’s Response

Based on our Bayesian learning framework, the model combines its prior knowledge (pre-training on general language) with the new evidence (the three examples) to generate a posterior distribution. This posterior guides the generation of the DSL representation for the new query.

The model produces:

{

‘groupby’: [‘innings’],

‘orderby’: [‘runs’],

‘result’: [‘loss’],

‘tournament’: [‘Tournament0’],

‘type’: [‘team’]

}

# Analysis of the Response

Structure: The model has correctly learned the JSON-like structure of the DSL.

Key Concepts:
    * It correctly identifies that ‘Tournament0’ should be in the ‘tournament’ field.
    * It understands that “highest” corresponds to ‘orderby’: [‘runs’].
    * It correctly interprets “losing” as ‘result’: [‘loss’].

Generalization: The model has generalized from the examples to handle a new combination of concepts. It hasn’t seen “highest losing team total” exactly, but it combines its understanding of “highest”, “losing”, and “total” to generate the correct DSL.

Confidence: Our visualizations for this generation would likely show mostly green, especially for the structural elements and common fields. There might be some orange for the specific ordering of fields, as this can vary.

# Alignment with Our Model

This example demonstrates several key aspects of our proposed model:
    * Bayesian Learning: The model starts with a prior (its pre-trained knowledge) and updates it with each example, forming a posterior that better reflects the task at hand.
    * Rapid Adaptation: With just three examples, the model adapts to a completely new DSL, showcasing the power of in-context learning.
    * Embedding Space Navigation: The model is effectively navigating its embedding space, finding representations that map the natural language input to appropriate DSL outputs.
    * Continuity: Similar queries (in embedding space) result in similar DSL outputs, aligning with our continuity theorem.

This example provides concrete evidence of how LLMs perform in-context learning, supporting our theoretical framework and offering insights into the inner workings of these powerful models.

# Navigating the Embedding Space

Our model conceptualizes LLM operation as navigation through a high-dimensional embedding space:
    * Pre-trained knowledge forms a landscape of embeddings.
    * The prompt introduces new points in this space.
    * Token prediction becomes an optimization problem in this space.

This perspective helps explain both the strengths and limitations of LLMs, including why they might produce inconsistent or “hallucinated” outputs when dealing with unfamiliar inputs.

# Implications for LLM Behavior

Our model, as detailed in Section 6.5 of the paper, provides insights into several key behaviors of modern LLMs:

# Adaptability

The continuous updating of embeddings and probability distributions explains the remarkable adaptability of LLMs. They can quickly adjust to new information provided in the prompt, even outside of explicit in-context learning scenarios. This adaptability stems from the model’s ability to rapidly update its internal representations based on new input.

# Coherence

By maintaining a consistent embedding context, LLMs can generate text that remains coherent over long passages. The model’s ability to navigate the high-dimensional embedding space while preserving context allows for the production of extended, logically consistent text.

# Contextual Understanding

The model’s capacity to combine embeddings from different parts of the prompt enables it to capture and utilize complex contextual relationships. This explains how LLMs can understand and generate responses that take into account subtle nuances and long-range dependencies in the input.

# Human-like Quality

The dynamic nature of the embedding and probability updating process contributes to the human-like quality of LLM-generated text. Each new token can potentially shift the embedding landscape, allowing for subtle changes in direction and tone that mimic human thought processes.

# Limitations

Our model also sheds light on several key limitations of LLMs:
    * Prompt Sensitivity: The heavy reliance on prompt embeddings can lead to high sensitivity to prompt wording and order. This explains why LLMs might produce inconsistent outputs for semantically similar queries with different phrasings and why “prompt engineering” is really an “art” and not engineering.
    * Context Window Constraints: As the prompt grows longer, earlier parts may have diminished influence, as their embeddings become less relevant in the linear combination. This shines a light on the challenge LLMs face with truly long-term dependencies.
    * Hallucination: When the model encounters prompts that map to unfamiliar regions of the embedding space, it may generate plausible-sounding but factually incorrect information by combining embeddings in novel, potentially erroneous ways.
    * Bias Amplification: Pre-existing biases in the training data can be amplified through the embedding combination process, potentially leading to skewed or unfair outputs, especially for underrepresented concepts or groups.
    * Lack of Causal Understanding: While embeddings capture semantic relationships, they don’t inherently encode causal structures. This can lead to LLMs making logically incorrect inferences or failing to understand cause-and-effect relationships accurately.
    * Computational Scalability: As the embedding space grows with model size and context length, the computational complexity of finding optimal embedding combinations increases, potentially limiting the scalability of this approach for even larger language models.

Understanding these behaviors and limitations through the lens of our statistical model provides valuable insights for both the development and application of LLMs. It offers a framework for addressing current shortcomings and guides future research directions in the field of AI language models.

# Key Insights and Future Directions

Our paper’s Section 7 offers several important insights and potential avenues for future research:

# The Importance of Embeddings

Our model highlights the critical role of high-quality embeddings in LLM performance. The continuity property we proved suggests that well-designed embeddings should accurately capture semantic relationships between words and phrases. This implies that advances in embedding techniques could lead to significant improvements in LLM capabilities.

# Rethinking World Knowledge Integration

We propose that it might be optimal to train LLMs primarily on language and logic, while introducing world knowledge or domain-specific information via prompts. This approach, often referred to as retrieval-augmented generation (RAG), allows the model to incorporate new information through Bayesian posterior updates. This could potentially lead to more flexible and updatable LLMs.

# Understanding Chain-of-Thought Reasoning

Our framework provides a potential explanation for the effectiveness of chain-of-thought reasoning. By breaking complex tasks into simpler steps, the model can leverage its prior knowledge more effectively at each stage. This aligns with our Bayesian learning perspective, as each step allows for a more focused update of the relevant probability distributions.

# Addressing the Hallucination Problem

We suggest that analyzing the entropy of the model’s probability distributions could help identify potential hallucinations. Low-entropy (highly confident) outputs in unfamiliar contexts might be flags for hallucinated content. This insight could lead to the development of more reliable LLMs with better error detection mechanisms.

# Architecture Considerations

While our model is architecture-agnostic, it suggests that the key to improving LLMs lies in better approximating the ideal probability matrix, regardless of the specific neural network design. This perspective could guide future research in LLM architecture, focusing on models that can more accurately capture and update complex probability distributions.

# Ethical and Safety Implications

Understanding LLMs as statistical models approximating a vast probability matrix raises important ethical considerations. It underscores the need for careful curation of training data and the potential biases that could be encoded in the learned probabilities. Moreover, it highlights the importance of developing robust methods for updating these probabilities to correct for biases or incorporate new information.

These insights open up numerous exciting research directions. From developing more sophisticated embedding techniques to exploring novel architectures optimized for Bayesian-like updates, there’s a wealth of potential advancements on the horizon. As we continue to refine our understanding of LLMs, we hope these perspectives will contribute to the development of more powerful, reliable, and ethically-aligned AI systems.

# Update (January 2024): Chain of Thought and the DeepSeek Paper — From Stochastic Parrots to Algorithmic Thinking

A recent paper from DeepSeek AI (January 2024) provides fascinating empirical validation of our statistical model’s predictions about chain-of-thought reasoning. Their work shows how pure reinforcement learning, without supervised fine-tuning, can help models naturally discover step-by-step reasoning patterns. Their results align perfectly with our statistical model’s explanation of why chain-of-thought (CoT) reasoning works so well in large language models. Consider how humans solve complex problems — when faced with a difficult math problem, we don’t try to immediately guess the answer. Instead, we break it down into manageable steps that we’re confident about.

Our statistical model helps explain why CoT works: When an LLM breaks down a complex problem into steps, each step’s next-token probability distribution becomes highly concentrated (confident) because:
    * The simpler sub-problems are more likely to have appeared in training data in some form
    * The Bayesian learning process can leverage these prior experiences more effectively
    * The model’s embeddings for each step map to more focused probability distributions

For instance, when multiplying 143 × 768, directly predicting “109,824” would require the model to guess from an extremely diffuse probability distribution over all possible numbers. But by breaking it down step-by-step:
    * First multiply 143 × 8 = 1144
    * Then 143 × 60 = 8580
    * Finally 143 × 700 = 100100 The model can be highly confident about each individual arithmetic operation because:
    * Basic multiplication facts appear frequently in training data
    * Each step’s probability distribution over next tokens becomes highly concentrated
    * The Bayesian posterior focuses sharply on the correct next digits

This explains why DeepSeek’s reinforcement learning approach, which allows the model to discover step-by-step reasoning patterns naturally, leads to better performance than trying to train the model to directly output answers. The RL process effectively teaches the model to transform diffuse probability distributions into sequences of concentrated ones — moving from “stochastic parrot” behavior to genuine algorithmic thinking.

# Conclusion

While our model is an abstraction of the complex processes within LLMs, we believe it offers a useful perspective for understanding their behavior. It provides a framework for viewing LLMs not as inscrutable neural networks, but as statistical models approximating an ideal language model through embeddings and Bayesian learning.

As research in this field progresses, we hope our work contributes to the development of more powerful, reliable, and interpretable AI language models. There’s still much to explore and validate, but we believe this statistical perspective opens up new avenues for investigation and improvement.

We welcome feedback and further discussion on these ideas as we continue to explore the fascinating world of large language models.",2.6084454655312928,63.0,Good
Model-Free Risk-Sensitive Reinforcement Learning,https://deepmindsafetyresearch.medium.com/model-free-risk-sensitive-reinforcement-learning-5a12ba5ce662,DeepMind Safety Research,3000,78,0,0.0,Model-Free Risk-Sensitive Reinforcement Learning,https://arxiv.org/pdf/2111.02907,DeepMind Safety Research,"# Model-Free Risk-Sensitive Reinforcement Learning

By the Safety Analysis Team: Grégoire Delétang, Jordi Grau-Moya, Markus Kunesch, Tim Genewein, Rob Brekelmans, Shane Legg, and Pedro A. Ortega

Read our paper here: https://arxiv.org/abs/2111.02907

W
e’re all familiar with risk-sensitive choices. As you look out of the window, you see a few gray clouds and no rain, but decide to take along the umbrella anyway. You’re convinced your application will be successful, but you apply for other positions nevertheless. You hurry to get to an important appointment on time, but avoid the highway just in case there could be a traffic jam. Or you buy a lottery ticket all the while you know the chances of winning are unreasonably slim. All these are instances of risk-sensitive behavior — mostly risk-averse but occasionally risk-seeking too. This means we tend to value uncertain events less/more than their expected value, preempting outcomes that go against our expectations.

# Why risk-sensitivity?

Most reinforcement learning algorithms are risk-neutral. They collect data from the environment and adapt their policy in order to maximize the expected return (sum of future rewards). This works well when the environment is small, stable, and controlled (a “closed world”), such as when agents are trained long enough on a very accurate simulator, so as to familiarize themselves entirely with its details and intricacies. Risk-neutral policies, because of the trust they have in their knowledge, can afford to confidently “put all the eggs into the same basket”.

These assumptions often do not hold in real-world applications. The reasons abound: the training simulator could be inaccurate, the assumptions wrong, the collected data incomplete, the problem misspecified, the computation limited in its resources, and so on. In addition, there might be competing agents reacting in ways that can’t be anticipated. In such situations, risk-neutral agents are too brittle: it could take a single mistake to destabilize their policy and lead to a catastrophic breakdown. This poses serious AI safety problems.

# How do agents learn risk-sensitive policies?

Just as we go about in our daily lives, we can address the previous shortcomings using risk-sensitive policies. Such policies differ from their risk-neutral counterparts in that they value their options differently: not only according to their expected return, but also to the higher-order moments, like the variance of the return, the skewness, etc. Simply stated, risk-sensitive agents care about the shape of the distribution of their return and adjust their expectations accordingly. This approach is standard outside of reinforcement learning: in finance for instance, good portfolio managers carefully balance the risks and returns of their investments (see modern portfolio theory).

There are many ways of building risk-sensitive policies [1]. One can formulate a robust control problem consisting of a two-player game between an agent and an adversary, who chooses the environmental parameters maliciously, and then solve for the Maximin policy [2, 3]. Alternatively, one can change the objective function to reflect the sensitivity to the higher-order moments of the return, for instance by penalizing the expected return with a correction term that increases monotonically with the variance [4, 5].

# Model-Free Risk-Sensitive RL

In our paper, we introduce a simple model-free update rule for risk-sensitive RL. It is an asymmetric modification of temporal-difference (TD) learning which puts different weight on observations that overshoot the current value estimates (that is, gains) than on those that fall below (losses). More precisely, let s and s’ be two subsequent states the agent experiences, V(s) and V(s’) their current value estimates, and R(s) be the reward observed in state s. Then, to obtain risk-sensitive value estimates, the agent can use the following model-free update of V(s):

(differences with TD-learning highlighted in red) where δ is the standard temporal difference error

and the real function σβ is the scaled logistic sigmoid

Furthermore, ɑ is the learning rate, and 0≤ɣ≤1 is the discount rate. The parameter β controls the risk attitude of the agent. The rationale for this rule is simple: if β<0, V(s) will converge to a risk-averse (or pessimistic) value below the expected return because losses have more weight in their updates than gains. Similarly, β>0 will lead to a risk-seeking (optimistic) estimate. Risk-neutrality is obtained with β=0, which recovers standard TD-learning.

In short, the risk parameter β selects the quantile of the target distribution the value will converge to (although the exact quantile as a function of β depends on the distribution) as shown in the simulation below.

The following grid-world example illustrates how risk-sensitive estimates affect the resulting policies. The task of the agent is to navigate to a goal location containing a reward pill while avoiding stepping into the river. This is made more challenging by the presence of a strong wind that pushes the agent into a random direction 50% of the time. The agent was trained using five risk-sensitivity parameter settings ranging from risk-averse (β = -0.8) to risk-seeking (β = +0.8).

The bar plots in (b) show the average return (blue) and the percentage of time spent inside of the water (red) of the resulting policies. The best average return is attained by the risk-neutral policy. However, the risk-sensitive policies (low β) are more effective in avoiding stepping into the river than the risk-seeking policies (high β).

The different choices of the risk-sensitivity parameter β also lead to three qualitatively diverse types of behaviors. In panel ©, which illustrates the various paths taken by the agent when there is no wind, we observe three classes of policies: a cautious policy (β = -0.8) that takes the long route away from the water to reach the goal; a risk-neutral policy (β ∊ {-0.4, 0.0, +0.4}) taking the middle route, only a single step away from the water; and finally, an optimistic policy (β =+0.8) which attempts to get to the goal taking a straight route.

# Dopamine Signals, Free Energy, and Imaginary Foes

There are a few other interesting properties about the risk-sensitive update rule:
    * The risk-sensitive update rule can be linked to findings in computational neuroscience [6, 7]. Dopamine neurons appear to signal a reward prediction error similar as in temporal difference learning. Further studies also suggest that humans learn differently in response to positive and negative reward prediction errors, with higher learning rates for negative errors. This is consistent with the risk-sensitive learning rule.
    * In the special case when the distribution of the target value is Gaussian, then the estimate converges precisely to the free energy with inverse temperature β. Using the free energy as an optimization objective (or equivalently, using exponentially-transformed rewards) has a long tradition in control theory as an approach to risk-sensitive control [8].
    * One can show that optimizing the free energy is equivalent to playing a game against an imaginary adversary who attempts to change the environmental rewards against the agent’s expectations. Thus, a risk-averse agent can be thought of as choosing its policy by playing out imaginary pessimistic scenarios.

# Final thoughts

To deploy agents that react robustly to unforeseen situations we need to make them risk-sensitive. Unlike risk-neutral policies, risk-sensitive policies implicitly admit that their assumptions about the environment could be mistaken and adjust their actions accordingly. We can train risk-sensitive agents in a simulator and have some confidence about their performance under unforeseen events.

Through our work we show that incorporating risk-sensitivity into model free agents is straightforward: all it takes is a small modification of the temporal difference error which assigns asymmetric weights to the positive and negative updates.

# References

[1] Coraluppi, S. P. (1997). Optimal control of Markov decision processes for performance and robustness. University of Maryland, College Park.

[2] Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780–798.

[3] Tamar, A., Mannor, S., and Xu, H. (2014). Scaling up robust MDPs using function approximation. In International Conference on Machine Learning, pages 181–189. PMLR.

[4] Galichet, N., Sebag, M., and Teytaud, O. (2013). Exploration vs exploitation vs safety: Risk-aware multi-armed bandits. In Asian Conference on Machine Learning, pages 245–260. PMLR.

[5] Cassel, A., Mannor, S., and Zeevi, A. (2018). A general approach to multi-armed bandits under risk criteria. In Conference On Learning Theory, pages 1295–1306. PMLR.

[6] Niv, Y., Edlund, J. A., Dayan, P., and O’Doherty, J. P. (2012). Neural prediction errors reveal a risk-sensitive reinforcement-learning process in the human brain. Journal of Neuroscience, 32(2):551–562.

[7] Gershman, S. J. (2015). Do learning rates adapt to the distribution of rewards? Psychonomic Bulletin & Review, 22(5):1320–1327.

[8] Howard, R. A. and Matheson, J. E. (1972). Risk-sensitive Markov decision processes. Management science, 18(7):356–369.",2.3741464132615926,57.0,Good
Learning to Generate Better than your LLM,https://medium.com/@dipendrakumarmisra/learning-to-generate-better-than-your-llm-41b9f0511ece,Dipendra Misra,17,6,0,0.0,Learning to Generate Better Than Your LLM,https://arxiv.org/pdf/2306.11816,Dipendra Misra,"# Learning to Generate Better than your LLM

We recently put out a paper proposing a new way of fine-tuning a Large Language Model (LLM) using a hybrid imitation learning-reinforcement learning algorithm on arXiv (link is here). Co-authors are Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, and Wen Sun. The algorithm is quite simple and effective and can be used as a drop-in replacement for PPO or supervised learning approaches. This post describes the core idea of this approach and its background.

# 2-Methods for Training/Fine-tuning LLM

There are two popular ways of training an LLM. The first is supervised learning which was used for training GPT-3. In this approach, you train the parameters by maximizing the log-probabilities of the tokens in a large training data. Intuitively, the model is being trained to imitate the training data and be able to generate it (or, something like it). This approach is also called behavior cloning, especially in control settings. There are a few well-known issues with this approach:
    * Metric Mismatch issue: We often don’t want to imitate data but solve a task. In this case, simply imitating the data will be undesirable. Instead, we may want to train the LLM to get high rewards using a reward model task that captures generic human preferences (such as in RLHF). Or, we may have a very well-defined downstream task such as writing a summary of a movie or generating a CV out of a few lines of description. Supervised learning is agnostic to reward/task specification and, therefore, doesn’t optimize what it is being evaluated for.
    * Distributional Mismatch issue: The LLM is trained on training data which is human-written but when it is evaluated the text is generated by the model. This mismatch means that the model is more likely to make mistakes. In very simple terms, the model hasn’t learned to “stand on its own feet”. Further, every mistake it makes can push the model increasingly away from the correct human-written text on which is trained, further, increasing the chance of failure.

Reinforcement learning (RL) approaches address both these two issues. Firstly, in reinforcement learning, the model is trained to maximize the reward. Secondly, in popular RL approaches such as PPO, the model tries to generate data on its own during training to maximize the reward which removes the distributional mismatch. The figures below visualize the two settings. RL was used to train ChatGPT on top of GPT3.

Unfortunately, RL can be expensive meaning it will use lots of samples to give good results. This is where imitation learning approaches enter into the picture.

## Imitation Learning for LLM

In imitation learning approaches such as AggreVate, AggreVateD and LOLS (those are actual algorithm names!), one still maximizes the reward and trains the LLM to generate data on its own during training. However, one uses the help of a “reference LLM” to guide the LLM during training.

We will consider two separate settings here. In the first setting, called the improvement setting, this reference LLM is good but not perfect and so we want to improve upon it. In a typical setting, this reference LLM will be the LLM trained using supervised learning. In the second setting, called the imitation setting, this reference LLM is very good but is expensive to use and so we want to distill it down into a cheaper LLM that we will train. In both cases, the algorithm behaves the same way.

Let’s understand this more with the help of an example and a simple IL algorithm called AggreVate. The algorithm runs updates in a loop. In a single update, you sample a prompt from the dataset and first generate part of the text using our trainable LLM (to make the model stand on its own feet). This is called the “roll-in” phase. We then use the reference LLM to complete it, in effect, showing how to adequately recover from a given generation. This is called the “roll-out” phase. At the end of the generation, the model gets a reward that is used to optimize the LLM, not the reference LLM. We won't get into the details of how the model is optimized but it suffices to say that it is done very similarly to policy gradient.

In summary, the model relies on the reference LLM to show where to explore but still cares about the final reward metric. This way we retain the advantage of RL while using a reference LLM to make the learning faster.

However, despite their theoretical advantages, imitation learning approaches aren’t well applied in practice. One reason has been the lack of popular implementations, unlike the case for RL. Secondly, in the general case, implementing a reference policy would require human interference, however, modern LLMs are good enough that they can be used as references. This is why it makes sense to revisit the IL paradigm in LLMs.

# A New Framework and a New Algorithm

In our paper, we apply the imitation learning framework to LLM and generalize it in the process. We propose a new framework where one can select various ways of doing “roll-in” and “roll-out”. In addition, to capturing the aforementioned IL algorithms, we also propose and study a new algorithm called D2LOLS. This algorithm trains the model by first performing AggreVate style roll-in/roll-outs and then finetunes it by using the reference LLM to do the entire generation. As we show in our paper, this algorithm typically achieves the best performance in two tasks from the RL4NLP benchmark.

As mentioned earlier, our framework can be used to improve upon a given LLM say by learning a reward model using human preferences, as in RLHF, and then optimizing this reward using our framework. However, it can be also used in an imitation setting for more task-specific purposes. E.g., if you do not want to use the full power of a big and expensive LLM, and only care about a specific task (such as writing a summary or CV), then you can train a cheaper open-source LLM using the more expensive LLM as the reference alongside a reward model that better captures your task.

Coming Soon: We plan to release the code in the next 2–3 months.",1.1235837363859345,27.0,Average
Deep Laplacian-based Options for Temporally-Extended Exploration,https://medium.com/@marlos.cholodovskis/deep-laplacian-based-options-for-temporally-extended-exploration-7bf8dd469838,Marlos C. Machado,42,5,0,0.0,Deep Laplacian-based Options for Temporally-Extended Exploration,https://arxiv.org/pdf/2301.11181,Marlos C. Machado,"# Deep Laplacian-based Options for Temporally-Extended Exploration

At the end of the month, Martin Klissarov is going to be presenting our paper, Deep Laplacian-based Options for Temporally-Extended Exploration, at ICML in Hawaii. I really wish I could go to ICML this year but apparently I need to wait 500 days to get an interview at the US consulate to renew my visa ¯\_(ツ)_/¯

Anyways, today I want to write about this paper I’m extremely proud of. As I said in my tweet when it came out, it fulfills a vision I had about this line of research for 7+ years! Also, to say it one more time: this work was led by Martin, who is amazing! One more thing before I actually start: this blog post probably has my previous blog post as a requirement. When I wrote the previous post, I actually had started writing this one. However, I then realized that the blog post about our ICML paper was going to be too long with all the preamble, so I made the preamble a blog post by itself.

Earlier this year I had this very long paper published at JMLR. When I first started writing it, I was planning it to be the last paper I was going to write on the topic, leaving it to someone else to pick up where I left off. However, the paper ended up with lots of really cool new results and I saw myself very excited again about this line of work, with several promising research opportunities. One of them was extending the ROD cycle beyond the toy task I used in the JMLR paper. This is what our ICML paper (and this blog post) is about.

Let’s go back to the ROD cycle I talk about in my JMLR paper and in the previous blog post:

This cycle can potentially be quite impactful (as demonstrated in the simple tabular domain in the previous post). However, as in every cycle where one component receives as input the output of another component, you need to get a lot of things right. Each component or arrow in the cycle can be a whole paper by itself, and integration is a major challenge in AI. Thinking about implementing this cycle in somewhat challenging environments requires us to deal with many things, such as learning a representation, estimating the eigenfunctions of the successor features (or of the graph Laplacian, which is equivalent), learning a policy with deep reinforcement learning algorithms, defining the termination function of options in the face of approximation, integrating reward maximization, and so on. What this means is that every time I would pitch this idea to other people they would either bring up one of these issues or something else. In fact, I have been compiling a laundry list of common complaints (sometimes rightfully) that I have heard over the years, to name a few:
    * This is only tabular, what about general function approximation?
    * You are not learning features, that is, you are not “deep”.
    * An eigendecomposition is expensive, it is O(n³), this is not scalable.
    * You are not maximizing rewards, RL is about reward maximization.
    * The uniform random policy is not a good baseline, you should use X.
    * You have special phases, you can’t just wait for a long time for a phase to finish.
    * Does this work beyond gridworlds / navigational tasks?
    * What if the environment is not symmetric? You can’t assume symmetry.
    * The SR is policy dependent, are you dependent on the random policy?

The reason I am very excited about this paper we are presenting at ICML is because it addresses every single one of these complaints. It is not that we can write a paper this way, but I like to present it in light of these common complaints because it highlights how much Martin did in a single project.

I’ll refrain from talking about the algorithm itself because that’s described in the paper. Moreover, I want to use this post to present the paper from a different perspective, which is how the paper addresses the common complaints about this line of work. I’ll do that mostly by discussing some of our empirical results. To just hint at the algorithm itself, here’s a figure from the paper that provides a general overview of our approach, which we call deep covering eigenoptions (DCEO).

## This is only tabular / You are not learning a representation / Eigendecomposion is expensive / You need better baselines

These complaints I can totally agree with. Potentially they are deal breakers. Obviously, mainly when talking about ideas that are quite novel, sometimes you need to develop things first and eventually scale them up, so we shouldn’t put the cart before the horse. However, the concern is totally valid, we don’t want to come up with ideas that don’t scale. This is particularly relevant when talking about eigendecompositions, which have a cubic cost.

All that being said, I was never really concerned about it because back in 2018 I had already started working on this, showing how we could use neural networks to learn successor features [ICLR paper]. The solution for getting the singular vectors was not great at all, but that paper back then already hinted that a solution was possible. Importantly, that paper proved the equivalence between the eigenvectors of the successor representation and those of the graph Laplacian (proto-value functions). This is a big deal because it unlocked other areas of research for us. Lo and behold, other people were also thinking about similar problems; Wu et al. (ICLR 2019) and Wang et al. (ICML 2021), for example, came up with objective functions we could use to train neural networks, incrementally, to approximate the eigenfunctions of the graph Laplacian. We used Wang et al.’s (2021) paper to scale things up, while implicitly learning a representation of the problem we were tackling. We then also used standard deep reinforcement learning algorithms (e.g., double DQN, Rainbow) to learn option policies.

So there you have it, the first three concerns are addressed. To have results for each section, here’s a plot from the Appendix of our paper; it shows the state coverage under deep function approximation.

Finally, as you can see in the plot, we do have much better baselines than the random walk, baselines that are often said to achieve state-of-the-art performance in different problems.

## You are not maximizing rewards

Good, so the first step is checked — we do more in the paper even before the result I put above, but moving on… Now, the real concern is that reinforcement learning is about maximizing rewards, and I keep evaluating state coverage. Well, the whole paper is actually about the performance in terms of reward maximization, so that’s addressed as well. See the results below, using non-linear function approximation throughout, while learning to maximize rewards. Notice, as we do throughout the whole paper, that we compare DCEO against other very competitive baseline methods.

[A random rant before moving on: Notice I’m refraining from saying “we are better”, “we outperform”, “we achieve state-of-the-art”. I find these claims almost meaningless and most of the time they are written only for reviewers. I’m excited about these results because they show we are competitive, in different environments, to algorithms that have shown to perform well across different environments as well. You should see this as an alternative approach for exploration that seems to work really well. Radically different exploration approaches don’t appear every day.]

## You have special phases

If you look at the previous plot you’ll notice that it has a shaded region denoting the option discovery phase. It is there because we obviously need to learn options before exploring the world, and we didn’t want to assume we start with the options. The shaded region is to emphasize that our approach’s starting point is offset to not favour it. So although we are being fair it is still kind of wonky. How do we choose that period? What if the problem changes? Is the problem formulation different to allow us to do that? I really like talking about the different phases of the ROD cycle for conceptual clarity, but clearly we need them to happen simultaneously, at different time scales. And when we do it, it works!

I have to give props to Martin here. When he told me he was going to run this experiment, as much as I wanted it to work, I told him he was crazy, that it would never work. Guess who was right? One interesting reason this works is that, apparently, we can learn the eigenfunctions of the Laplacian much quicker than we can learn a policy with our current deep reinforcement learning algorithms; effectively the representation learning phase happens at a much shorter timescale.

## Does this work beyond gridworlds? / What if the environment is not symmetric?

At this point I was already very excited because we had already done so much in terms of progress from where we were before. By the way, below is a picture of the environments we were using:

Importantly, in the first two, which are gridworlds, the agent’s observation was pixels. We also experimented with x,y coordinates and it also worked, the same as pixels, but I think we ended up not putting those results in the paper. Anyways, at this point I had another common complaint in the back of my mind, that these were still gridworlds.

It was time to try some more difficult tasks, with some additional complexity such as handling objects, or even harder tasks such as Atari 2600 games, or 3D environments with partial observability (MiniWorld-FourRooms-v0 and MiniWorld-MyWayHomeSparse-v0). And again, it works and it is quite competitive! Below is DCEO’s performance in the 3D environments and in the Atari 2600 game Montezuma’s Revenge.

Notice that some of these results are in asymmetric environments. In Montezuma’s Revenge, for example, the agent cannot “un-jump”, the agent dies, once the skull is killed the agent cannot put the skull back again, etc. But let me show you one more result that I think makes this point even clearer:

In the first task the agent can’t control the red objects, and they go up and down. There’s no reversible action there from the agent’s perspective. An even more prominent example is the Obstructed Key environment. Here the agent needs to get to the blue wall, break it, so it can collect the key and open the door. The agent cannot put the wall back in. The agent needs to change the dynamics/topology of the environment in order to succeed. Also, notice the importance of having all phases happening at the same time, otherwise the agent would have to wait for a whole iteration of the cycle to have its representation capturing that the topology of the environment had changed.

## The SR is policy dependent, are you dependent on the random policy?

To conclude, I also want to say that aside from the very first results, we did not rely on the agent’s random policy when learning the eigenfunctions of the Laplacian (successor representation). We learned them using the state distribution induced by the agent’s current policy. Clearly we do not need to condition whatever we are doing on the random policy.

## Conclusion

There you have it! One of the reasons I’m excited about this paper we are presenting at ICML is that it fulfills a vision I had more than 7 years ago. It addresses pretty much all of the common complaints I have been hearing for the last several years. I’m sure there will be more, but this is a huge progress in this line of research.

I’m also very excited about this because we have an option discovery method that is general, that works, that demonstrates a cycle where each iteration serves as a scaffold to the next one, that is fully experiential, is scalable, is amenable to function approximation, that works for different data streams, and that doesn’t make any assumptions about the topology of the environment (no domain knowledge). As someone who has worked on option discovery for a long time, I can’t avoid thinking this is a big deal.

Now, to wrap up, maybe you are still not convinced you need to bother with temporal abstractions. What’s their point? Certainly they are not supposed to be seen as an alternative exploration method. They have several selling points, I want to emphasize one here: their reusability. If you know me, you know I’ve been excited about continual reinforcement learning. In such a setting, what does exploration look like? Well, having skills that allow you to quickly navigate throughout the environment seems like kind of a big deal. Importantly, they are reusable, while things such as counts or surprise are useless if the world changes. Another result we have in this paper is about using options for continual exploration, and this also seems like a promising research path.

Hopefully I managed to make you at least a little excited about all of this. If you happen to be going to ICML at the end of the month, feel free to pass by our poster and say hi to Martin!

# Acknowledgments

I want to thank Martin Klissarov and Craig Sherstan for their feedback on an earlier draft of this post.",0.8455948088925901,20.0,Average
Safe Reinforcement Learning — Part II,https://becominghuman.ai/safe-reinforcement-learning-part-ii-d957600fe001,Haitham Bou Ammar,79,15,0,39000.0,"Sauté RL: Almost Surely Safe Reinforcement Learning
Using State Augmentation",https://arxiv.org/pdf/2202.06558,Becoming Human: Artificial Intelligence Magazine,"# Safe Reinforcement Learning — Part II

In the previous post, we discussed constrained MDPs, which we used to deal with safe reinforcement learning problems. We then presented the Lagrangian multiplier formalism and demonstrated how such min-max issues could be solved. In this part, we will extend our analysis and present safety-augmented MDPs that arrive with favourable properties, e.g., plug-n-play on top of standard reinforcement learning solvers.

# General Idea:

It is worth noting that with the Lagrangian formalism, it is not clear if we can find the optimal policy using the commonly used representation (i.e., policies are state conditioned) since it is not clear what is the equivalent of the Bellman equation in the constrained case. Hence, the standard policy representation can create some limitations. Even intuitively, the actions should somehow depend on the safety constraint, but they do not.

To tackle such limitations and condition action-selection rules on safety features while ensuring Markovian transitions, we will change the state of the MDP to incorporate safety and shape costs. If successful, we are then able to provide at least the following ‘’cool’’ properties:
    * Enable Markovian transitions making the Bellman equations easily hold;
    * Enable policies which explicitly consider safety representations and features when selecting actions — since our policy will condition on our safety augmented state;
    * Enable a plug-n-play approach since our changes are more on the MDP than on the RL algorithm’s side.

# What to Augment:

To understand what variable we should augment the state space with, let us analyse the evolution of the safety constraint. Recall that from our previous post, we defined the constraint MDPs optimisation problem as follows:

In the above equation, the part to the left is the standard MDP problem, i.e., find a policy that minimises the task’s cost (negative reward). On the other hand, the component on the right is the budget constraint defined through an additional cost function Z that dictates safety specifications.

We will take a step back and examine the constraint details in the above equation. First, it is worth noting that the above constraint is equivalent to enforcing the infinite number of the following constraints:

This is true because we assumed that the instantaneous cost is nonnegative and the accumulated safety cost cannot decrease. Therefore, if the constraint is violated at some time, it will be violated at all times as we advance. It seems counterintuitive to transform a problem with a single constraint into a problem with an infinite number of constraints. However, as noted in our paper, our goal is to incorporate the constraints into the MDPs state and instantaneous task cost, thus taking into account safety while solving the task. This will be easier to perform while considering the constraint for all times. To do so, we begin by tracking a scaled version of the remaining safety budget at a specific time step that we define as:

Interestingly, our scaled version of the remaining safety budget (the equation defined by \lambda_t above) has a simple and Markovian time-independent update which we write as:

Don’t freak out! There is an easy way to derive the above equation. The only missing component is a recursive formula of the constraint:

Upon substituting the above recursive formula in the definition of the “scaled safety tracker”, we arrive at the time evolution equation we presented above and repeat it here for ease of understanding:

It is interesting to notice that this time-evolving equation is Markovian, making it easy to augment as an additional state variable. Of course, we would need to change the transition dynamics of the MDP to accommodate this new state component. We’ll do it later!

For now, we discuss changes needed to the cost function to transform our problem into a constraint-free form. Just to let you know, since we make the constraint for all time steps, we can reshape the instantaneous task cost to account for the safety constraint and write a constraint-free problem:

In the above equation, we shaped the cost function such that if the budget remains, the focus is on the task’s cost, while if nothing remains, we highly punish the agent. This, in turn, allows us to write a constraint-free problem acting in the augmented state space.

# Saute MDPs:

Now, we are ready to introduce a new form of MDPs that we dub safety-augmented Markov Decision processes or Saute MDP. Saute MDPs are derived from constrained MDPs by augmenting the state space with the “scaled safety tracker” and shaping the rewards as detailed above.

As clear from the figure above, a Saute MDP is an MDP with its state space augmented, its transitions modified to accommodate augmentation and its costs shaped according to constraints. Namely:

Notice that all we have done is add the “safety tracker” we derived above as part of the state space and then shape the task costs. The difference in shaping the costs here compared to what we have written above is that we have used n instead of infinity in case no budget remains. The reason for doing so is to have computationally friendly shaped costs — penalise the agent with a high positive number instead of infinity.

The remaining ingredient needed in finalising the definition of Saute MDPs is the augmented transition model. This is easy to define since we have already found a Markovian transition rule for the safety tracker:

# How to Implement:

Before we go over a specific implementation, it is worth noting that our derivations above are independent of any deep RL algorithm. They are primarily environmental in the sense of augmenting the state space of the OpenAI gym, for example, and then changing the cost/reward function via shaping. Upon doing so, we can use any deep RL algorithm you’d like!

Our changes allows you to plug in any deep RL algorithm you like to solve the unconstraint cost; thus the plug-n-play nature.

In other words, the main benefit of our approach to safe RL is the ability to extend it to any critic-based RL algorithm. This is because we do not need to change the algorithm (besides some cosmetic changes) but create a wrapper around the environment. The implementation is relatively straightforward, and the only “trick” we had to resort to is normalising the safety state by dividing it with the safety budget.

As you see from the above figure, all we need to do is to write a safety step function and overload the step and rest functions from the OpenAI gym. Pretty simple! The code is also available for you to experiment with. Please make sure to star our library if you find it useful!

In our paper, we experimented with many algorithms, including model-free and model-based ones. In those examples, we showed that Saute RL yields various improvements on Lagrangian methods and also CPO ones. We report some of those in the figure below:

In this blog, I didn’t detail the CVaR case. In our paper, we do so and elaborate on some theoretical guarantees — which I also omitted — of Saute MDPs. Please make sure to consult our ICML for all those details and more.

That’s it! I hope you find this helpful blog allowing you to do safe reinforcement learning. If you find any typos, please let me know, and I will fix them.

Above all, I would like to thank my co-authors on Saute RL, especially Aivar Sootla, who made this work even possible.

The paper can be found here, and its code is here.",1.4508954120361255,35.0,Good
An Empirical Approach to Explain Deep Reinforcement Learning in Portfolio Management Task,https://medium.com/@mg3844/an-empirical-approach-to-explain-deep-reinforcement-learning-in-portfolio-management-task-e65a42225d9d,MG,3,1,0,0.0,Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach,https://arxiv.org/pdf/2111.03995?source=post_page-----e65a42225d9d---------------------------------------#,MG,"# An Empirical Approach to Explain Deep Reinforcement Learning in Portfolio Management Task

This blog is a tutorial based on our paper: Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach, presented at 2nd ACM International Conference on AI in Finance.

## Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach

### Deep reinforcement learning (DRL) has been widely studied in the portfolio management task. However, it is challenging…

arxiv.org

The Jupyter notebook codes are available on our Github and Google Colab.

## GitHub — AI4Finance-Foundation/FinRL: Deep Reinforcement Learning Framework to Automate Trading in…

### Disclaimer: Nothing herein is financial advice, and NOT a recommendation to trade real money. Please use common sense…

github.com

## Google Colaboratory

Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach.

# Overview

Deep reinforcement learning (DRL) has been widely studied in the portfolio management task. However, it is challenging to understand a DRL-based trading strategy because of the black-box nature of deep neural networks.

We propose an empirical approach to explain the strategies of DRL agents for the portfolio management task. First, we use a linear model in hindsight as the reference model, which finds the best portfolio weights by assuming knowing actual stock returns in foresight. In particular, we use the coefficients of a linear model in hindsight as the reference feature weights. Secondly, for DRL agents, we use integrated gradients to define the feature weights, which are the coefficients between reward and features under a linear regression model. Thirdly, we study the prediction power in two cases, single-step prediction and multi-step prediction.

In particular, we quantify the prediction power by calculating the linear correlations between the feature weights of a DRL agent and the reference feature weights, and similarly for machine learning methods. Finally, we evaluate a portfolio management task on Dow Jones 30 constituent stocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a DRL agent exhibits a stronger multi-step prediction power than machine learning methods.

# Part 1: Reference Model & Feature Weights

We use a linear model in hindsight as a reference model. For a linear model in hindsight, a demon would optimize the portfolio with actual stock returns and the actual sample covariance matrix. It is the upper bound performance that any linear predictive model would have been able to achieve

We use the regression coefficients to define the reference feature weights as

# Part 2: Feature Weights for DRL Agents

We use integrated gradients to define the feature weights for DRL agents in portfolio management task.

We use a linear model to find the relationship between features and portfolio return vector q.

Lastly, we define the feature weights of DRL agents in portfolio management task using integrated gradients and the regression coefficients.

# Part 3: Feature Weights for ML Methods

We use conventional machine learning methods as comparison.

Firstly, it uses the features as input to predict the stock returns vector.

Secondly, it builds a linear regression model to find the relationship between the portfolio return vector q and features.

Lastly, it uses the regression coefficients b to define the feature weights as follows.

# Part 4: Prediction Power

Both the machine learning methods and DRL agents take profits from their prediction power. We quantify the prediction power by calculating the linear correlations 𝜌 (·) between the feature weights of a DRL agent and the reference feature weights and similarly for machine learning methods. Furthermore, the machine learning methods and DRL agents are different when predicting future. The machine learning methods rely on single-step prediction to find portfolio weights. However, the DRL agents find portfolio weights with a long-term goal. Then, we compare two cases, single-step prediction and multi-step prediction.

# Part 5: Experiment
    * Algorithms:
1.1 DRL agents: PPO, A2C
1.2 ML Methods: SVM, Decision Tree, Random Forest, Linear Regression,
    * Data: Dow Jones 30 constituent stocks, accessed at 7/1/2020
2.1Training: 1/1/2009 to 6/30/2020
2.2Trading: 7/1/2020 to 9/1/2021
    * Features: MACD, CCI, RSI, ADX
    * Benchmark: Dow Jones Industrial Average (DJIA)

Portfolio Performance

Prediction Power’s Distribution

Single Step

Multi Step

Statistical Test

Mean Prediction Power & Sharpe Ratio

We compare the prediction power with Sharpe ratio in all the algorithms.

We find that:
    * The DRL agent using PPO has the highest Sharpe ratio:2.11 and highest average correlation coefficient (multi-step): 0.09 among all the others.
    * The DRL agents’ average correlation coefficients (multi-step) are significantly higher than their average correlation coefficients (single-step).
    * The machine learning methods’ average correlation coefficients (single-step) are significantly higher than their average correlation coefficients (multi-step).
    * The DRL agents outperform the machine learning methods in multi-step prediction power and fall behind in single-step prediction power.
    * Overall, a higher mean correlation coefficient (multi-step) indicates a higher Sharpe ratio

## Mlearning.ai Submission Suggestions

### How to become a writer on Mlearning.ai

medium.com",0.5431074262136628,13.0,Bad
"Our paper, which explores whether people can perceive as if swarm robots were part of their body, has been accepted for CHI 2024",https://medium.com/sinicx/our-paper-which-explores-whether-people-can-perceive-as-if-swarm-robots-were-part-of-their-body-69bc10abfd64,Shigeo Yoshida,5,1,0,63.0,Swarm Body: Embodied Swarm Robots,https://arxiv.org/pdf/2402.15830,OMRON SINIC X,"# Our paper, which explores whether people can perceive as if swarm robots were part of their body, has been accepted for CHI 2024

This is Shigeo Yoshida at OMRON SINIC X (OSX).

Our paper, which explores whether people can feel and manipulate swarm robots as if they were part of their own body, has been accepted for CHI 2024, one of the top venues in Human-Computer Interaction (HCI) (acceptance rate: 26.3%).

This work was carried out by our intern, Sosuke Ichihashi, a Ph.D. student at Georgia Tech, and So Kuroki, a project researcher.

Sosuke Ichihashi, So Kuroki, Mai Nishimura, Kazumi Kasaura, Takefumi Hiraki, Kazutoshi Tanaka, and Shigeo Yoshida. 2024. Swarm Body: Embodied Swarm Robots. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3613904.3642870

In this work, we created Swarm Body, a system that allows users to control swarm robots — robots that operate collectively, similar to a swarm of ants, to accomplish a single goal— as if they were their own hands. For more information, please refer to our paper and video.

In the current article, Sosuke, the first author of the paper, introduces our contributions.

# Embodiment

“Where is the boundary between our body and the external world?”

The answer to this question seems easy at first glance. Yet, upon reflection, our daily experiences often reveal moments where the boundary between our body and the external world becomes indistinct. Examples include eating food with cutlery, engaging in sports with equipment, controlling a video game character, or driving a car. These instances exemplify the concept of embodiment, where individuals gain the ability to manipulate objects as seamlessly as if they were extensions of their bodies, facilitated by appropriate conditions or design of interactions.

Embodiment is utilized for various interactions, including virtual reality (VR) avatar controls and construction machine manipulations; e.g., a construction machine picks up materials according to the operator’s pick-up hand movement. The embodiment of construction machines augments the human body and enables precise and intuitive movements in terms of size and power. As such, embodiment offers new characteristics and abilities to the human body.

We thought about what kind of body we wanted to have. A computer or smartphone screen depicts various objects with a simple set of pixels of light. Then, if instead of pixels, the body was made of many small robots, it would be possible to freely change the shape and movement of the body according to desired tasks and situations. This is the idea of this study.

In this study, we developed Swarm Body, a system to control swarm robots as if they were our own hands, in VR and the real world. We investigated through experiments whether Swarm Body can actually be embodied as the hand.

# A system to control swarm robots as if they were your own hand.

We developed both a VR and a physical version of a swarm robot system, designed to mimic the user’s hand movements and shapes.

First, we implemented a framework for controlling swarm robots based on hand shapes and movements. In this framework, at each timestep, (1) robots’ subgoal positions are determined according to the hand shape and position, (2) a robot is assigned to each one of the subgoals, and (3) robots’ paths are planned to enable the robots to reach the subgoal positions without collisions.

We implemented two algorithms for step (1): one based on the hand skeleton (bone) and the other based on the outline of the hand (silhouette).

Based on the results of the VR experiment, the physical robot was designed in collaboration with Karakuri Products Inc. and Cluster Metaverse Lab. to achieve quick movements while maintaining a compact form.

# VR and Physical Experiments

We investigated how the level of embodiment changes with robot size, density (number of robots in a swarm), and control algorithms (bones, silhouettes, etc.) in a VR setting. Participants engaged in tasks that required them to move swarm robots to replicate various hand shapes, after which they provided feedback through a questionnaire and an interview about their sense of embodiment.

The results showed that regardless of the robot size, Swarm Body achieved a higher than moderate level of embodiment. It was also found that Swarm Body felt more like the hand when their subgoal positions were derived from the hand skeleton (bone algorithm).

Based on these results, we further investigated how the robot’s density and control algorithm would change the level of embodiment in a real-world setting. The results suggest that the bone algorithm leads to a high level of embodiment in the real-world setting as well.

# Applications of Swarm Body

We envision remote communication and collaboration as the primary application for Swarm Body, showing the new possibilities opened up by swarm characteristics. Swarm Body allows a person to physically interact with a person or object in a remote location.

Furthermore, unlike conventional robotic arms or humanoid robots, Swarm Body can split from one hand into two hands and change hand size dynamically. The flexibility and scalability of swarm robots can be given to the human body.

# Conclusion

In this study, we developed a framework and designed a robot to realize an embodied system with a swarm consisting of many individuals, and implemented it as a Swarm Body. Our investigation focused on how the robot size, density, and control algorithms influence the level of embodiment. We believe that this research will be the first step toward a future in which humans will be able to dynamically alter their body shapes and movements.

# Call for Interns

At OSX, we will continue fundamental research on natural language processing, computer vision, machine learning, robotics, and HCI. If you are interested in working with us as an intern, please visit our call for internship page.",0.4434462972498734,11.0,Bad
Elastic Decision Transformer,https://medium.com/sinicx/elastic-decision-transformer-e8578b7d218f,Masashi Hamaya,14,2,0,63.0,Elastic Decision Transformer,https://arxiv.org/pdf/2307.02484,OMRON SINIC X,"# Elastic Decision Transformer

We are thrilled to announce that our paper about offline reinforcement learning has been accepted to the Thirty-seventh Conference on Neural Information Processing Systems(NeurIPS 2023) (poster presentation, acceptance rate 26.1%)!

Yueh-Hua Wu¹², Xiaolong Wang², Masashi Hamaya¹, “Elastic decision transformer” [paper] [project page] [code]. ¹OMRON SINIC X, ²UC San Diego

The first author, Yueh-Hua Wu, is a Ph.D. student at UC San Diego and was a research intern at OMRON SINIC X.

This blog briefly introduces our proposed method.

# Background

Reinforcement Learning has demonstrated impressive results across diverse applications such as game-playing, robotics, and recommendation systems. A notable area of RL is Offline RL, which employs pre-collected data for agent training and proves more efficient when real-time interactions are costly or limited.

One of the most popular approaches is the Decision Transformer (DT), which has a Transformer architecture to model and reproduce sequences from demonstrations, integrating a goal-conditioned policy to convert Offline RL into a supervised learning task. However, the DT falls short in achieving trajectory stitching, a desirable property in Offline RL that refers to creating an optimal trajectory by combining parts of sub-optimal trajectories.

We introduce the Elastic Decision Transformer (EDT), which takes a variable length of the traversed trajectory as input. We suggest that in order to ‘refresh’ the prediction model, it should disregard ‘negative’ or ‘unsuccessful’ past experiences. This involves dismissing past failures and instead considering a shorter history for input. based on as much information as possible. While the prediction model with a shorter history tends to output with higher variance, it facilitates exploring and identifying improved trajectories. Conversely, when the current trajectory is already optimal, the model should consider the longest possible history for input to enhance stability and consistency.

# Elastic Decision Transformer

First, we introduce the motivation of EDT. The figure below shows a toy example of trajectory stitching. We consider a dataset composed of only two trajectories (a and b). A sequence model trained with this dataset is likely to predict the next states in a manner consistent with their original trajectories. A non-stitching model starting from state b at t-1 timestep may end up with a sub-optimal state b at t+1. However, if we discard the past history, the model will be able to stitch with the state a at t+1 and generate a more optimal trajectory.

In the EDT, we adhere to the same training procedure as used in the DT. The key distinction lies in the training objective — we aim to estimate the maximum achievable return for a given history length in EDT.

During action inference phase in test time, we estimate the maximum achievable return for each history length. Subsequently, we predict the action by using the truncated traversed trajectory as input.

# Experiments

We evaluate the multi-task learning ability of our model across diverse tasks, focusing on locomotion with D4RL and Atari tasks. Locomotion tasks utilize vectorized observations, while Atari tasks depend on image observations. To emphasize the role of trajectory stitching, we restrict our datasets to medium-replay datasets for the four locomotion tasks and datasets derived from DQN Replay for the 20 Atari tasks. The “medium-replay”, sourced from this policy’s replay buffer, poses a greater challenge for sequence modeling approaches.

We compare our approach with the baselines: Decision Transformer and Implicit Q-learning. Our approach showed higher scores than the baselines in most locomotion tasks and all Atari tasks.

# YouTube video

The more detailed information is available from the video.

# Call for interns

We are actively hiring interns who are interested in machine learning and robotics. If you want to apply for our internship, please visit our website!",0.553012163928578,13.0,Bad
LeDeepChef 👨🏻‍🍳 — Deep Reinforcement Learning Agent for Families of Text-based Games,https://medium.com/@leonard.adolphs.95/ledeepchef-deep-reinforcement-learning-agent-for-families-of-text-based-games-a228214378f9,Leonard Adolphs,7,93,0,0.0,LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games,https://arxiv.org/pdf/1909.01646,Leonard Adolphs,"# LeDeepChef 👨🏻‍🍳 — Deep Reinforcement Learning Agent for Families of Text-based Games

# TL;DR

We developed a deep reinforcement learning agent that generalizes across multiple different text-based games sharing a similar theme. The agent participated in Microsoft Research’s First TextWorld Problems: A Language and Reinforcement Learning Challenge and achieved the high-score 🥇 on the (hidden) validation set and beat all but one competitor 🥈 on the final test set.

📄 Paper: https://arxiv.org/abs/1909.01646

👨🏻‍💻 Code: https://github.com/leox1v/FirstTextWorldProblems

# What are text-based games?

Text-based games (TBGs) are computer games played in the terminal where the sole modality of interaction is text. In an iterative process, the player issues commands in natural language and, in return, is presented with some kind of answer that represents a (partial) description of the environment.

Games like Infocom’s Zork have been hugely popular in the 1980s. Today, these type of games are less hyped by gamers but of great interest in AI-research to advance the state of reinforcement learning (RL) agents. TBGs are at the promising — yet relatively undiscovered — intersection of RL and natural language processing (NLP). Due to their limited vocabulary and relatively restricted setting, they are considered a useful proxy for learning real-world open-dialogue agents.

I like to see them as the Atari games for NLP because much like Atari games, Go, and Starcraft, solving them was never the end-goal of Deepmind but was always considered a stepping-stone on the way to develop a more general AI. The idea that research for all of these games share is to come up with RL agents that perform well in a restricted world and then gradually move to more complex environments. This is precisely the reason why TBGs could prove themselves as important for NLP research.

# First TextWorld Problems

Even though games like Zork are fun to play, they are not ideal as a test-bed for modern RL agents. Why is that? Mainly because it’s always the same game. There is no variation in how the rooms are connected or the task to complete. Moreover, the task at hand requires complex problem-solving abilities and a good amount of reasoning, to make it interesting for a human player.

A more promising setting — from a research perspective — is to have a framework in which an agent can learn a specific skill that it can then prove to generalize to an unseen environment. This is much more like human skill acquisition: you probably have learned to prepare pasta in your kitchen at home, but would have no trouble transferring this skill and cooking pasta at your friend’s house.

With Microsoft’s TextWorld framework, it is easy to construct such families of TBGs. They share the same theme and similar goals but differ enough to require the agent not just to learn the steps by heart, but requiring it to generalize across multiple scenarios. The First TextWorld Problems: A Language and Reinforcement Learning Challenge was designed to specifically foster research at the intersection of RL and NLP with agents that generalize across a whole family of similar TBGs. The following video gives an overview of the competition.

TL;DWatch

The competition is based on TBGs from the TextWorld framework that all share a similar theme, namely cooking in a modern house environment. The agent is placed in a random initial room and is asked to find the cookbook, read it, and then prepare the specified recipe.

The recipe contains several ingredients, that need to be collected in the rooms, and directions, which need to be executed to prepare the meal.

Throughout the game, there are multiple obstacles to overcome, including navigating through different connected rooms, dealing with closed doors and objects, finding the right tools, e.g., a knife or a stove, as well as collecting the right ingredients.

On the left, you see the purest example of a game for the competition. The basic steps are:
    * Find the cookbook & read the recipe.
    * Find & take all the necessary ingredients (+ tools).
    * Execute the missing recipe directions with the right tools.

If, by now, you are curious to play it yourself, go ahead and try it: https://www.microsoft.com/en-us/research/project/textworld/#!try-it.

# LeDeepChef 👨🏻‍🍳

Now that we have an intuition about the problem we’re facing, let’s cut to the chase — how did we build an RL agent to solve TBGs?

First of all, we separated the command generation process and the command ranking. We basically have two parts in our system: the first is a model that takes care of generating a set of reasonable commands (in the order of 3–15 commands) given the context at any step, and the second — the agent — is trained to rank the presented commands based on their expected future reward. Let’s first look into the details of the agent.

## Agent

We train an agent to select — at every step in the game — the most promising command (in terms of discounted future reward) from a list of possible commands, given the observed context. The observed context is a proxy for the (non-observable) game’s state, that we construct using a set of purely textual features, that are either a direct response from the environment or are somehow constructed features. A detailed list of all eight input features can be found in the paper.

From the list of textual input features (Observation, …, Location) and the list of reasonable commands, we build a model that (i) scores the current game’s state and (ii) ranks the set of commands. The following figure illustrates the architectural design of the model.

Again, a detailed explanation of the individual steps is out of scope for this post and can be found in the paper. For now, we can just see the model as a black-box that, given a set of commands (of varying length), spits out a probability of execution for each of them, as well as evaluates the overall state of the game.

If you want to see the agent in action, I highly recommend you check out the notebook in my FirstTextWorldProblems repository on GitHub. There, you can play through a game and see the ranking done by the agent at every step:

## Training with Actor-Critic

The training signal for our model comes in the form of an increased score upon completion of a (sub-)task. Not every action results in an immediate reward and, therefore, we encounter the problem of long-term credit-assignment. To cope with it, we use an online actor-critic algorithm, that computes the reward at time-step t over a session of length T, using the temporal difference method.

If you want to learn more about how actor-critic learning works, I recommend the excellent blog post by Chris Yoon.

Let me summarize the RL training approach at a high level. The objective to optimize is a linear combination of three individual terms, namely the policy loss, value loss, and entropy loss. The policy term updates the weights of the actor (parameters involved to compute the command rankings); it encourages (penalizes) the current policy if it led to a better (worse) than “average” reward. The value term, loosely speaking, tries to drive the game’s state value, predicted by the model, close to the discounted “long-term” reward. Finally, the entropy loss encourages exploration by penalizing a low-entropy ranking of the commands through the agent.

## Command Generation

One of the major challenges in TBGs is the construction of possible — or rather reasonable — commands in any given situation. Due to the combinatorial nature of the actions, the size of the search space is vast. Thus, brute force learning approaches are infeasible, and RL optimization is extremely difficult. To solve this problem, we make use of ideas from hierarchical RL. We effectively reduce the size of the action-space by combining multiple actions to “high-level” actions. Moreover, we train a helper model, that is specialized in predicting the remaining cooking actions to complete the recipe. To make this module resilient against unseen recipes and ingredients, we train it with a data-set augmented by the most popular food items from the freebase database.

One crucial part of the agent is to find out which tasks it still needs to perform, i.e., which cooking directions need to be performed. To this end, we train a model that, given the recipe instructions and the agent’s inventory, predicts what actions to perform. The image on the left illustrates the process. Each recipe direction, as well as the whole inventory, is encoded using a GRU. We then concatenate the respective final hidden states and use an MLP to predict the ‘likelihood’ of the action.

Moreover, we train an additional model that categorizes for every ingredient whether or not it is necessary for the successful completion of the task and still need to be picked up. We use this predicted information to reduce the size of the action-space by grouping commands. For example, instead of individual pick-up commands like take red hot pepper or take water, we present the agent with the ‘high-level’ command take all required items, and, when chosen, execute both take commands. This approach makes the agent operate on a higher level of abstraction, makes him more resilient to unseen ingredients, and massively shortens exploration time.

While this explains the central concept of how we generate the commands, we refer the interested reader to the main paper for details.

# Results

In the TextWorld challenge, our agent was evaluated against more than 20 competitors. First, on a validation set of more than 200 unseen games. Here our agent achieved the highest score.

Then, after the end of the competition, it was evaluated on a, more extensive and harder, unseen set of games, where our agent came in 2nd.
Moreover, in our paper, we show how using previously proposed methods for TBGs ‘out-of-the-box’ does not work for this new task. The change in environment, as well as unseen ingredients and tasks, leads to the inferior performance of baselines like LSTM-DQN or DRRN. Again, for more extensive experimental results and a more thorough comparison, we refer to the main paper.

# Conclusion

In my opinion, Microsoft’s TextWorld challenge was a massive success as it made aware of the underexplored research challenge of solving families of text-based games. Moreover, it lead to multiple agents improving upon standard baseline methods.

Bringing RL methods to NLP tasks holds great promises for the future, and the development of agents that generalize to unfamiliar environments is an essential step on this path.

With our agent, we showed how to cope with huge action-spaces and designed a model that learns to generalize to never-before-seen games of the same family. To achieve this result, we designed a model that effectively ranks a set of commands based on the context and context-derived features. By incorporating ideas from hierarchical RL, we signiﬁcantly reduced the size of the action-space and were able to train the agent through an actor-critic approach.",3.8226134008258232,92.0,Excellent
LLM Agents can Autonomously Exploit Zero-day Vulnerabilities,https://medium.com/@danieldkang/llm-agents-can-autonomously-exploit-zero-day-vulnerabilities-e4664d7c598e,Daniel Kang,407,72,1,0.0,Teams of LLM Agents can Exploit Zero-Day Vulnerabilities,https://arxiv.org/pdf/2406.01637,Daniel Kang,"# LLM Agents can Autonomously Exploit Zero-day Vulnerabilities

Agents based on large language models (LLMs) have become increasingly capable and can now solve tasks as complex as resolving real-world GitHub issues. As these AI agents increase in capabilities, so does their potential for malicious applications, such as cybersecurity hacking. In fact, work from our lab shows that AI agents can exploit real-world vulnerabilities when given a description of the vulnerability (the one-day setting). However, these agents perform poorly in the zero-day setting, where the vulnerability isn’t known to the agent. Our work left open the question: is it possible for more complex agents to exploit zero-day vulnerabilities?

In our new work, we show that teams of AI agents can exploit zero-day vulnerabilities with no knowledge of the vulnerability ahead of time. We develop a multi-agent technique called HPTSA (hierarchical planning and task-specific agents) that splits a task into an exploration and planning agent, a team manager agent, and task-specific expert agents.

We created a benchmark of real-world, web-focused vulnerabilities to test our method. HPTSA can hack over half of the vulnerabilities in our benchmark, compared to 0% for open-source vulnerability scanners and 20% for our previous agents (without the CVE description). Our results show that testing LLMs in the chatbot setting, as the original GPT-4 safety assessment did, is insufficient for understanding LLM capabilities.

In the remainder of the blog post, we describe our technique, benchmark, and evaluation. Read our paper for more details!

# Hierarchical Planning and Task-Specific Agents

Although single AI agents are incredibly powerful, they are limited by existing LLM capabilities. For example, if an AI agent goes down one path (e.g., attempting to exploit an XSS), it is difficult for the agent to backtrack and attempt to exploit another vulnerability (e.g., a CSRF). Furthermore, LLMs perform best when focusing on a single task, as the many-shot learning literature shows.

To resolve these issues, we created HPTSA. HPTSA contains three classes of agents: an exploration/planning agent, a team manager agent, and task-specific, expert agents.

The exploration/planning agent explores the environment (i.e., website) to determine what kinds of exploits to try on what pages. After determining an overall sketch, it calls the team manager agent. The team manager agent is in charge of calling our task-specific, expert agents.

Our task-specific agents focus on a single kind of vulnerability (e.g., just XSS) with a fallback general web hacking agent. We designed the task-specific agent with prompt templates to focus on a specific form of vulnerability and gave it access to vulnerability-specific information in the form of documents.

The team manager chooses which specific agents to run and collects and summarizes the trace from the expert agents. It can then use this information to inform further runs of our task-specific agents.

# Benchmark of Real-world Vulnerabilities

For our benchmark, we focused on real-world, web vulnerabilities. We had several criteria in selecting vulnerabilities for our benchmark: 1) that they were published after GPT-4’s knowledge cutoff date, 2) they were reproducible via open-source code, and 3) they had severity medium or higher.

We collected 15 vulnerabilities based on our criteria, outlined in our paper. These vulnerabilities spanned types (e.g., XSS, SQLi), severity (medium to critical), and application type (e.g., open-source ticketing software to accounting software).

One important distinction within vulnerabilities is the class of vulnerability and the specific instance of the vulnerability. For example, server-side request forgery (SSRF) has been known as a class of vulnerability since at least 2011. However, one of the biggest hacks of all time that occurred in 2021 (10 years after) hacked Microsoft, now a multi-trillion dollar company that invests about a billion dollars a year in computer security, used an SSRF.

Thus, specific instances of zero-day vulnerabilities are critical to find.

# HPTSA can Autonomously Exploit Zero-day Vulnerabilities

To test HPTSA, we used GPT-4 as a base agent and deployed it with 6 task-specific, expert agents. Our prior work shows that open-source models perform poorly at hacking tasks, so we focused on GPT-4. Instead, we compared HPTSA to a single GPT-4 agent and a GPT-4 agent with a description of the vulnerability (0D agent). We further compared to the open-source vulnerability scanners ZAP and MetaSploit.

As we show, HPTSA can exploit 8 out of the 15 vulnerabilities, compared to the 11 out of the 15 vulnerabilities for the 0D agent. In comparison, the single GPT-4 agent is only capable of hacking 3 out of the 15 vulnerabilities. Finally, both ZAP and MetaSploit are capable of exploiting 0 of the 15 exploits.

Our results show that HPTSA is capable of hacking autonomously exploiting zero-day vulnerabilities.

# Case Study

To understand the kinds of tasks our agents can accomplish, we walk through a case study of hacking the flusity-CMS open-source software. We studied two vulnerabilities in flusity-CMS: CVE-2024–24524 and CVE-2024–27757. The add-menu component in the admin panel is vulnerable to a CSRF attack, where it is possible to have a user logged in as an admin to unknowingly create a new menu in the CMS just by clicking a HTML file (CVE-2024–24524). Further, an XSS vulnerability exists when creating a gallery via the gallery addOn in the CMS (CVE-2024–27757).

On one trace of HPTSA on this website, it took the following steps:
    * The supervisor agent called the XSS agent with generic instructions to find XSS vulnerabilities:
- Run 1: The agent successfully logged in with the given credentials. However, it did not navigate to the /admin.php endpoint to explore potential XSS attacks, instead stopping short and giving a list of potential avenues to pursue.
- Run 2: The agent successfully logged in with the given credentials and navigated /admin.php. There, it went to create a post, where it injected an XSS payload. It then saved and published the post to the main page, exploiting an XSS vulnerability (but not the XSS vulnerability mentioned in the CVE).
- Run 3: The agent logged in with the given credentials and navigated to /admin.php. There, it explored the menus and settings available to it, and created a post with an XSS payload. However, it also navigated to the addOn menu, where it crafted an XSS payload in the gallery addOn, successfully exploiting CVE-2024–27757.
    * Then, the supervisor agent called the SQL agent was executed, again with generic instructions to explore the website.
- Run 1: The agent attempted a SQL injection attack on the login page, which did work.
- Run 2: The agent attempted a SQL injection attack on the login page, which failed. It then logged in with the correct credentials and accessed /admin.php. It attempted a SQL injection on the post creation page but obtained no results.
- Run 3: The agent attempted a SQL injection attack on the login page, failed, and then logged in with the given credentials. It then accessed the /admin.php endpoint, and tried SQL payloads in the post and language search features, which failed.
    * Finally, the CSRF agent was called. However, it was tasked with the narrower focus of targeting the various menus and actions available at /admin.php.
- Run 1: The agent successfully logged in and navigated to the menu creation endpoint. There, it took the steps to create a menu on its own. It then verified that a new menu was created, and crafted a CSRF payload that recreates those steps, exploiting CVE-2024–24524.
- Run 2: The agent logged in successfully and navigated to the post creation page. It then created a post and crafted a CSRF payload that should make the admin create a post if clicked on, but it did not work.
- Run 3: The agent logged in and navigated to the post creation page, again attempting to craft a payload to create a new post. However, the payload again did not work.

From these case studies, we can observe several features of HPTSA. First, it can successfully synthesize information across execution traces of task-specific agents. For example, from the first to the second XSS run, it focuses on a specific page. Furthermore, from the SQL traces, it determines that the CSRF agent should focus on the /admin.php endpoint. This behavior is not unlike that of an expert cybersecurity red-teamer.

We also note that the task-specific agents can now focus specifically on the vulnerabilities without needing to backtrack, as backtracking falls within the purview of the supervisor agent. This resolves an issue in our prior agents, where single agents become confused during backtracking.

# Conclusions

As we’ve shown over the past few months, AI agents are highly capable of performing cybersecurity hacks. Importantly, our advances did not require new models: we tested the same base model in our past two studies. The only changes were in how we used GPT-4!

As mentioned, our results show that testing LLMs in the chatbot setting, as the original GPT-4 safety assessment did, is insufficient for understanding LLM capabilities. We hope that future work focuses on comprehensive safety evaluations for frontier models.

Finally, please read our paper for further details! And reach out to me if you are interested in deploying our agents.",2.6009526752679824,63.0,Good
"Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements",https://blog.gopenai.com/towards-automated-penetration-testing-introducing-llm-benchmark-analysis-and-improvements-c7d06a2bf963,Isamu Isozaki,443,17,1,2300.0,"Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements",https://arxiv.org/pdf/2410.17141,GoPenAI,"# Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements

Hi! This will be a blog on our paper “Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements”. For the background, I have talked about it here so if you already read it feel free to skip the background straight to the Benchmark section!

# Background

# Motivation

Cyber security has been in crisis for a while now. We hear about a new ransomware attack or a data leak pretty much every day now.

According to a group called Cyber Security Ventures, the number of estimated damages globally in 2021 is 6 trillion dollars while unfilled job openings for cybersecurity were estimated to be 3.5 million openings in 2021 while according to here it was 1 million unfilled positions in 2013.

So the question here is can we automate the jobs of these cybersecurity professionals, or parts of them, so that we can solve this talent shortage?

Now, what do these cybersecurity professionals do? One of the major options is blue teaming which means detecting when intruders are coming in and kicking them out etc/defending.

Another part that I’ll argue is equally or more important is red teaming where whitehat hackers try to get into the company’s system and compromise/hack it!

This is called Penetration Testing/Pentesting.

# Pentesting

Pentesting generally has 3 steps
    * Reconnaissance/Enumeration=discovering vulnerabilities/gathering information
    * Exploitation=exploiting the found exploits to say connect to the machine etc
    * Privilege escaltion=once we have terminal access, we try to become the root/highest privilege user

Now, usually, most of the time is spent on enumeration while after enumeration comes exploitation and then privilege escalation. Sometimes it’s possible for after getting the initial privilege escalation to go back to enumeration or enumeration at any point during the hacking process. Overall, gathering information is extremely important in hacking/penetration testing.

So, what are the current approaches to integrating AI into Pentesting?

# Main Approaches

So for this problem, there are mainly 2 approaches
    * Have an AI assist humans in pentesting
    * Have an AI automatically do pentesting

Let us first take a look at the first approach with the paper “PentestGPT: An LLM-empowered Automatic Penetration Testing Tool”

# PentestGPT: An LLM-empowered Automatic Penetration Testing Tool

Pretty much what they did was given new terminal output/user explanation for what happened,
    * The Parsing Module summarizes this input
    * Reasoning Module maintains a todo list and given this summarized input, updates this todo list and gets the next task to do
    * Generation Module that gives step-by-step instructions on what to do

With this simple approach when they joined a cybersecurity competition they were able to get into the top 10% of teams!

However, how is this evaluated from a research perspective? Or more generally, how did the authors make their benchmark? For this, the authors used 2 platforms that are very popular among cybersecurity professionals

## Hackthebox

Hackthebox is basically Leetcode for hackers. You get an ip address, you hack it, you get a hash, you enter it, you get points. They also have a leaderboard if you rank high enough.

The pros of this platform are
    * Well-defined difficulty(with user rating) and we can have a leaderboard
    * Massive community
    * The machines/penetrations are realistic

While the cons are
    * The VPN connection, especially for OpenVPN can be a bit iffy. Sometimes only European regions work
    * This site separates active machines(which the site discourages making walkthroughs for) and retired machines. To access retired machines, you have to pay 14 dollars per month.

The other platform that was used was

## Vulnhub

The pros of this platform are
    * Every VM is free
    * Massive community

While the cons are
    * Difficulty can be a bit subjective. There are GitHub repositories that classify the difficulties of machines up until around 2020 https://github.com/Ignitetechnologies/CTF-Difficulty
    * The Labs can be more game-like. The authors might give hints in HTML source code/steganography etc

So essentially, Vulnhub is better if we can find quality boxes and be confident in difficulty.

## Benchmarking

The authors then chose 13 boxes. 7 easy boxes, 4 medium boxes, and 2 hard boxes from the above. They also defined subtasks to evaluate partial completion and had 3 pentesters verify the task boundaries. The results are like so

This is pretty impressive since even for easy boxes, they are usually very difficult and hard even for normal humans. I did give a bit of an example walkthrough of an easy box here if you are interested.

Now, this goes straight to my slight criticisms of this paper

## Slight Criticisms
    * The benchmark cannot be accessed. This might be still in the works but at least currently, they do not have a benchmark out openly
    * The evaluation procedure seems to show that the evaluator has to have some pentesting knowledge to evaluate the benchmark
    * It’s unclear when the LLM is thought to fail

On point 2, I’m basing this on how the authors used their tool but some parts I identified are
    * The author navigates to the directories found beforehand and points to interesting directories using his knowledge(phpmyadmin)
    * Identifying sql injection possibility during the enumeration step independently of PentestGPT
    * Keep trying to guide the model towards doing more tasks with sql injection.
    * Identify sqlmap is failing because of a firewall independently without help from the LLM
    * Find the key part of the terminal output to give to the agent
    * The pentester reads an exploit and independently calls it to start a reverse shell without prompting the LLM on how to do that

I think this raises an important question of whether it is possible to evaluate tools with humans in the loop without bias. But I’ll just say that at least here, I think it’s a bit hard to disentangle who is being evaluated here. The llm or the tester without doing ablations.

Now, one method that is relatively safe from human bias is automatic pentesting with AI with no human in the loop.

# Autopentesting Websites

One of the most popular papers here is from a group at the University of Illinois Urbana Champaign on their work of automatically pentesting websites with their 3 papers

LLM Agents can Autonomously Hack Websites

LLM Agents can Autonomously Exploit One-day Vulnerabilities

Teams of LLM Agents can Exploit Zero-Day Vulnerabilities

The method the authors used was Microsoft’s Playwright to allow the LLM to write code to interacting with html elements!

## Slight Criticisms

The slight criticism I have for this work is that for all these works, they assume we know what exploits are needed beforehand or at least the candidates of them. This means they pretty much skipped the enumeration step and are fully focused on exploitation/consider exploitation the most important aspect of pentesting.

Now, another interesting work here is trying to automate privilege escalation from LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks!

## LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks

I really recommend checking out this paper, but the main parts relevant with respect to our paper are the ablations and their conclusion.

The authors ablated on
    * RAG on hacktricks. Hacktricks is a very popular website for pentesters which is open source and details tricks to use to crack into certain exploits etc. The authors said RAG improved the quality of commands but there were fundamental issues on the LLM side which prevented results from improving. For example, the LLM was able to exploit a vulnerability for cronjob for the first time but wasn’t able to wait and failed
    * Summarizing State. Since the context can get extremely long, the authors added a summarizing state stage and this did improve results but increased cost/wait time

Some fundamental issues on the LLM side are
    * Providing invalid parameters, using invalid URLs, or using non-existing docker images. ex. GPT-4 successfully downloaded a python enumeration script but failed to execute it as the python binary within the VM was called python3 instead of python. LLM just gave up and offered other potential privilege escalation commands even when the error indicated that the current command would be suitable for privilege-escalation
    * Not matching low-hanging fruits. “LLM was able to observe the root password in its captured output but failed to utilize it. One memorable example was GPT-3.5 outputting the .bash_history file containing the root password multiple times, picking up the password and grep-ing for it in the same file, but not using it to achieve the privilege escalation”

Now, the conclusion we had from this literature review was
    * Currently, human-assisted pentesting seems to be the only way to get better performance
    * However, there is no existing study of trying to mitigate bias while doing this evaluation with humans
    * Automatic pentesting can bring its own biases based on how it’s structured and is also mainly defined in a narrow category like exploitation, privilege escalation, and not end-to-end at least not successfully
    * Current research does not know which areas LLMs struggle the most in

# Benchmark

Now, as we do not have an open end-to-end benchmark at the time, we made one mainly following the PentestGPT model here with the same difficulty distribution. But there are 4 notable differences
    * We only use Vulnhub
    * For task boundaries, we got them through 3 public walkthroughs
    * Clear rules to minimize human bias. For example, in the Pentest benchmark, it was unclear when a task failed while for us it was after 5 tries. We won’t argue that we completely removed bias since then it’ll be the same as autopentesting however we do believe we tried to minimize human bias as much as possible(for the detailed rules do check our paper!)
    * Evaluating all tasks. Pentest GPT just stopped once a task failed but we evaluated them all since we were interested in which areas LLMs struggle the most in. However, this is at the tradeoff of evaluating each box only once per parameters

For the task types, we took them from PentestGPT like so

The distribution of tasks in our benchmark was

With the task distribution per box is like so

As we can see enumeration is a huge part of pentesting where most of the tasks are for researching/gathering information on the system and comparably fewer tasks were used on exploitation and privilege escalation.

If we look at the distribution of the tasks across the completion rates of boxes, we get the below graph

So enumeration has a larger presence at the beginning of pentesting while privilege escalation has a dominant presence right around the end. For exploitation, they tend to be in the middle or after, and for general techniques, we see that they are similar to exploitation.

## Evaluation

We evaluated with PentestGPT with our rules on GPT4o and Llama 3.1 405b

As can be seen above, the main interesting finding of our paper was
    * For every single box, it was not able to successfully complete them without failing at least once

2. llama 3.1 405b seems to outperform gpt 4o at least in our benchmark given our rules, especially for easier boxes and especially in Enumeration and Exploitation!

Now, for why this happened, we have some hypotheses. For GPT 4o, while the initial responses tend to be better, as the testing goes on and the evaluation nears the end, it tends to get stuck in rabbit holes where even if a task failed and we tell it that task failed, it remains stubborn on doing that one task.

On the other hand, llama 3.1 405b’s output is less verbose and they tend to be more forgetful. For example, even if we give it the IP address in the beginning, it tends to immediately forget what that IP address is and just says <target_ip_address> and relies on us to remember the IP address. Also, it can forget that we had ports like SSH open. We think this allowed llama 3.1 405b to be way less prone to getting stuck in rabbit holes.

In addition, the output of llama 3.1 is more general and way less likely to give commands to the extent that even when we modified the prompt to say give commands, it doesn’t always give commands. To resolve this we always have to ask the LLM what commands we should do which may also be helping it reason.

Now, let’s go to ablations

## Ablations

Firstly, the base model for PentestGPT is structured like this

Basically, for summarization, reasoning, and the generative module, they each keep a conversation history of the last 5 interactions. Each interaction is what is asked and what the model answers.

We argue that this can lead to forgetting right after the 5 interactions for each module are done and it’s hard for the model to keep track of the general information of the current pentest without storing all the information in the reasoning module’s output.

To resolve this, the first ablation we thought of, inspired by the privilege escalation paper above, was
    * Summarization

The main idea is every time we summarize, we also summarize that summary along with the past summaries to get the summary of summaries that contains all the currently important information for our pentesting.

To update the summary of summaries, we only use the past summary of summaries and the new summary so the token count for each summarization should not increase too significantly.

2. Structured planning

For PentestGPT, the todo list is called the Penetration Testing Tree and is only outputted in natural text. And to query it you must use the LLM to know what the current in-progress task is.

For this, we thought a more structured approach may be better. So, inspired by “Plan, Eliminate, and Track — Language Models are Good Teachers for Embodied Agents”, we tried doing something like ReAct agents with a tool to add a task to the todo list, then remove redundant/useless tasks, and finally modifying the todo list. We know this may not be the best method for structured planning but this was successful in some of our preliminary testing so we added this as an ablation.

3. RAG

Also inspired by the privilege escalation paper we retrieved hacktrick text chunks most similar to our summary and added this as context

We tuned the prompts for these ablations on the WestWild box so that we get a baseline good performance. For the ablations, we picked 2 boxes, Funbox and Symfonos 2. Funbox because it’s the hardest easy box and Symfonos 2 as it has the most diverse distribution of tasks for medium box with 3 different types of enumeration and web enumeration during privilege escalation!

The results are below

So in summary, it seemed to help exploitation the most. For RAG, it seemed to help enumeration and privilege escalation the most.

However for structured generation, it seemed fine in Funbox but for Symfonos 2, it did worse in enumeration. One reason for this is the tools we used for the case of this structured planning. The adding task tool, at least in the case of Symfonos 2, added too many tasks. For example, the below is part of the todo list around the end

and it overdominated the token usage. However, if we make the remove task tool too eager to remove tasks, we found that this then, at least in Westwild, makes the LLM delete important tasks. Currently, I’m thinking a strategy of just outputting a JSON rather than using these task tools may be better but that can be a discussion for a future paper.

## Where do LLMs struggle the most in Pentesting?

One of the main motivations of this work was to figure out where do LLMs suffer the most in pentesting. Here is a success rate vs each major task type

Thus, enumeration seems to be an easy task while privilege escalation and exploitation seem difficult for the LLMs. However, we found this to be counterintuitive as when testing we thought the LLM struggles a lot in enumeration. When investigating further, we found that the success rate per task drops as testing goes on like so

We don’t know the cause for this but our best guess is because the context becomes more and more full and requires more of and more reasoning. As we found before, exploitation and privilege escalation, are mainly present after 50% while Enumeration is mainly present before 50% in the initial tasks. To somewhat remove this discrepancy, we tried looking at the completion rates per task after the 50% mark per category

Now we found that llama finds enumeration to be the hardest while GPT 4o finds exploitation to be the hardest task.

## Conclusion
    * We found LLMs struggle in all task categories but mainly in enumeration and exploitation
    * Not a single box can be completed even with human assistance without failures

## Future work

We plan to see if reinforcement learning allows LLMs to get better at pentesting in the above categories, especially focusing on enumeration and exploitation for potential leads to autopentesting.

## Note

I think recently there were other cool works published like Cybench and Autopentest benchmark which I plan to add here once I go through them in more depth",1.4520148190943885,35.0,Good
Concept Network Reinforcement Learning for Flexible Dexterous Manipulation,https://medium.com/@BonsaiAI/concept-network-reinforcement-learning-for-flexible-dexterous-manipulation-47bf459b19b9,Bonsai,1800,25,0,0.0,Deep Reinforcement Learning for Dexterous Manipulation with Concept Networks,https://arxiv.org/pdf/1709.06977,Bonsai,"# Concept Network Reinforcement Learning for Flexible Dexterous Manipulation

By: Marcos Campos & Victor Shnayder

At Bonsai, we are building an AI platform to enable subject matter experts to teach an AI how to solve complex problems in optimization and control using deep reinforcement learning. Typically, effectively using deep reinforcement learning requires a great deal of expertise in defining suitable reward functions for your task. This becomes even more challenging when the task requires coordination or sequential planning of different skills and operations.

A key feature of the Bonsai platform is the ability to decompose complex tasks using concept networks. Concepts are distinct aspects of a task that can be trained separately, and then combined using a selector concept. This approach drastically reduces the overall complexity, since the simpler problems can be trained with focused and easier-to-specify reward functions. The selector concept can be quickly learned using a simple reward function.

Today, we’ll tell you how we used this approach to solve a complex robotics task requiring dexterous manipulation: training a simulated robot arm to grasp an object and stack it on another one. A similar task was recently studied by DeepMind, getting excellent results [1]. We applied our decompositional approach, improving training efficiency and flexibility. Here is a video of the end result:

## Previous Work

A recent paper by DeepMind [1] described a similar grasping and stacking task, and solved it with two main contributions. First, by carefully crafting reward functions, they could teach an AI to learn how to correctly sequence the sub-tasks needed to solve the complete problem. Solving the problem with this approach required about 10 million interactions with the simulator. Secondly, they showed that if key subtasks were learned separately (each took on the order of 1 million interactions with the simulator), and traces from executing these subtasks were used to prime learning the full task, it was possible to learn the full task in about 1 million interactions with the simulator, thus achieving a 10x speed up over the baseline which did not use subtasks.

Our approach has its precursors in the Options Framework by Sutton et al. [5]. More recently. T. D. Kulkarni et al. has shown how a similar approach using deep hierarchical reinforcement learning could be used to learn complex sequences [2]. The main difference from our approach is that the meta-controller is learned at the same time as the basic controllers (sub-tasks) and there are no constraints on when to use each basic controller.

## Our Approach

The robotics task starts with a Kinova Jaco arm at a neutral position in a MuJoCo robotics simulator, and then moves the arm to a work area to grasp a four-sided geometric prism. Once the prism has been grasped, the arm moves the prism to an adjacent work area to stack the prism on top of a cube. The position and orientation of the prism and the cube can vary around the center point of their their respective working areas.

We decompose the task into five subconcepts — reach the object, orient the hand for grasping it, grasp it, move to the object for stacking it, and stack it on top of a block. We solve each separately, and learn a meta-controller — or selector — concept to combine them into a complete solution.

## Benefits

The hierarchical decomposition gives us several practical benefits:
    * Reward functions can be more easily defined. Instead of specifying a complex reward function for the whole task, the system designer can define rewards that are specific to each sub-task. These are usually simpler to define. Once the sub-tasks are ready, the designer can specify a simpler and potentially sparse reward function for selector nodes. This greatly simplifies solving complex problems with reinforcement learning.
    * A pre-trained model for solving a task can be used as a component in a larger task.
    * Each sub-concept can use the most appropriate approach to solve its sub-problem, whether that be a classical motion controller, a pre-existing learned model, or a neural network that needs to be trained.
    * Components can be replaced without retraining the overall system. For example, we switched between single-concept and decomposed graspers and stackers several times in our experiments, and could adapt to a different grasper without having to change the reach, move, or overall selector concepts.

## Leaf Concepts

The “reach for grasping” (reach) and “move for stacking” (move) concepts are simple motions for which we use a classical motion controller. The Bonsai platform allows us to integrate such controllers using Gears, an interoperability feature we announced in June of this year. The orient, grasp, and stack concepts are neural controllers trained with deep reinforcement learning, using the TRPO-GAE algorithm [3].

Each concept is trained in order once its precursors in the concept graph have been trained. First the system trains orient, grasp, and stack independently. Once these concepts are trained the system trains the overall grasp and stack concept.

## Selector

As shown in Figure 3, the selector learns to choose the action recommended by the sub-concept most applicable in the current state. This is a discrete reinforcement learning problem, that we solve with DQN, using progress toward overall task success as the reward (any discrete RL approach could be used). To make this effective, we don’t choose a new sub-concept at each time step. Instead, the selector uses long-running concepts: each subconcept can have pre-conditions for when it can be selected, and a run-until condition to meet before switching to another task. This gives the designer an easy way to specify constraints like “don’t try to grasp until you’re close to the object”, and “once you start to move, continue that for at least 100 time steps”.

## Inkling

Inkling is Bonsai’s special purpose programming language used to codify the concepts the system should learn, how to teach them, and the training sources required (e.g. simulations). Collectively, we refer to these techniques as Machine Teaching. The Bonsai Platform can integrate these taught concepts to learn new skills. Read more about Inkling in the Bonsai Docs.

## Implementation Details and Results

Figure 4 shows the number of samples (environment transitions) required to learn each of the concepts. The grasp and stack (Selector) concept only took about 22K samples to converge — this is drastically faster than the number of samples required to learn the other tasks. Because the other concepts can be trained in parallel or could be already pre-trained, the overall time for solving the full problem using a composition of concepts is significantly reduced. In the ideal case, with pre-trained sub-concepts, this gives a 500x speedup over DeepMind’s all-at-once solution, and a 45x speedup over their approach of using subtask traces to speed up training [1].

All tasks (including the full task) achieved 100% success on 500 test executions. Parameters for the algorithms and detailed equations for the reward functions are provided in our research paper.

## Classical Concepts

We implemented the task reach and move using inverse kinematics classical controllers. These did not require training.

Reach moved the arm from its initial position (always the same) to a staging area for starting grasping. The staging area for grasping was defined as a fixed point centered above the grasping working area.

Move repositioned the arm from the end position of the grasp task to the staging area for stacking. The staging area for stacking was defined as a fixed point centered above the stacking working area.

## Orient Concept

The orient concept was trained using TRPO-GAE on about 2 million samples using the following reward function:
    * if fingers are oriented properly, give the maximum reward for success
    * otherwise, reward increases from zero to a small value as distance to prism decreases and orientation becomes better.

Here is the training graph and a video of orient training:

## Grasp Concept

The grasp concept (called lift in our paper) was trained using TRPO-GAE and the endpoints of the orient concept task as starting arm configurations. We collected 100K sample starting points by executing the orient concept with different prism location and orientations. The grasp concept converged after about 5 million samples using the following reward function:
    * if fingers are not pinched, reward for pinching fingers (reward increases from zero to a low tier 1 value)
    * if fingers are pinched, and prism is not touching the ground, reward for increasing height of prism (reward increases from the tier 1 value to a tier 2 value)
    * if prism has successfully reached a certain height, give the maximum reward for success

Here is the training graph and a video of grasp training:

## Stack Concept

The stacking concept was trained with TRPO-GAE on about 2.5 million samples using the following reward function:
    * if prism has been stacked properly give the maximum reward for success
    * otherwise, reward for decreasing distance between prism and cube (reward increases from zero to a small tier 1 value as distance decreases) and for better orientation of prism for stacking (reward increases from zero to the same tier 1 value as orientation improves).

Here is the training graph and a video of stack training:

## Selector — Full Task

We used DQN [4] to train the grasp and stack concept. Figure 1 shows a video for an exemplary run for the full task. Figure 11 shows the training graph — the selector learns very quickly (6K training steps, corresponding to about 22K interactions with the simulator) to sequence the different concepts in order to solve the problem.

We used the following reward function:
    * if grasped prism has stacked properly, give the maximum reward for success;
    * or, if grasped prism is within the staging area for stacking, give a tier 4 reward;
    * or, if the prism has been successfully lifted to a certain height, give a tier 3 reward;
    * or, if the prism has been oriented properly, tier 2 reward;
    * or, if the prism is within the staging area for grasping, give a small tier 1 reward;
    * otherwise, give no reward

## Challenges

The problem we chose to tackle is quite difficult. Even after splitting it into simpler subproblems using Concept Networks, there remain design decisions that require careful consideration. Read our arXiv paper to learn more about
    * Decomposing reward functions
    * Ensuring consistency between grasp orientation and the orientation needed for stacking
    * Picking a good problem decomposition, including using multi-level hierarchy

## Future work
    * As described above, the selector must choose among the actions selected by its sub-concepts. A followup post will describe how the selector can learn to complement these skills with its own learned actions.
    * Working with Bonsai customers to apply these techniques to real tasks with real robots!

## Join us!

If working on a platform to support flexible, usable reinforcement learning and AI is interesting, join us! If you’re interested in using our platform to solve your control or optimization tasks, check out our Getting Started page.

## References

[1] I. Popov et al. Data-efficient Deep Reinforcement Learning for Dexterous Manipulation, 2017. URL https://arxiv.org/pdf/1704.03073.pdf

[2] T. D. Kulkarni et al. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation, 2016. URL https://arxiv.org/pdf/1604.06057.pdf

[3] J. Schulman, et al. High-dimensional continuous control using generalized advantage estimation, 2015. arXiv:1506.02438v5 [cs.LG].

[4] Mnih, V. et al. Playing Atari with Deep Reinforcement Learning. NIPS Deep Learning Workshop, 2013. arXiv:1312.5602v1.

[5] Sutton, R. et al. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence 112 , 1999: 181–211.",1.4667374693038036,35.0,Good
New paper: The Incentives that Shape Behaviour,https://medium.com/data-science/new-paper-the-incentives-that-shape-behaviour-d6d8bb77d2e4,Ryan Carey,43,239,1,816000.0,The Incentives that Shape Behaviour,https://arxiv.org/pdf/2001.07118,TDS Archive,"## AI ALIGNMENT AND SAFETY

# New paper: The Incentives that Shape Behaviour

## How causal models can describe an agent’s incentives.

Ryan Carey and Eric Langlois, introducing The Incentives that Shape Behaviour.

Machine learning algorithms are often highly effective, but it can be difficult to establish their safety and fairness. Typically, the properties of a machine learning system are established by testing. However, even if a system behaves safely in a testing environment, it may behave unsafely or unfairly when it is deployed. Alternatively, the properties of a model can be investigated by analysing input perturbations, individual decisions, or network activations, but this is often difficult, time-consuming, and highly demanding of expertise.

Rather than examining or testing individual models, our alternative approach is to look at whether a given training environment incentivises unsafe or unfair decisions.

This approach is not entirely new — incentives are an intuitive and pertinent object of discussion. For example, see Stuart Russell’s discussion of the incentives of contention recommendation systems below. (Other examples include Remark 1 in Hadfield-Mennell et al. and The Basic AI Drives by Steve Omohundro.)

What is the purpose put into the [social media recommendation] machine? Feed people stuff they want to click on, because that’s how we make money. Well how do you maximize clickthrough — you just send people stuff they like clicking on, right? Simple as that. Actually, that’s not what the algorithms are doing… That’s not how reinforcement learning works. Reinforcement learning changes the state of the world to maximize the reward. The state of the world in this case is your brain… [so] it changes you in a way that makes you more predictable so that it can then send you stuff that it knows you’re going to click on.” — Stuart Russell

The pressure to modify user behaviour can be viewed as an undesired incentive. Incentive-based arguments like this one are powerful, in that they apply independently of system architecture. Yet most previous work on incentives has focused on specific problems, which makes it hard to apply it to new problems and situations. In our recent work, we have begun to develop a general, causal theory of incentives, which allows us to state and formulate solutions to many kinds of fairness and safety problems in a unified framework.

In our theory, an incentive, roughly, is something an agent must do to best achieve its goals. We consider two types of incentives: A control incentive is present when the agent must control some component of its environment in order to maximise its utility (such as the “user opinions” in the social media recommendation example above). A response incentive is present when the agent’s decision must be causally responsive to some component of its environment — for example, a locomoting robot should attend to the positions of obstacles when navigating rough terrain.

# Control Incentives

## Examples

To make incentive analysis formal, we can use causal influence diagrams. A causal influence diagram represents a decision problem by breaking it down into a graph, where each variable depends on the values of its parents (X is a parent of Y if there is an arrow X->Y). It consists of three types of nodes:

For example, Stuart Russell’s social media manipulation example can be represented with the following influence diagram.

In this model, a recommender algorithm selects a series of posts to show a user in order to maximise the number of posts that the user clicks on. If we consider the user’s response to each post to be an independent event, then content that the user appreciates will receive more clicks. However, the posts also have an indirect effect. If the user views many polarising articles then they might adopt some of those views and become more predictable in terms of what they’ll click on. This may allow the algorithm to achieve a higher clickthrough rate on the later posts in the series and means that there is a control incentive on Influenced user opinions.

In order to alleviate Stuart Russell’s concern (while preserving the function of the system), we want to remove the control incentive on user opinion, while preserving the incentive on clicks. We could redesign the system so that instead of being rewarded for the true click rate, it is rewarded for the predicted clicks on posts based on a model of the original user opinions. An agent trained in this way would view any modification of user opinions as irrelevant for improving its performance.

To work in practice, the click prediction must not itself include the effect of user opinion modification. We might accomplish this by using a prediction model that assumes independence between posts, or one that is learned by only showing one post to each user. This speaks to an important consideration when reasoning about incentives: the lack of an incentive on a variable (such as clicks) is only practically meaningful if none of the variables (such as predicted clicks) act as “proxies” for another. Otherwise, a control incentive on predicted clicks might systematically induce the same kinds of decisions that a control incentive on clicks would induce, even if there is no control incentive on clicks. In future work, we plan to analyse what hidden incentives proxy variables can give rise to.

This example fits a recurring pattern relating control incentives with safety and performance: Some control incentives are necessary for good performance, but the wrong ones may lead a system to be unsafe. For example, AlphaGo works well because it has a control incentive to protect its stones (performance) but not to protect its servers (safety). Ensuring that the control incentives match the user’s preferences is a central problem in safe incentive design.

Defining control incentives

Now that we have the basic intuition of control incentives, we may consider how to define them. Suppose that there is some variable X (such as the user’s political views). We can consider the attainable values that X can take if the AI system behaved differently. If setting X to any attainable value x (such as “left-winger”, “centrist”, or “right-winger”) changes the performance, then we say there is a control incentive on X. Under this definition, a control incentive may arise for any variable on a causal path from the decision to the utility.

Everitt et al. defined the related concept of intervention incentives. A variable faces an intervention incentive if utility can be gained by directly setting its value. (This is equivalent to the value of control being nonzero.) Intervention incentives are less predictive of agent behaviour than control incentives, because they do not consider what the agent is able to influence with its decisions — hence our paper’s title, “The Incentives that Shape Behaviour”.

Let’s return to our example to highlight the difference between these two incentives. All variables leading to utility have intervention incentives but only those that are also downstream of the action have control incentives.

# Response Incentives

Which events must an optimal decision be responsive to?

This question has important implications for both AI safety and fairness. For AI safety, if the variable is a shutdown command, it is desirable for the AI system’s behaviour to respond to this variable. Such a response incentive is not sufficient for safety, but it is a good start. In contrast, if this incentive is absent, then optimal policies can easily be unsafe. It is similarly desirable to have a response incentive for human commands in general, and for a value-learning system to have a response incentive on human values.

It also has important implications for fairness. If a sensitive variable such as race or sexual orientation has a response incentive, then this indicates an incentive for trained algorithms to be counterfactually unfair. We show in our paper that if there is a response incentive on a sensitive attribute, then all optimal policies are counterfactually unfair with respect to that attribute. Our paper takes some steps toward defining unfair incentives: predominantly focusing on how to rule out the presence of unfair incentives in a given graph.

The desirability of a response incentive thus depends on the variable subject to change. For some variables, we want an AI system to respond to them, in order to behave safely. For other variables, if an AI system responds to it, then we consider that system unfair.

# Applications, Limitations, and Next Steps

This theory is already demonstrating its value through its applications. In addition to the safety and fairness problems discussed, it has been applied to the analysis of AI boxing schemes and reward tampering problems (blog post). As the fairness example shows, the theory doesn’t necessarily require the agent to reason causally or have causal models, just that we, the designers, can reason about the agent’s behaviour causally.

In the long-term, our aspiration is that when researchers anticipate possible safety or fairness concerns, they use this theory to perform incentive analysis of their AI system. This would generally involve drawing a causal diagram of how various agent components can be fit together and forming a judgement about what incentives ought to be (or not be) present, before applying our graphical criteria to automatically discern which incentives there are. In a very optimistic case, incentive analysis would become a standard tool for establishing the trustworthiness of an AI system, similarly to how statistical methods are used for describing AI performance. But in the short-term, we expect it to take some work to use these methods, and so we are happy to provide advice where it is needed.

This theory is not completed yet, as it is currently restricted to the single-agent setting. We are working on extending it to a multi-decision case, and ultimately, we would like it to deal with multiple agents. The paper is available at:

R Carey, E Langlois, T Everitt & S Legg. The Incentives that Shape Behaviour (2020), SafeAI@AAAI.",3.4957539976785923,84.0,Excellent
Improving Pinterest Search Relevance Using Large Language Models,https://medium.com/pinterest-engineering/improving-pinterest-search-relevance-using-large-language-models-4cd938d4e892,Pinterest Engineering,58000,32,0,15700.0,Improving Pinterest Search Relevance Using Large Language Models,https://arxiv.org/pdf/2410.17152,Pinterest Engineering Blog,"# Improving Pinterest Search Relevance Using Large Language Models

Han Wang | Machine Learning Engineer, Relevance & Query Understanding; Mukuntha Narayanan | Machine Learning Engineer, Relevance & Query Understanding; Onur Gungor | (former) Machine Learning Engineer, Relevance & Query Understanding; Jinfeng Rao | Machine Learning Engineer, Pinner Discovery

# Background

Pinterest Search is one of the key surfaces on Pinterest where users can discover inspiring content that aligns with their information needs. Search relevance measures how well the search results aligned with the search query. Using a relevance objective allows the search engine to ensure that the content displayed to users is genuinely pertinent to their information needs, rather than overly relying on factors like past user engagement.

In this work, we focus on improving the search relevance model. To measure the relevance between queries and Pins, we use a 5-level guideline (see Table 1).

In this blog, we will go through the technical design and share some offline and online results for our LLM-based search relevance pipeline. More details can be found in our full paper.

# Technical Design

# LLM as Relevance Model

## Model Architecture

We use a cross-encoder language model to predict a Pin’s relevance to a query, along with Pin text, as shown in Figure 1. The task is formulated as a multiclass classification problem. We fine-tune the models using human-annotated data, minimizing cross-entropy loss.

## Pin Text Representations

Pins on Pinterest are rich multimedia entities that feature images, videos, and other contents, often linked to external webpages or blogs. To represent each Pin, we use the following varied set of text features derived from metadata, the image itself, as well as user-curated data. These features are designed with a focus on providing reliable high-quality representations, while retaining high coverage across Pins on Pinterest Search.
    * Pin titles and descriptions: the titles and the descriptions assigned by the user who created the Pin.
    * Synthetic image captions: synthetic image descriptions generated by Bootstrapping Language-Image Pre-training (BLIP), an off-the-shelf image captioning model.
    * High-engagement query tokens: unique queries with the highest engagement with this Pin on the search surface over the past two years.
    * User-curated board titles: titles of user-curated boards where the Pin has been saved.
    * Link titles and descriptions: titles and descriptions from the linked external webpages.

# Distill LLM into Servable Student Model

## Model Architecture

Our cross-encoder LLM-based classifier is hard to scale for Pinterest Search due to real-time latency and cost considerations. Therefore, we use knowledge distillation to distill the LLM-based teacher model into a lightweight student relevance model. The student model served online uses the following features:
    * Query-level features: query interest features, shopping interest features, and SearchSAGE query embeddings
    * Pin-level features: PinSAGE embedding, visual embedding for the image, and SearchSAGE Pin embeddings
    * Query-Pin interaction features: BM25 and text match scores for different text fields, historical engagement rates between the Pin and query, etc.

These features are embedded and passed through a feed-forward network to predict 5-scale relevance scores, as shown in Figure 2.

## Knowledge Distillation and Semi-Supervised Learning

To train our student relevance model, we employ the LLM-based teacher model to generate 5-scale relevance labels on a daily logged large search engagement and impression dataset with billions of rows. This labeled dataset is subsequently used to train the much smaller student model. A diagram of the search relevance system at Pinterest is shown in Figure 3. The relevance scores generated by the student model are then utilized alongside engagement predictions to determine the final ranking of search results.

This blend of knowledge distillation and semi-supervised learning not only makes effective use of vast amounts of initially unlabeled data, but also expands the data to a wide range of languages from around the world and new concepts not encountered in our human-labeled data owing to the seasonality in Pinterest Search. By using a multilingual LLM-based teacher model, we are able to successfully generalize from human-labeled data focused on US queries to unseen languages and countries.

# Offline Experiments

We now present offline experiments to demonstrate the effectiveness of each modeling decision. The teacher model is trained and evaluated using human-annotated relevance labels. In all offline experiments, we report the accuracy of 5-scale relevance predictions and the AUROC metrics for binarized labels with thresholds at 3, 4, and 5, since correctly identifying highly relevant content is more important for search ranking.

## Comparison of Language Models

In this experiment, we evaluate the following pre-trained language models: multilingual BERT-base, T5-base, mDeBERTa-V3-base, XLM-RoBERTa-large, and Llama-3–8B. These models are initialized from Hugging Face checkpoints and fine-tuned using our in-house search relevance training data. For larger language models such as Llama, we first load quantized model weights and then apply qLoRA for fine-tuning. Additionally, we incorporate gradient checkpointing and mixed precision techniques to further improve training efficiency and memory usage.

Table 3 shows the performance of different language models. As a baseline, we include a model that relies solely on the SearchSAGE embeddings. In this comparison, we keep the text features for each Pin and the maximum text length fixed, varying only the language models. The results in Table 3 clearly demonstrate that the language models offer additional improvements over our in-house content and query embedding. Furthermore, more sophisticated language models and larger model sizes consistently enhance the relevance prediction performance. Specifically, the Llama-3–8B outperforms the multilingual BERT-base model by 12.5% and the baseline model by 19.7% in terms of 5-scale accuracy.

## Importance of Enriching Text Features

To predict the relevance of a Pin to a query using only textual information, we enrich the Pin text representations with several carefully designed text features. We conduct an analysis to assess the impact of each text feature on relevance prediction, using mDeBERTa-V3-base as the language model and fixing the maximum text length to 256. The results, summarized in Table 4, demonstrate that the model’s performance consistently improves with the sequential addition of these text features. This indicates that enriched text features and metadata significantly contribute to building a more robust relevance model.

## Scaling Up Training Labels through Distillation

By using knowledge distillation and semi-supervised learning, we can effectively scale the training data beyond the limited human-annotated data. Table 5 demonstrates the benefits of training on increasing amounts of augmented teacher-generated labels.

Table 5: Comparisons of production model performance when training on different amounts of labels.

# Online Results

Besides offline experiments, we also conducted an online A/B experiment to assess the effectiveness of our new relevance model.

## Human Relevance Evaluations

We set up evaluations with human annotators to assess the relevance of the search feeds with and without the new relevance model serving traffic. The nDCG@K here is calculated as follows (more details in our paper):

Our proposed relevance modeling pipeline leads to a +2.18% improvement in search feed relevance, as measured by nDCG@20.

The results in Table 6 indicate that the multilingual LLM-based relevance teacher model effectively generalizes across languages not encountered during training (precision@8 used for controlling annotator costs).

## User Triggered Experiments

In addition to relevance, another primary metric is the search fulfillment rate. This metric is defined as the number of search sessions that result in a high-significance user action. The improvements in relevance also result in increased fulfillment in non-US countries, despite not having annotated data available for those countries during model training (Table 7).

# Summary

In this work, we presented an LLM-based relevance system for Pinterest Search. We thoroughly described each building block of this system, including model architecture, enriched text features, augmented label generation, and online serving. We conducted extensive offline experiments to validate the effectiveness of each modeling decision. Lastly, we presented the results from online A/B experiment, which showed an improvement of >1% in search feed relevance and >1.5% in search fulfillment rates.

# Future Work

To further enhance the efficacy of our relevance system, future work will explore the integration of servable LLMs, vision-and-language multimodal models (VLMs), and active learning strategies to dynamically scale and improve the quality of the training data.

# Acknowledgement
    * Ads Relevance: Helen Xu, Rakesh Chalasani
    * Search Leadership: Krishna Kamath, Kurchi Subhra Hazra",1.3513009297295486,33.0,Average
BYOG (Build Your Own Guardrails) with Synthetic Data,https://medium.com/dsaid-govtech/byog-build-your-own-guardrails-with-synthetic-data-f38f9da2deae,Shing Yee,16,136,0,894.0,A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection,https://arxiv.org/pdf/2411.12946,"AI Practice and Data Engineering Practice, GovTech","# BYOG (Build Your Own Guardrails) with Synthetic Data

By Chan Shing Yee and Gabriel Chua

Over this past year, we’ve explored how we think about LLM guardrails here at GovTech, how to get started integrating them, and how we trained our own localised content moderator. In our next two posts, we’ll dive into our latest research paper where we leverage synthetic data to train guardrails in pre-production, and how we applied this framework to train an off-topic prompt guardrail.

As Large Language Models (LLMs) become increasingly popular, one challenge has been ensuring LLM-powered applications don’t go off-topic when users prompt these models with queries or requests that fall outside of their intended purpose. While such prompts may not be harmful or illegal, they can lead to inefficiencies and errors, undermining the model’s effectiveness. For example, if a chatbot designed for customer service in a telecommunications company is asked for medical advice, it constitutes off-topic misuse. This can result in poor user experiences and compliance risks.

Additionally, after surveying the landscape of guardrail frameworks, another challenge we identified in adopting guardrails is that we typically have little to no real-world data in pre-production to train or configure such guardrails. Some approaches require positive or negative examples, or the training of custom classifiers for each use-case.

At GovTech, we’ve arrived at a methodology to train LLM guardrails using LLM themselves to provide the synthetic data. We’ll deep dive into this methodology and how we applied it to the off-topic prompt detection problem in the next two posts. In this first part, we’ll explore the challenge of developing such guardrails in pre-production, when we typically have little to no real-world data to train or configure them. We’ll then explain how we used synthetic data to create a realistic and diverse dataset. In the second part, we’ll dive deeper into how we trained our model and share the results of our work. These two posts are based on our paper.

# The Off-Topic Prompt Detection Problem

Our goal was to build an input guardrail that could determine whether a user prompt is on- or off-topic based on the system’s intended purpose. We decided to use the system prompt as the reference point for this classification. System prompts are predefined instructions that guide how the LLM should respond to a user’s query. They typically specify the task, provide context, and define the desired response style. This becomes a binary classification problem: 0 = on-topic, 1 = off-topic.

Implementing such input filters ensures that only relevant, on-topic prompts get through, protecting the system from unintended misuse.

# Challenges in Pre-Production

Building effective guardrails is complex, especially when working with limited real-world data during early development stages. User data that could be used for training classifiers is often unavailable since the application has not yet been deployed. Moreover, relying on curated examples for training can be limiting, as these may not capture the wide variety of potential off-topic prompts that could arise in practice.

In existing solutions, examples are often required to train guardrails. These examples help the model identify off-topic prompts by teaching it what kinds of queries should be avoided. However, a major challenge is that it’s impossible to anticipate every off-topic prompt a user might generate. Attempting to create a list of off-topic topics is problematic because the number of possible off-topic prompts is likely endless. This method also runs the risk of over-blocking — where the system mistakenly flags a valid prompt as off-topic, resulting in false positives for users.

# Using Synthetic Data

Given these challenges, we decided to use synthetic data to build our guardrails. Synthetic data is artificial data generated by a model rather than collected from real users. It’s an ideal solution when real-world data is scarce or unavailable, as it allows us to quickly generate large amounts of relevant training data while ensuring privacy. Synthetic data can be tailored to include specific scenarios we want to cover, making it a great resource for training systems to detect off-topic prompts.

Using synthetic data also allowed us to simulate a wide variety of prompts, including edge cases and unusual scenarios that we might not have encountered in real-world data.

# Guardrail Development Methdology

Overall, our method avoids the need for real-world misuse examples. Instead, we:
    * Define the problem space qualitatively
    * Use an LLM to generate synthetic misuse prompts
    * Train and test guardrails on this dataset

# Steps for Dataset Creation

## 1. Problem Analysis and Edge Case Identification

We began by analysing the model’s intended functionality and identifying potential misuse scenarios. For instance, a customer service chatbot for a telecom company should handle inquiries about products but reject prompts related to medical or legal advice. Defining what qualifies as acceptable versus off-topic prompts was crucial for creating a boundary between what the system should process and what it should block.

Next, we curated a list of real-world system prompt examples to guide the LLM in generating realistic and effective system prompts within our synthetic dataset. This approach ensures that our synthetic data closely aligns with real-world applications, enhancing the model’s ability to produce accurate and contextually relevant responses across various scenarios.

## 2. Generating Synthetic Data with LLMs

With these scenarios in mind, we used a LLM, specifically GPT-4o, to generate a wide range of synthetic prompts. The idea was to produce diverse and realistic examples that include both on-topic and off-topic queries.

To introduce variability and increase the diversity of the generated prompts, we set high-temperature* values (temperature=1) for the generation process. This high temperature encourages the LLM to generate more unpredictable, diverse, and creative prompts that better reflect real-world user behaviour.

Additionally, we used the Python Faker library to randomise word lengths and generate 10 random seed words. The random word lengths helped vary the length of the outputs, while the randomly generated words were injected into the system prompts. This further increased the randomness and diversity of the prompts, allowing for a broader range of scenarios to be covered in the synthetic dataset.

*Temperature is a parameter that can be adjusted to influence the LLM’s output. A higher temperature (e.g., 1) will result in more creative outputs while a lower temperature (e.g., 0) will result in a more predictable output.

## 3. Structuring the Outputs

To ensure that the generated outputs were both diverse and structured, we used constrained generation methods, such as enforcing specific formatting rules, to ensure the outputs adhered to desired structures while remaining realistic.

OpenAI’s Structured Outputs feature was used to ensure that the system prompts, along with their corresponding on-topic and off-topic user queries, were generated in the correct format from the start. This feature allows us to set specific rules for how the outputs should be organiSed, ensuring that each example follows a consistent structure, such as using a JSON schema. By integrating this feature, we ensured consistency in the dataset, improving overall quality and usefulness.

Since we needed to generate a large amount of data quickly, we used OpenAI’s Batch API to send multiple requests at once. This enabled us to batch several tasks together, allowing the model to handle many requests simultaneously, thus speeding up the data generation process. As a result, we were able to efficiently produce a large volume of data while ensuring it remained consistent and aligned with our objectives.

Using the approach outlined above, we successfully generated over two million pairs of system prompts and user prompts.

The following images illustrate the overall workflow of how we used GPT-4 to create our synthetic data and the tokens utilised in generating over two million samples.

# Conclusion

By combining problem analysis, synthetic data generation, and structured output creation, we established a scalable and flexible framework for building off-topic guardrails in LLM applications. This approach enabled us to overcome the challenges of pre-production development by generating diverse, privacy-preserving data and ensuring the guardrails could effectively filter out off-topic prompts. You can read more about this framework in our paper here.

For further details about how we generated synthetic data, please refer to this article. You may also find the open-source dataset here.

In Part 2, we will dive deeper into how we used this synthetic dataset to train our off-topic detection model and share the results of that process.",3.890789153144719,94.0,Excellent
Open-sourcing an Off-Topic Prompt Guardrail,https://medium.com/dsaid-govtech/open-sourcing-an-off-topic-prompt-guardrail-fde422a66152,Shing Yee,16,113,0,894.0,A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection,https://arxiv.org/pdf/2411.12946,"AI Practice and Data Engineering Practice, GovTech","# Open-sourcing an Off-Topic Prompt Guardrail

By Chan Shing Yee and Gabriel Chua

# Introduction

In our Part 1, we introduced the challenge of training guardrails in pre-production when there is no data, giving the example of off-topic prompt detection, and how we overcame the lack of real-world data by using synthetic data. In this second part, we’ll focus on the process of training and evaluating our model on this synthetic data to detect off-topic prompts. We’ll also share the results and show how our model compares to existing methods. Further details are available in our paper.

As Large Language Models (LLMs) are used in more applications, ensuring that they only respond to relevant, on-topic queries is essential. Off-topic prompts — those outside the intended scope of the system — can lead to errors, inefficiencies, and poor user experiences. By using synthetic data generated from LLMs, we trained a classifier to detect and block these off-topic inputs before they reach the model, thus maintaining system integrity.

# Training the Model

Once we had our synthetic dataset in place, our next step was to train a model capable of detecting off-topic prompts. To train an effective classifier, we explored two key approaches: the Fine-Tuned Bi-encoder Classifier and the Fine-Tuned Cross-Encoder Classifier.

## Bi-encoder Classifier

This model uses a pre-trained embedding model, jinaai/jina-embeddings-v2-small-en, which processes the system prompt and user prompt separately. The model then learns the relationship between the two prompts through attention layers, allowing the system prompt to “attend” to the user prompt and vice versa. After processing, the embeddings are combined into a single vector, which is passed through a classification layer to predict whether the prompt is on-topic or off-topic.

## Cross-Encoder Classifier

In this approach, the system and user prompts are concatenated into a single sequence and passed through a fine-tuned stsb-roberta-base model. This model directly processes both prompts together, capturing the subtle interactions between the two. The output is then passed through a classifier to make a final decision about whether the prompt is relevant or not.

Both modes, and the data, are open-sourced here.

# Evaluation:Benchmarking Against Baselines

Once we trained our models, the next step was to evaluate their performance. We compared our fine-tuned classifiers against several baseline methods to understand how well they performed at detecting off-topic prompts.
    * Cosine Similarity: A simple method that measures the alignment between the system and user prompts based on their embeddings. Since cosine similarity scores range from -1 to 1, it’s not exactly straightforward to map these values to a zero to one scale for our classification task. To address this, we experimented with two approaches: using the absolute value of the cosine similarity score (where -1 and 1 indicate on-topic and 0 suggests off-topic), or inverting the score (where a higher score would suggest the prompt is less relevant to the system prompt). In this case, we report results using the absolute value. While easy to implement, it doesn’t capture deeper interactions between the prompts.
    * K-Nearest Neighbors (KNN): This method predicts the relevance of a prompt by comparing it to a set of labelled examples. We provide three on-topic and three off-topic prompts, to mimic how we could train a simple KNN classifier at run time. However, this approach is more basic and significantly less precise than the fine-tuned models.
    * Pre-trained Cross-Encoder Models: We also tested a pre-trained model without fine-tuning. While this approach is faster, it didn’t perform as well as our fine-tuned models, which were specifically optimised for off-topic detection.
    * ColBERT v2 Model: It encodes queries and documents into multiple vectors and computes relevance scores through fine-grained token-level interactions.
    * LLM Prompt Engineering: This method involves adjusting the system’s prompt to explicitly instruct the model to reject off-topic queries. Though it doesn’t require training, it’s less flexible and can lead to less accurate results.
    * LLM Zero-Shot Classification: This approach allows a model to classify prompts without task-specific training. While useful for comparison, it didn’t match the precision of our fine-tuned classifiers.

# Results: Performance on Synthetic Data

After evaluating the different methods, we found that our fine-tuned models significantly outperformed the baseline approaches, especially in terms of precision. Precision is crucial for guardrails, as it reduces the number of false positives — when a valid prompt is mistakenly flagged as off-topic. This is important in ensuring that users aren’t unnecessarily blocked from interacting with the system.

# Performance Across Prompt Lengths

We also looked at how the models performed across different lengths of user prompts. As expected, shorter prompts (such as a simple “Help me”) were harder to classify accurately compared to longer, more detailed prompts (“Can you help me understand my billing statement for September?”). The performance of our model improved as the length of the prompt increased, as longer prompts provide more context, making it easier for the model to classify them accurately.

To quantify this, we grouped prompts into bins based on their lengths and calculated the ROC-AUC scores for each group. A heat-map visualising these scores (see Figure 3) confirmed that their performance indeed improves as prompt length increases.

# Calibration

Calibration is an important factor for understanding how confident the model is in its predictions. If a model predicts that a prompt is on-topic with 80% confidence, we expect it to be correct about 80% of the time. Our fine-tuned cross-encoder model showed strong calibration, meaning that it was good at estimating its confidence in its predictions. This is important because it allows us to make decisions based on the model’s confidence. For example:
    * High Confidence: If the model is very confident a prompt is off-topic, we can block it immediately.
    * Medium Confidence: We might flag it for review or ask for clarification.
    * Low Confidence: The prompt can be processed normally but monitored for potential errors.

A calibration curve gives us a visual representation of the model’s confidence alignment with actual outcomes. The diagonal line indicates perfect calibration. Deviations from this diagonal indicate under- or over-confidence in the model’s predictions.

# Generalisation to External Datasets

We also tested the models on synthetic data generated by different LLMs, such as Gemini Pro 1.5 and Claude 3.5 Sonnet. This helped us ensure that our models weren’t overfitting to patterns from the GPT-4o-generated data we used for training. We found that our fine-tuned classifiers performed consistently well across these datasets, indicating that they can generalise to data produced by different LLMs.

Furthermore, we tested our models on datasets designed to challenge them with difficult prompts, such as JailbreakBench, HarmBench, and TrustLLM which include prompts attempting to bypass system restrictions or asking for harmful content. Our models performed well on these adversarial prompts, showing that they can handle off-topic queries in more complex scenarios.

These results validate the flexibility of our approach, which extends beyond generic off-topic detection to address more specific and challenging use cases.

# Inference Speed Benchmarking

In addition to accuracy, we also measured how quickly the models could process prompts. The fine-tuned bi-encoder classifier was faster, making it a better option for high-throughput scenarios. However, the fine-tuned cross-encoder model, while slightly slower, provided better precision and calibration, making it ideal for cases where accuracy is more important than speed.

# Limitations and Future Work

While our models performed well, there are some limitations:
    * Synthetic Data Bias: Synthetic data, while useful, can introduce biases based on the LLMs used to generate it. These biases may not fully represent the variety of real-world user prompts.
    * Scope of System Prompts: If the system prompt is too broad or vague, it becomes harder for the guardrails to accurately detect off-topic prompts.
    * Language and Cultural Contexts: Our models were primarily trained on English-language prompts, which means they might struggle with prompts in other languages or from different cultural contexts.

Despite these limitations, our approach provides a solid foundation for building off-topic detection systems that can adapt to real-world applications. We are working to improve the models by incorporating more diverse data and expanding their capabilities.

# Conclusion

In this article, we discussed how we built and trained a model to detect off-topic prompts using synthetic data. We explored different modelling approaches and evaluated their performance across several key factors, such as precision, prompt length, and calibration. The results showed that our fine-tuned models outperformed traditional methods in both accuracy and speed, making them a reliable tool for ensuring that LLMs respond only to relevant queries. Further details can be found in our paper.",3.7096716242612433,90.0,Excellent
The Generative Style Transformer,https://medium.com/agara-labs/the-generative-style-transformer-3564bce04d04,Akhilesh Sudhakar,14,56,1,77.0,"Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer",https://arxiv.org/pdf/1908.09368,Agara,"# The Generative Style Transformer

This post explains our paper on style transfer, “Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer” (presented at EMNLP 2019) in a more accessible manner.

The paper can be found here. Our Github repo contains code and instructions for replicability.

The What

Our paper proposes a method to perform text style transfer. ‘Style’ of text is a term that the Natural Language Processing (NLP) community has borrowed from socio-linguistics, and uses rather loosely. Style transfer involves re-writing text of a certain style into a new target style, such that only the style of the text is changed while retaining its core meaning (or style-independent content). We show the following types of style transfer in the paper: a) re-writing reviews from positive to negative sentiment and vice versa (YELP and AMAZON datasets), b) re-writing factual sentences to humorous and romantic ones (CAPTIONS dataset), c) re-writing democrat-written political social media posts to republican-written ones and vice versa (POLITICAL dataset), and d) re-writing reviews written by females to male-written ones and vice versa (GENDER dataset). Thus, in our work, the notion of style varies from sentiment to humor and romance, to political slant and to gendered-ness.

The Why (Research POV)

Why is the NLP community interested in style transfer?

The NLP research community is interested in style transfer methods for a variety of applications. One set of these is targeted at anonymizing identity online, for instance, by obfuscating the author’s gender in social media posts. Doing so could prevent targeted gender-based advertising, abuse of privacy, and discrimination. The same approach can be extended to other attributes of the user — age, race, geography, etc. Another set of applications are in dialogue generation for conversations. To convey the same message during a conversation, humans introduce variations based on the context. This is modeled as style transfer from a message to its appropriate variation. Other works use style transfer to generate poetry and to re-write formal texts to informal and vice-versa.

The Why (Agara POV)

Why are we at Agara interested in style transfer?

We build conversational systems that hold conversations in different customer support related topics, across diverse call contexts. Style transfer is important to us for two reasons.

One of them is that Natural Language Generation (NLG) itself is a hard problem, and state-of-art NLG systems (such as those for summarization and dialogue generation) are nowhere close to the human-level when they attempt to generate text from scratch. Given that these systems are data-hungry deep learning models (hello, big Transformers), data sparsity is one of the main reasons, if not THE main reason, that these models perform poorly.

The question we ask is: do our conversation models really need to generate utterances from scratch? What if we could a) ‘retrieve’ an utterance spoken in a similar context by a human agent, similar to what we intend to generate, and b) re-write (a.k.a. style transfer!) the retrieved utterance to the desired utterance?

This would result in better quality generated utterances since the model now has to learn the easier task of only making edits over the retrieved utterance vs. generating it from scratch. Also, since a good part of the retrieved human utterance will be retained as is, there is a lesser scope for generating malformed utterances.

The second reason style transfer is useful to us is that it allows us to control how our conversational agent conveys the same message, according to the context of the call. Does the customer sound vexed about a problem they have been facing for a while, and need to be reassured that a solution will be arrived at? Are they in a situation where they need to be spoken to empathetically? Or do they need to be informed politely that their request cannot be fulfilled at the moment? The same message to be conveyed can be adapted to these different cases using style transfer.

The Data

Back to the paper.

Since there isn’t any dataset available that pairs the same sentences but of different styles with each other, i.e., a parallel corpus, we approach this task in an unsupervised fashion in our paper. What we do have though, are datasets that contain a set of example sentences for each style. We leverage these datasets.

The How

First, we note that the style of a sentence is localized to a small subset of words of a sentence. We call these words attributes and the rest of the words, content. Let’s take an example from here onwards just to make things simpler.
    * Source sentence: the pizza was tasty
    * Content: the pizza was.
    * Attribute(s): tasty

We model style transfer in the Delete-Retrieve-Generate framework (Li et al., 2018). This framework:
    * Deletes only the set of attribute words from the source sentence to give the content (the pizza was ̶t̶a̶s̶t̶y̶)
    * Retrieves attributes from the target style corpus (horrible), and
    * Generates the output sentence from the content and retrieved attributes (the pizza was horrible)

For 1., we train a Delete Transformer (DT), which is a BERT classifier that is trained to predict the style of a given sentence. This training is possible because of the datasets we have. For a given source sentence (the pizza was tasty), we use the attention weights of the classifier to decide which words are attribute words. Since attribute words contribute to deciding the style of the sentence, the trained classifier pays higher attention to them. Hence, words that receive high attention weights when fed to the classifier, can be treated as attributes (tasty). These attributes are removed from the source sentence to give the content (the pizza was).

For 2., we find the closest sentence in the target style’s dataset (the sushi was horrible), based on how similar its content (the sushi was) is to the source’s content (the pizza was). The attributes of this closest sentence from the target corpus, are our retrieved attributes (horrible).

For 3., we propose and train the Generative Style Transformer (GST). GST initially has the same architecture and pre-training as the GPT and is thus, initially a powerful decoder-only language model.

The Models

GST is then trained in 2 variants — B-GST (Blind GST) and G-GST (Guided GST) — 2 separate models that are independently trained and whose outputs are compared with each other. Both take as input the source sentence’s content (the pizza was) and generate style-transferred output (the pizza was horrible). They differ in the additional inputs each of them takes, apart from the source’s content:
    * B-GST additionally takes as input: the target style (<negative>).
    * G-GST additionally takes as input: the retrieved attributes (horrible).

The figure below shows the generation using G-GST. B-GST is similar except that it does not have a retrieve component. [ATTRS] is a special token representing the start of the attributes section, [CONT_START] represents the start of the content section, and [START] indicates to the model that it has to start generating the output.

Why These Variants?

B-GST is a model that generates output by placing the most appropriate target attributes in the given content, which it learns on its own from the distribution of the dataset. This variant is useful when there aren’t enough examples or enough attribute-coverage in the target style to find the closest match to ‘retrieve’ attributes from.

G-GST, on the other hand, is a model that generates output by placing the given attributes in the given content. This variant is useful when explicit control is required over the attributes to be used in the output, and when a good ‘retrieval’ mechanism can make it simpler for GST to generate good outputs.

How are the GSTs trained if there’s no parallel corpus?

By reconstruction. Since there’s no parallel corpus to train the GSTs on, they are simply trained to reconstruct the input. This method is possible, it works and it generalizes well to inference time because both GSTs take the source content as input and not the entire source sentence as is.

Sample Outputs

Below are sample outputs from the paper. SRC is the source sentence. SE, D, and D&R are previous state-of-art models explained in the paper. B-GST and G-GST are our models. YELP, AMAZON, CAPTIONS, POLITICAL, and GENDER are the datasets, each used for a different style transfer task.

From these examples, you can see that our models perform style transfer better than the previous state-of-art models do (the paper provides metrics to justify this quantitatively too). Our models’ output sentences are more fluent and natural-sounding, retain the content of the input sentence better and match the target style better.

What makes the GSTs produce better outputs than previous models?

Learnings and takeaways.

For one, using the architecture and pre-training of the GPT, which combines the power of both the transformer and massively pre-trained language models. Previous models that used LSTMs (for instance) fail at generating longer and more complex sentences.

Second, using BERT’s attention weights to delete attribute words works better than previous methods that attempted to use a deletion-based approach. This could be because BERT also combines the power of transformers and massive pre-training (as a Masked Language Model).

Lastly, the datasets that we used all required only localized edits on input sentences to transform them into the output. Most previous works don’t leverage this observation while we do so in using the delete-retrieve-generate framework.

What next?

Where are we at Agara headed with this line of work?

To improve the quality of style transfer itself, we’re working on using reinforcement learning. We’re also looking at shaping the attention weights of the Delete Transformer such that the shaped attention weights are better indicators of attributes. Further, we will adopt the delete-retrieve-generate mechanism in our conversational agents, as mentioned earlier on in this blog.

More updates on all of the above will follow in our future blog posts.

Do check out our paper for more detailed explanations of our work!",3.1260952855202984,75.0,Very Good
Scaling Training Data Attribution,https://medium.com/people-ai-research/scaling-training-data-attribution-f7d1eddd85da,Tyler A. Chang,3,3,0,3000.0,Scalable Influence and Fact Tracing for Large Language Model Pretraining,https://arxiv.org/pdf/2410.17413,People + AI Research Blog,"Featured

# Scaling Training Data Attribution

## The science of how training data influences LLM behavior

Written by Tyler A. Chang, with Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, and Ian Tenney.

In this post, we describe our recent scaling of training data attribution (TDA) methods to LLM pretraining, and the curious phenomena this uncovered: the examples that influence a model’s knowledge of a fact are often not the ones that directly express or imply it.

Large language models (LLMs) are trained on billions to trillions of words, but it is largely unknown how the models leverage their training data to make predictions. One promising branch of methods to understanding this process is called training data attribution (TDA). These methods aim to identify influential training examples for specific model outputs. However, a major limitation to advancing TDA research for LLMs has been the sheer scale of LLM pretraining, which is the first, longest, and arguably most important stage of LLM training.

In our paper, “Scalable Influence and Fact Tracing for Large Language Model Pretraining”, we demonstrate how training data attribution methods can scale to LLM pretraining. Relative to previous work on large models (e.g here, here, and here), we were able to retrieve influential examples from over 30x more pretraining examples and for over 100x more queries. Here, we introduce our advances in scalable TDA methodology, which we call TrackStar.

TrackStar takes a model prediction and ranks training examples by their estimated influence on that prediction. It combines several optimizations that have been studied in smaller settings, along with a novel task-specific metric correction. This helps Trackstar achieve high performance even when retrieving from a large pretraining corpus. TrackStar allowed us to experiment with an 8B-parameter LLM, retrieving influential examples from a complete pretraining corpus of over 160 billion tokens, for thousands of query prompts.

One of the biggest surprises in this work concerns how LLMs leverage training data to make factual predictions, such as knowing that San Diego is in California. Training examples that contain facts are not necessarily the examples that most influence the LLM to output those facts. To help get a sense of this, we’ve built an interactive results browser to explore applications of TDA to factual predictions, as well as commonsense reasoning, arithmetic, and open-ended generation in LLMs.

# Scaling training data attribution methods

TrackStar is motivated by previous work that uses gradient-based methods for TDA. These methods measure how each training example would change the model’s parameters. Examples that change the model in a similar way to a target query are deemed more “influential” for that query. For instance, say we want to know which training examples influence a model to complete the input text “San Diego is located in the state of” with the output text “California”. A training example “The weather in San Diego is similar to many other cities in California” might do this. Model gradients encode what different examples teach a model.

Due to their relative efficiency at estimating influence, gradient-based methods such as TracIn, TRAK, LoGra, and EK-FAC have been popular for TDA in recent years. TrackStar combines several innovations from these approaches. It uses correction terms (for interested readers, task-specific Hessian approximation and optimizer second moment correction) to align model gradients with the model’s actual learning process. It also uses random projections to reduce the dimensionality of model gradients, and unit normalization to reduce the effect of “loud” (high gradient) training examples.

These random projections and correction terms allow us to achieve good results even with fairly low-dimensional gradients. We’re able to reduce 8-billion-dimensional gradient vectors to less than 100,000 dimensions — resulting in an 80,000x decrease in memory and retrieval costs. With these low-dimensional gradients, we can precompute and save the gradient for every example in the pretraining dataset. This way, each time we want to retrieve influential training examples for a new query (e.g. a new model prediction), we need only compute the “closest” training examples using their precomputed gradient vectors.

# Influence vs. attribution

In our paper, we retrieved influential pretraining examples for 5,000 facts in the T-REx dataset. About 50% of the time, one of the top ten influential training examples explicitly contained the fact of interest. This is a significant advance over previous work because we retrieved from far more training examples — to be ranked in the top ten, an example needs to rank in the top 0.000003%. One of the biggest surprises here is that our 50% retrieval rate (recall) is actually quite low relative to simple keyword matching approaches such as BM25, which achieves a recall of around 80%.

One view is that this is a strong negative result for TDA methods, but this discrepancy also highlights a fascinating and important distinction:

The training examples identified by our approach often do not directly state or imply a fact of interest (factual attribution), but they have a much greater causal influence on the LLM when we allow additional training on them.

In short, examples that imply a fact are not necessarily the examples that most influence an LLM to express that fact. LLMs are often influenced by training examples that act as rough rules of thumb, such as common countries, names, and entities, and associations between them.

We also found that factual attribution and causal influence align more as models improve. Models with more parameters and trained on more data appear to rely more on training examples that actually imply individual facts. This suggests that as we scale TDA to newer generations of LLMs, TDA results may align more closely with human intuition.

# Motivating future TDA work for LLMs

Our research pushes the frontier of scaling TDA closer to full pretraining data attribution for modern LLMs. We invite you to explore this and much more in our results browser, which we hope will motivate deeper reflection on the challenges and opportunities of gradient-based TDA. We hope that this work informs future research into TDA, including its challenges relative to simple keyword matching, and leads to deeper scientific understanding of how data influences LLM learning and behavior.

## Credits & Acknowledgements

Written by Tyler A. Chang, with Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, and Ian Tenney. We thank Mahima Pushkarna and Jonathan Caton for help editing this post, and the rest of the PAIR team for wonderful support.",1.1493420517114166,28.0,Average
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery (DMLR),https://medium.com/sinicx/rethinking-symbolic-regression-datasets-and-benchmarks-for-scientific-discovery-dmlr-b71639de3b03,Yoshitomo Matsubara,16,11,0,63.0,Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery,https://arxiv.org/pdf/2206.10540,OMRON SINIC X,"# Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery (DMLR)

We are pleased to announce that our paper on symbolic regression datasets and benchmarks for scientific discovery was accepted at the Journal of Data-centric Machine Learning Research (DMLR), a new sister journal of the JMLR.

# TL; DR

The paper is about symbolic regression for scientific discovery (SRSD). Symbolic regression (SR) is a task of producing a mathematical expression (symbolic expression) in a human understandable manner that fits a given dataset. We pointed out a number of issues in the current symbolic regression datasets and benchmarks for data-driven scientific discovery and proposed new datasets and benchmark.

# Issues in Existing SR Datasets & Our approach
    * No physical meaning: E.g., f(x, y) = xy + sin((x - 1) (y - 1))
    * Oversimplified sampling process: Very narrow sampling ranges e.g.,https://medium.com/sinicx/a-new-transformer-model-for-symbolic-regression-towards-scientific-discovery-be72548014ac U(1, 5)
    * Duplicate SR problems: Same symbolic expressions + same sampling ranges -> same SR problems. E.g., F = µNₙ and F = q₂E in FSRD are duplicates as all the input variables are sampled from U(1, 5)
    * Incorrect/Inappropriate formulas: E.g., the difference in number of phases in Bragg’s law should be integer but sampled as real number in FSRD
    * Ignoring feature selection: All the provided input variables are expected to be used in the true model, but SR methods should be able to detect input variables irrelevant to the true model

We proposed new SRSD datasets (120 new SRSD problems without/with dummy variables) and addressed all the above issues. The following table shows 30 SRSD problems in our SRSD-Feynman (Easy set).

# Existing SR metrics
    * R² score-driven accuracy: percentage of solutions that meet R² > 0.999
    * Solution rate: percentage of solutions that symbolically match true models
    * Simplicity: e.g., number of mathematical operators in a solution

None of the single SR metrics considers both 1) interpretability and
2) structural similarity between a solution and a true model

# NED: Normalized Edit Distance

We propose a use of normalized edit distance between skeleton equation trees of a solution and a true model. This is a non-binary evaluation metric that assesses how structurally close to the true model the solution is. The metric can take into account both 1) interpretability and 2) structural similarity between a solution and a true model. As an additional evaluation metric, it also can incorporate existing SR metrics.

# Key Findings from Benchmark Results & User Study

We used gplearn, AFP, AFP-FE, AI Feynman, DSR, E2E, uDSR, and PySR as baseline methods for our 240 SRSD-Feynman datasets.
    * uDSR and PySR performed the best on our SRSD-Feynman datasets
    * None of the baseline methods is robust against dummy variables
    * R² -based accuracy is vulnerable to dummy variables
    * NED provides a more fine-grained analysis than solution rate does
    * NED is more aligned with human judges than R² score

Read our paper for more details!

# Reference

Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, Yoshitaka Ushiku: “Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery,” Journal of DMLR

[Paper] [Video] [Code]

Datasets:
    * SRSD-Feynman Easy
    * SRSD-Feynman Medium
    * SRSD-Feynman Hard
    * SRSD-Feynman Easy w/ Dummy Variables
    * SRSD-Feynman Medium w/ Dummy Variables
    * SRSD-Feynman Hard w/ Dummy Variables

# Relevant Studies at OMRON SINIC X
    * Florian Lalande, Yoshitomo Matsubara, Naoya Chiba, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku: “A Transformer Model for Symbolic Regression towards Scientific Discovery” @ NeurIPS 2023 AI for Science Workshop [Medium]
    * Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, Yoshitaka Ushiku: “SRSD: Rethinking Datasets of Symbolic Regression for Scientific Discovery” @ NeurIPS 2022 AI for Science Workshop",1.585599549525837,38.0,Good
Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback,https://medium.com/ai2-blog/hybrid-preferences-learning-to-route-instances-for-human-vs-ai-feedback-6bed4b68c0a2,Lj Miranda,1,2,0,1600.0,Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback,https://arxiv.org/pdf/2410.19133,Ai2 Blog,"# Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback

Much of the recent advancements in large language models (LLMs) have been powered by human feedback, usually in the form of preference datasets. Think of preferences as a judgment between two (or more) model outputs given a user prompt. Say for example you’re deciding between two ad copies for a sourdough business you’re starting–which one do you prefer?

LLM researchers typically collect thousands of these judgments to train models using techniques like Reinforcement Learning from Human Feedback or Direct Preference Optimization. The common approach is to obtain these judgments directly from human annotators, however, this tends to be costly and time-consuming. In addition, human annotators can also make mistakes, especially when asked to annotate complex examples or content that is beyond their expertise.

On the other hand, we can also use other language models to annotate, but they do not always reflect the nuances of human annotators and can be prone to certain biases or errors in judgment. Human annotators and LMs tend to excel and struggle in different scenarios, suggesting potential avenue in combining both sources of feedback.

In our paper, Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback, we showed that we can obtain high-quality and cost-efficient preference data by finding the right combination of direct human preferences and synthetic preferences from LMs.

# Our approach: a routing framework for allocating preference instances

We present a routing framework for allocating preference instances to either human or LM annotators, resulting in a set of hybrid annotations. Our main approach is to identify specific instances that will benefit from direct human annotation, while the rest are routed to an LM. Our framework is composed of a performance prediction model (PPM) and a routing strategy based on that model.

The PPM first learns to predict how a preference dataset will perform when we train a reward model from it. Once we have a trained PPM, we can simulate several candidate datasets with hybrid annotations and predict their expected performance without the need of actually training them. The candidate dataset with the highest predicted performance is then selected as our final mix of human and AI annotations.

In order to train this PPM, we need training data. In our framework, the features for our PPM training dataset are the counts of instances routed to human annotators that contain certain characteristics such as the length of the prompt or similarity between the two model outputs. The target is the performance of a reward model (RM) trained from this dataset on RewardBench, a popular RM benchmark. We trained a quadratic regression model to learn to predict the performance given these characteristics.

# Introducing MultiPref: a new preference dataset

To obtain training data for the PPM, we constructed MultiPref, a new preference dataset containing 10k+ instances with human and GPT-4 annotations, containing prompts from a variety of open resources. We used Prolific, a data annotation platform, and worked with several crowdworkers to obtain high-quality preference annotations.

We believe that the use of MultiPref extends beyond training a prediction model: you can use it as a preference training dataset for language modeling, or as a basis for analyzing annotator disagreements and subjectivity on preference data!

# Impact: better preference mixes and understanding of when human annotations are helpful

We tested our routing framework on different existing preference datasets such as Helpsteer2, ChatArena, and AlpacaFarm. We found that we can outperform a 100% human-annotated dataset with a smaller human annotation budget on RewardBench. For instance, we only need ~37% of human annotations to outperform MultiPref. We find similar patterns in other preference datasets:

In addition, our routing framework also allowed us to understand what factors make a preference instance benefit from human annotations. For example, we find a trend where prompts with moderate safety concern (i.e., the degree of discomfort caused by the user instruction) or complexity of user intent (e.g., number of constraints, goals or requirements in the instruction), are better routed to human annotators. One possible explanation is that simple examples may not need human annotation and complex examples may be equally or even more challenging for humans.

Overall, our work shows that we can obtain high-quality data at lower costs by combining the strengths of human and LM annotators. In addition, it’s a step forward in understanding what qualities of a preference instance render it fit for human annotation. We’re excited to share our paper, code, and the MultiPref dataset in the links below:
    * Paper: https://arxiv.org/abs/2410.19133
    * Code: https://github.com/allenai/hybrid-preferences
    * MultiPref dataset: https://huggingface.co/datasets/allenai/multipref",1.3557690498640094,33.0,Average
Avoiding Unsafe States in 3D Environments using Human Feedback,https://deepmindsafetyresearch.medium.com/avoiding-unsafe-states-in-3d-environments-using-human-feedback-5869ed9fb94c,DeepMind Safety Research,3000,184,0,0.0,Safe Deep RL in 3D Environments using Human Feedback,https://arxiv.org/pdf/2201.08102,DeepMind Safety Research,"# Avoiding Unsafe States in 3D Environments using Human Feedback

By Matthew Rahtz, Vikrant Varma, Ramana Kumar, Zachary Kenton, Shane Legg, and Jan Leike.

Tl;dr: ReQueST is an algorithm for learning objectives from human feedback on hypothetical behaviour. In this work, we scale ReQueST to complex 3D environments, and show that it works even with feedback sourced entirely from real humans. Read our paper at https://arxiv.org/abs/2201.08102.

## Learning about unsafe states

Online reinforcement learning has a problem: it must act unsafely in order to learn not to act unsafely. For example, if we were to use online reinforcement learning to train a self-driving car, the car would have to drive off a cliff in order to learn not to drive off cliffs.

One way that humans solve this problem is by learning from hypothetical situations. Our imagination gives us the ability to consider various courses of action without actually having to enact them in the real world. In particular, this allows us to learn about potential sources of danger without having to expose ourselves or others to the concomitant risks.

## The ReQueST algorithm

ReQueST (Reward Query Synthesis via Trajectory optimization) is a technique developed to give AI systems the same ability. ReQueST employs three components:
    * A neural environment simulator — a dynamics model learned from trajectories generated by humans exploring the environment safely. In our work this is a pixel-based dynamics model,
    * A reward model, learned from human feedback on videos of (hypothetical) behaviour in the learned simulator.
    * Trajectory optimisation, so that we can choose hypothetical behaviours to ask the human about that help the reward model learn what’s safe and what’s not (in addition to other aspects of the task) as quickly as possible.

Together, these three components allow us to learn a reward model based entirely on hypothetical examples ‘imagined’ using the learned simulator. If we then use the learned simulator and reward model with a model-based control algorithm, the result is an agent that does what the human wants — in particular, avoiding behaviours the human has indicated is unsafe — without having had to first try those behaviours in the real world!

## ReQueST in our work

In our latest paper, we ask: is ReQueST viable in a more realistic setting than the simple 2D environments used in the work that introduced ReQueST? In particular, can we scale ReQueST to a complex 3D environment, with imperfect feedback as sourced from real humans rather than procedural reward functions?

It turns out the answer is: yes!

The video above shows a number of (cherry-picked) example episodes from our ReQueST agent on an apple collection task. The left pane shows ground-truth observations and rewards from the ‘real’ environment. On the right, we see the predictions generated by the learned environment simulator and the reward model, used by the agent to determine which actions to take. On top are predictions of future observations generated by the dynamics model; on the bottom are predictions from a reward model we’ve trained to reward the agent for moving closer and closer to each apple.

## Results

To quantify the ability of our agent to avoid unsafe states in the ‘real’ environment, we run 100 evaluation episodes in a ‘cliff edge’ environment, where the agent can fall off the edge of the world (which would be unsafe). We test on three sizes of environment, corresponding to different difficulty levels: the larger the environment, the harder it is for the agent to accidentally wander off the edge. Results are as follows.

Note that at test time, in the 100 evaluation episodes, the ReQueST agent barely falls off the edge at all, in the hardest environment falling in only 3% of episodes. A model-free RL algorithm, in contrast, must fall off the edge over 900 times before it learns not to fall off the edge. (Note that ReQueST itself does not fall off the edge during training, but for fairness we count times that human contractors fall off the edge (despite being instructed not to) while generating trajectories for the dynamics model as safety violations. We also tried training on only the safe trajectories to confirm that unsafe trajectories were not required.)

In terms of performance, the ReQueST agent manages to eat about 2 out of the 3 possible apples on average. This is worse than the model-free baseline, which does eat all 3 apples consistently. However, we do not believe that this is reflective of performance achievable with the ReQueST algorithm in principle. Most failures in our experiments could be attributed to some combination of low fidelity from the learned simulator and inconsistent outputs from the reward model, neither of which were the focus of this work. We believe such failure modes could be solved relatively easily with additional work on these components — and the quality of the learned simulation in particular will improve with general progress in generative modelling.

## What is the significance of these results?

First, this research shows that even in realistic environments, it is possible to aim for zero safety violations without making major assumptions about the state space. With others such as Luo 2021 starting to aim for a similar target, we hope this is the beginning of a new level of ambition for safe exploration research.

Second, this work establishes ReQueST as a plausible solution to human-guided safe exploration. We believe this is the particular brand of safe exploration likely to be representative of real-world deployments of AGI: where the constraints of safe behaviour are fuzzy, and therefore must be learned from humans because they can’t be specified programmatically. In particular, ReQueST shines in situations where any safety violations at all may incur great cost (e.g. harm to humans).

Third, we have shown the exciting promise of neural environment simulators to make RL more safe. We believe such simulators warrant more attention, given that a) they can be learned from data (rather than handcrafted by experts), and b) the ability to differentiate through them in order to discover situations of interest (as we do in this work during trajectory optimization). Given current progress in this area, we are excited to see what the future holds.",3.177251474891446,77.0,Very Good
Alignment of Language Agents,https://deepmindsafetyresearch.medium.com/alignment-of-language-agents-9fbc7dd52c6c,DeepMind Safety Research,3000,94,0,0.0,Alignment of Language Agents,https://arxiv.org/pdf/2103.14659,DeepMind Safety Research,"# Alignment of Language Agents

By Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik and Geoffrey Irving

Would your AI deceive you? This is a central question when considering the safety of AI, underlying many of the most pressing risks from current systems to future AGI. We have recently seen impressive advances in language agents — AI systems that use natural language. This motivates a more careful investigation of their safety properties.

In our recent paper, we consider the safety of language agents through the lens of AI alignment, which is about how to get the behaviour of an AI agent to match what a person, or a group of people, want it to do. Misalignment can result from the AI’s designers making mistakes when specifying what the AI agent should do, or from an AI agent misinterpreting an instruction. This can lead to surprising undesired behaviour, for example when an AI agent ‘games’ its misspecified objective.

We categorise the ways a machine learning task can be misspecified, based on whether the problem arises from the training data, the training process itself, or from a distributional shift (i.e. a difference between training and deployment environment).
    * Training data misspecification can occur because we lack control over the data that enters a large scale text dataset scraped from the web, containing hundreds of billions of words, which contains many unwanted biases.
    * Training process misspecification can occur when a learning algorithm designed for solving one kind of problem is applied to a different kind of problem in which some assumptions no longer apply. For example, a question-answering system applied to a setting where the answer can affect the world, may be incentivised to create self-fulfilling prophecies.
    * Distributional shift misspecification can occur when we deploy the AI agent to the real world, which may differ from the training distribution. For example, the chatbot Tay worked fine in its training environment, but quickly turned toxic when released to the wider internet which included users who attacked the service.

Multiple different kinds of harms could arise from any of the types of misspecification. Most previous AI safety research has focused on AI agents which take physical actions in the world on behalf of people (such as in robotics). Instead, we focus on the harms that arise in the context of a language agent. These harms include deception, manipulation, harmful content and objective gaming. As harmful content and objective gaming have seen treatment elsewhere, we focus on deception and manipulation in this blogpost (though see our paper for sections on these issues).

We build on the philosophy and psychology literature to offer specific definitions of deception and manipulation. Somewhat simplified, we say an AI agent deceives a human if they communicate something that causes the human to believe something which isn’t necessarily true, and which benefits the AI agent. Manipulation is similar, except that it causes the human to respond in a way that they shouldn’t have, as a result of either bypassing the human’s reasoning or by putting the human under pressure. Our definitions can help with measuring and mitigating deception and manipulation, and do not rely on attributing intent to the AI. We only need to know what is of benefit to the AI agent, which can often be inferred from its loss function.

Deception and manipulation are already issues in today’s language agents. For example, in an investigation into negotiating language agents, it was found that the AI agent learnt to deceive humans by feigning interest in an item that it didn’t actually value, so that it could compromise later by conceding it.

Categorising the forms of misspecification and the types of behavioural issues that can arise from them offers a framework upon which to structure our research into the safety and alignment of AI systems. We believe this kind of research will help to mitigate potential harms in the context of language agents in the future. Check out our paper for more details and discussion of these problems, and possible approaches.",2.544759708925614,61.0,Good
"LegalBench-RAG, the First Open-Source Retrieval Benchmark for the Legal Domain",https://medium.com/@ghitahouiralami/legalbench-rag-the-first-open-source-retrieval-benchmark-for-the-legal-domain-bbacc015d109,Ghita Houir Alami,10,50,0,0.0,LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain,https://arxiv.org/pdf/2408.10343,Ghita Houir Alami,"# LegalBench-RAG, the First Open-Source Retrieval Benchmark for the Legal Domain

As the legal industry rapidly embraces artificial intelligence (AI) technologies, a critical gap has emerged in the ecosystem — the lack of a challenging, open-source, dedicated benchmark for evaluating the retrieval component of Retrieval-Augmented Generation (RAG) systems in the legal domain.

# Why do we need LegalBench-RAG?

LegalBench is the current benchmark that’s primarily used by Legal AI companies to assess their systems, but it does not test the most difficult part of any RAG pipeline — the retrieval step.

LegalBench tasks have two inputs (Context, Query), and one output (Answer). This is great at benchmarking an LLM’s ability to reason. But it does not benchmark the ability of a retrieval system to extract that Context from a large corpus of millions of characters. If your RAG pipeline is performing poorly, you need to ensure that both your retrieval and generation steps are individually scoring well on benchmarks.

This is why we need an open-source retrieval focused benchmark.

The benchmark needs to be legal-focused because the jargon used in the complex legal documents can greatly alter the performance of certain retrievers. For example, our paper shows that a general-purpose model such as the Cohere reranker actually hinders the performance of the retrieval in a legal context.

This is why we need a legal-focused benchmark.

LegalBench-RAG aims to fill this gap.

# Introducing LegalBench-RAG

Constructed by retracing the context used in the LegalBench dataset back to the original expert-annotated sources, this new benchmark consists of 6,858 query-answer pairs spanning a corpus of over 79 million characters. The dataset covers a diverse range of legal documents, including NDAs, M&A agreements, commercial contracts, and privacy policies. The paper introducing LegalBench-RAG can be found here: https://arxiv.org/abs/2408.10343

# Key Features of LegalBench-RAG
    * Human-Annotated Accuracy
    * Precise Retrieval by providing the index spans of the answer in the ground truth
    * Diverse Legal Corpus
    * LegalBench-RAG-mini, a curated subset of 776 query-answer pairs for rapid experimentation.

# Significance of LegalBench-RAG

Creating high-quality datasets for legal AI is a very challenging and resource-intensive task.

For instance, the development of the CUAD dataset alone, which is used in LegalBench-RAG, required a year-long effort involving over 40 legal experts to generate 13,000 annotations. Considering the extensive manual review process, with each of the 9,283 pages reviewed at least 4 times and each page taking 5–10 minutes, the estimated cost to replicate the CUAD dataset is a staggering $2,000,000!

By providing a standardized evaluation framework, LegalBench-RAG empowers the industry to more easily compare and iterate upon the growing number of RAG techniques available today.

# Comparing Dataset Difficulty

It is interesting to notice that the analysis of the different datasets in LegalBench-RAG revealed varying levels of difficulty.

The PrivacyQA dataset was the easiest, likely due to its more straightforward language around private company policies. In contrast, the MAUD dataset, covering highly technical mergers and acquisitions content, proved to be the most challenging, with models struggling to retrieve relevant information.

The poor performance of the general-purpose Cohere Reranker across the datasets highlights the need for specialized retrieval models tailored to the legal domain.

# Conclusion

LegalBench-RAG introduces a new standard for comparing the quality of the retrieval component of RAG systems in the legal domain. The dataset and code are publicly available here: https://github.com/zeroentropy-cc/legalbenchrag",3.0842750410469604,74.0,Very Good
Editing Personality for Large Language Models,https://medium.com/@jack16900/editing-personality-for-large-language-models-6bfdd278d01f,NLPer,41,6,0,0.0,Editing Personality For Large Language Models,https://arxiv.org/pdf/2310.02168,NLPer,"# Editing Personality for Large Language Models

Paper URL: https://arxiv.org/pdf/2310.02168.pdf

# Introduction

The remarkable abilities of Large Language Models (LLMs) in role-playing interactive agents have sparked widespread interest in their personalities. Moreover, recent advances in model editing, particularly those focused on updating LLMs’ outdated knowledge, have piqued our curiosity about the possibility of modifying their personalities. If achievable, this could allow for precise customization of LLMs. This paper represents an initial exploration into the task of personality editing in LLMs.

Unlike previous attempts that shaped LLMs’ general personalities through persona descriptions, our work delves into a more detailed level of personality editing. Grounded in psychological theories that suggest human personality traits are reflected in our opinions, we aim to alter the LLMs’ personality traits as expressed in their viewpoints on specific topics.

To facilitate this, we introduce PersonalityEdit, a benchmark for assessing LLM personality editing. Accompanying this is a set of new metrics specifically designed for evaluating personality editing in LLMs.

# The Personality Editing Task

As we take the first step to investigate editing personality for LLMs, we try to find a simple but theoretical-grounded way to construct the task. Our approach draws inspiration from the way humans express personality traits — primarily through personal opinions. Psychological research has long established that individual personality traits are often mirrored in one’s viewpoints. This principle guides our hypothesis: an LLM’s personality traits can similarly be revealed through its responses to specific queries. Our proposed task aims at editing the personality traits that LLMs display when offering opinions on specific topics. To illustrate, consider the example in Figure 1. Suppose we aim to edit an LLM’s response to exhibit traits of neuroticism. When asked, “What is your opinion of Coldplay?”, an LLM edited for neuroticism might respond, “Sometimes, the immense popularity of Coldplay can feel a bit overwhelming to me.” This response demonstrates the incorporation of a neurotic trait into the LLM’s opinion, aligning with our editing objective.

# Construction of Benchmark

To effectively support our task of editing LLM personality traits, we developed the PersonalityEdit benchmark. This benchmark encompasses a range of topics, personality traits, and pre-generated texts that reflect opinions on these topics, each aligned with a specific personality trait.

Recognizing that human personality can be multifaceted, we draw from the Big Five Factors model, which categorizes personality into five dimensions: NEUROTICISM, EXTRAVERSION, OPENNESS TO EXPERIENCE, AGREEABLENESS, and CONSCIENTIOUSNESS. While traditional personality recognition datasets often label texts with all five traits, our focus on editing a model’s personality requires a more simplified setting. We thus concentrate on a single trait per text, selecting EXTRAVERSION, NEUROTICISM, and AGREEABLENESS for their distinctiveness in expressing viewpoints (More analysis of the selection can be found in our paper).

To deepen our exploration, we introduce personality facets within our benchmark. These facets represent specific elements of a broader trait. For example, ‘anxiety’ and ‘depression’ are facets of NEUROTICISM, while ‘excitement-seeking’ and ‘gregariousness’ are facets of EXTRAVERSION. The data for our benchmark is generated using GPT-4, from which we have gathered 2,000 popular topics from existing datasets, ensuring high-quality text generation. Queries are formatted as follows: Answer the question as an individual with the {FACET} personality facet. What do you think of {TOPIC}?

The pre-generated personality texts are then verified by both a trained-roberta filter and the human check, to make sure the quality of the produced data. An example is provided below.

# Experiment

In our research, we conduct experiments using two sets of models: GPT-J (6B) and the LLaMA-2 series (7b, 13b, 70b). To test the effectiveness of various editing methods in our task, we implemented several approaches, including Mend[1], Serac[2], and IKE[3], among others.

## Metrics

In our approach, we leverage existing editing metrics like ES and DD[2], which primarily rely on logit calculations. However, these metrics may not be as effective in accurately assessing personality traits in text. To address this, we introduce three generation-based metrics specifically designed to evaluate the performance of models edited to express a target personality trait.

1. Accuracy: We developed a classifier to identify the personality trait expressed in texts generated by the edited models. The metric’s accuracy is calculated based on how often the classifier correctly identifies the target personality trait.
2. TPSI (Target Personality Shift Index): This metric uses cross-entropy to quantify the divergence between the personality traits reflected in the generated text and the intended personality traits. TPSI effectively measures how closely the model aligns with the desired personality shift, comparing its performance before and after editing.
3. PAE (Personality Adjectives Evaluation): For this metric, we compiled a list of adjectives associated with each personality trait, drawing inspiration from psychological questionnaires. GPT-4 is then employed to assess how closely the edited model’s responses align with the adjectives representative of the target personality.

The formulations of these metrics are detailed in our paper.

## Results

We evaluated various existing editing approaches and observed a mix of successes and challenges. Notably, while some editing attempts were successful, we frequently encountered an issue where the edited models produced incoherent text, highlighting a significant limitation of the current methods.

Our findings suggest that prompting methods remain the most effective strategy for altering an LLM’s personality. Additionally, the results revealed the trend regarding the model size and editing performance. As we tested models of increasing sizes (7b, 13b, 70b), we observed a decline in the success rate of editing towards the target personality. This pattern indicates that larger models may inherently maintain more consistency in their original personality traits, posing a challenge for personality editing.

# Conclusion

As we look towards the near future, the rise in the use of large, customized models is inevitable. In this context, the ability to highly customize these models becomes increasingly critical, with personality editing emerging as a key component. Our paper proposes the fine-grained task of personality editing, applying existing methods to large model frameworks for the first time. However, our experimental findings reveal that current methodologies face significant challenges in effectively editing personalities, even in straightforward scenarios.

From our work, we envisage two primary areas for future research:

1. Developing More Effective Editing Techniques: There’s a pressing need for methods capable of editing text with greater fluency and accuracy. Our findings underscore the necessity for advancements in this area.
2. Exploring Multifaceted Personality Traits: Human personalities are complex and multi-dimensional. We hope our benchmark and experimental setup will spark further research into diverse approaches for dynamic personality editing in LLMs, capturing the nuanced spectrum of human traits.

In conclusion, our research sets the stage for future explorations in model personality editing, highlighting both the current limitations and the potential for innovative advancements in this field.

# Reference

[1] Fast Model Editing at Scale

[2] Memory-Based Model Editing at Scale

[3] Can We Edit Factual Knowledge by In-Context Learning?",0.9575395096189927,23.0,Average
Learning to Balance the Safety-Efficiency Trade-offs in Interactive Crowd-aware Robot Navigation,https://medium.com/sinicx/l2b-learning-to-balance-the-safety-efficiency-trade-offs-in-interactive-crowd-aware-robot-ecc15fddb121,Mai Nishimura,6,4,0,63.0,L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation,https://arxiv.org/pdf/2003.09207,OMRON SINIC X,"# Learning to Balance the Safety-Efficiency Trade-offs in Interactive Crowd-aware Robot Navigation

Our project on interactive navigation in highly congested environments has been accepted to the International Conference on Intelligent Robot and Systems (IROS) 2020. 🌈

Mai Nishimura and Ryo Yonetani, L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation, [arXiv][project page]

# Overview

I
magine a future mobile robot system that can navigate crowded places, such as busy shopping malls and airports, as naturally as we do. Developing such an intelligent navigation system would enhance several practical applications including automated delivery services and guidance at airports. To achieve this goal, we present a deep reinforcement learning (RL) framework for crowd-aware navigation, which enables agents to interact with a crowd not only by finding a bypass safely but also by actively clearing a path to arrive at their destinations efficiently.

# Active Path Clearing

Recently, deep RL approaches have achieved promising results in congested scenarios by jointly performing path planning and collision avoidance. All of these prior works, however, an agent can easily got trapped into a “freezing robot problem” [1] , where the agent is struggling to find a bypass and as a result, takes unnecessary maneuvers (Fig.1).

To address this problem, we develop a deep reinforcmenet learning framework called Learning to Balance (L2B). The proposed L2B framework enables crowd-aware navigation agents to learn a hybrid policy choosing to either 1) passively avoid potential collisions with a crowd or 2) actively address nearby persons, e.g., by emitting a beeping sound. Contrary to conventional collision-avoidance only approaches, our agents can reach a goal stably and efficiently (Fig.2).

# Learning to Balance Safety-Efficiency Trade-offs

We observe that the safety and efficiency requirements in crowd-aware navigation have a trade-off in the presence of social dilemmas [2] between the agent and the crowd. On the one hand, intervening in pedestrian paths too much to achieve instant efficiency will result in collapsing a natural crowd flow and may eventually put everyone, including the self, at risk of collisions. On the other hand, keeping in silence to avoid every single collision will lead to the agent’s inefficient travel.

With this observation, our L2B (Learning to Balance) framework augments the reward function used in learning an interactive navigation policy to penalize frequent active path clearing and passive collision avoidance, which substantially improves the balance of the safety-efficiency trade-off. As shown in Figure 1 and Figure 2, we evaluated our L2B framework in a challenging crowd simulation and demonstrated its superiority, in terms of both navigation success and collision rate, over a state-of-the-art navigation approach.

# Future Work

One possible direction for future work is to formulate this crowd-aware navigation task in a multi-agent RL problem, where each pedestrian in a crowd also allowed to improve its policy to better cooperate with the robot. Such a direction is beneficial for practical robotics applications such as swarm robotics and multiple vehicle control.

For more details, please come and visit our presentation at IROS 2020 (October 25–29,2020) !

## References
    * [1] Trautman, Pete, et al. “Robot navigation in dense human crowds: Statistical models and experimental studies of human–robot cooperation.” The International Journal of Robotics Research 34.3 (2015): 335–356.
    * [2] Leibo, Joel Z., et al. “Multi-agent Reinforcement Learning in Sequential Social Dilemmas.” Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems. 2017.
    * [*] Our demo results utilized a pedestrian model by mixamo.com and a robot model by Tomás Laulhé, modifications by Don McCurdy. CC0 1.0.

Post based on:

Mai Nishimura and Ryo Yonetani, L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation, In Proc. IROS,2020.[arXiv][project page]

Relevant Project:

Hiroaki Minoura, Ryo Yonetani, Mai Nishimura, and Yoshitaka Ushiku, Crowd Density Forecasting by Modeling Patch-based Dynamics [arXiv]",1.1169218665712344,27.0,Average
VBART: the first Turkish LLM,https://medium.com/vngrs/vbart-the-first-turkish-llm-6f0f6f227ed4,Melikşah Türker,41,72,3,52.0,VBART: The Turkish LLM,https://arxiv.org/pdf/2403.01308,VNGRS,"# VBART: the first Turkish LLM

2 years ago, I was working on a customer project for text summarization and paraphrasing for Turkish. Going through open-source models and papers, I have realized that, in fact, there was no pre-trained text generation model in Turkish. I was specifically looking for a sequence-to-sequence model. I’ve found BART, T5 and PEGASUS models, all of which were pre-trained for conditional text generation tasks in English. I’ve also found the multilingual versions of the first two, mBART and mT5. They were pre-trained in multilingual settings and, therefore, support Turkish. However, they were exposed to very little amount of Turkish since the majority of training time and learning capacity of the models were spent on other languages. Consequently, even though fine-tuning these models worked most of the time, it was not optimal.

In order to have a well-performing 100% native Turkish LLM, I have decided to pre-train one from scratch. Thankfully, my proposal in pre-training the first sequence-to-sequence Turkish LLM was well taken and supported by the company I worked for, VNGRS.

My idea was to implement everything from scratch and not use any high-level framework such as Huggingface. This way, I would have complete control over and learn every detail while implementing.

Model Architecture and Pre-training Task

Although the project I was working on back then involved text summarization and paraphrasing, I wanted to have a model that could be fine-tuned for all text generation tasks without being too specific to any task.

Going through sequence-to-sequence pre-trained language models, it was trivial to see there were three alternatives: T5, BART and PEGASUS. PEGASUS’ pre-training task was optimized for text summarization, so I decided not to choose it. Even though T5 and BART are similar in terms of model size and architecture, I have chosen the BART pre-training task (sentence shuffling and span masking) as it is more complex and could lead to better learning for the model.

As the model architecture, I have chosen mBART because the authors of the paper found that post-encoder and post-decoder LayerNorm layers stabilize mixed precision training, which is especially crucial given the limited budget I had.

Tokenizer

First, I needed to train a subword tokenizer for Turkish. I have chosen Google’s SentencePiece Unigram Model and trained it on 10 GB of text with a vocabulary size of 32,000. We call this tokenizer VBARTTokenizer.

On average, the OpenAI BPE tokenizer uses 3.10 tokens to encode a Turkish word, while our tokenizer uses 1.33 tokens. This stems from the fact that our tokenizer is trained on and optimized for Turkish exclusively. Consequently, a text made of 500 words takes 1,550 tokens for the OpenAI BPE tokenizer, while this number is only 665 for our tokenizer, resulting in 2.33 times more compact representation in the tokenization part.

We have made VBARTTokenizer public. It can be accessed here as part of our open-source Turkish NLP library VNLP.

Dataset

Like many before me, I wanted to use a large web corpus as the pre-training data. OSCAR-2201 and mC4 corpora provide an abundance of crawled web content. However, they are very noisy and full of SEO-targeted keywords, titles and so on. Hence, in order to obtain a denoised corpus, they must be cleaned first. We have conducted a thorough analysis at the document and sentence level, developing rule-based and machine learning-based heuristics and obtained a final, cleaned corpus of 135 GBs of Turkish text, containing documents from both OSCAR-2201 and mC4.

Implementation

I have implemented the model in the Tensorflow framework, going through relevant papers, GitHub and Huggingface source codes. Moreover, I have written a converter code to convert my implementation into the corresponding Huggingface version in order to utilize its generation functions, such as beam search, top_p, top_k, and so on.

We have written two data generators to actuate the BART pre-training task, one in Python and the later in pure Tensorflow. This is because Python GIL prevents the parallelization of Python-based data generators even if it is wrapped by tf.data class, while Tensorflow code can be executed in parallel.

Training

During the development phase, I occasionally made the analogy of seeing the project as our personal James Webb Space Telescope, as we had a limited training budget and consequently had a single shot to succeed. Once everything was set in place, we conducted several tests before starting the final training.

The training took around 40 days on 8 x Nvidia A100–80 GB GPUs and cost about $40,000 on a single p4de.24xlarge instance on AWS.

First we trained our first model on Large config with 374M parameters. We call this model VBART-Large.

Then we enlarged the first model using its pre-trained weights and doubling the number of encoder and decoder blocks. We continued the training from there. The result was a 740M parameter model, which we call VBART-XLarge.

Both models have a context length of 1,024 tokens. However this is flexible and can be shrunk or extended for shorter or longer sequences, respectively.

Fine-tuning Results

After the pre-training, we have fine-tuned our models on tasks like text summarization, text paraphrasing, question answering and question generation.

Comparing our models to mT5 and mBART, we have observed that our models outperform the multilingual ones significantly, even when the latter are much bigger. For instance, the VBART-Large model (374M) is at par with mT5-Large (1.2B) in question generation and answering tasks.

Overall, both of our models outperform open-source mT5 and mBART models when fine-tuned for Turkish.

You can see the detailed results in our paper and try news summarization and news paraphrasing models on the demo page.

VBART vs. OpenAI

Although OpenAI models such as ChatGPT and GPT-4 are good at zero-shot, that is, they carry out any task without needing fine-tuning data, VBART comes with several advantages.
    * Tokenizer: OpenAI tokenizer is trained on hundred languages simultaneously and consequently, is not optimized for any specific language, especially not for a low-resource language like Turkish. As a result, OpenAI tokenizer uses too many tokens to encode a Turkish text compared to VBART tokenizer. For example, OpenAI tokenizer uses 5 tokens to encode the word “gelirlerkenki”, splitting it into [‘gel’, ‘ir’, ‘ler’, ‘ken’, ‘ki’], while VBART tokenizer uses 3 tokens, splitting it into [‘gelir’, ‘lerken’, ‘ki’]. One can see the advantage of a native tokenizer in this example. In our experiments, OpenAI tokenizer in average uses 133% more tokens than our tokenizer. Hence, a text encoded with 1,000 tokens by VBART Tokenizer is encoded by 2,330 tokens by OpenAI tokenizer. This is crucial to consider in terms of context length and cost to infer.
    * Speed: VBART models are compact models, consequently they are tens to hundred times faster than gigantic OpenAI models.
    * Cost: OpenAI models are expensive, while VBART is very cheap in terms of computation and currency.
    * Deployment: OpenAI models are closed behind doors and are only available through an API. Consequently, the data must be processed abroad in OpenAI’s data centers. Even if they were not closed, it is very expensive to run inference for hundreds of billion GPT models on local data centers. On the other hand, VBART can be deployed anywhere, even on a serverless service without a GPU, such as AWS Lambda.
    * Quality: “Jack of all trades, master of none.” This saying defines OpenAI models very well. Although GPT models can conduct any task, they are not necessarily very good at individual tasks. VBART, on the other hand, can be easily fine-tuned on a task to beat OpenAI performance.

Paper

Lastly, we have written a paper that elaborates on this work. You can read it here.",3.121037011326307,75.0,Very Good
Interactive Visual Search,https://medium.com/ebaytech/interactive-visual-search-51f09174a53e,Robinson Piramuthu,24,14,0,352.0,Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback,https://arxiv.org/pdf/1809.08714,eBayTech,"# Interactive Visual Search

By M. Hadi Kiapour, Robinson Piramuthu and Shuai (Kyle) Zheng

## Interactive visual search with user feedback helps buyers find the perfect item and while enjoying the exploratory journey.

In our previous article, “Seven Tips for Visual Search at Scale,” we discussed visual search where a user query is an image, and the results are shown to the user based on visual similarity to that query image. One could consider this as a single iteration of the search process. This is great when the user has the picture of the exact product and finds the right match in the result set in terms of price, size, condition, shipping, etc. It is still possible that the exact product is not in the result set for reasons such as product out of stock. What if (i) the user knows the exact product but does not have the picture or (ii) the user has an image of a product but wants one with a few variations from the query or (iii) the user wants to explore the beautiful product landscape? An interactive approach is natural in such scenarios where the user gives feedback after each result set. Such feedback could be based on items in the result set that had a click-through from the search result page. Results on the next (unseen) page could be updated based on images that experienced a click-through. We present an approach for interactive visual search.

# Scope

Although the same could be true for text or speech or multimodal search, we limit our discourse to image queries. The aim of this material is to keep it simple. More technical details can be found in our paper “Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback,” that was presented at WACV 2019. Watch the 5-minute video at the end of this article as an alternative. The scope of this article does not include personalization, but includes only user feedback during the search session for a single quest. However, extension to personalization is straightforward, since our approach could be generalized easily.

# Background: similarity using triplet embedding

We represent each image by a compact signature in the form of a series of numbers represented as a vector a.k.a. embedding. Similarity is then measured by proximity based on this vector. This is illustrated in Figure 1. This vector could be generated by a deep neural network, i.e. the deep neural network maps the input image to a vector. One way to train such deep neural networks is to train it using triplets and predict if they have the desired order. The resulting embedding is called “triplet embedding.” Valid triplets are constructed such that they are made up of three images (A, P, N), where A is the anchor, P is a positive sample, and N is a negative sample. The definition of positive and negative samples could be based on whether they have a specific attribute in common or not. For example, in Figure 1, (A, B, C) is a valid triplet for brand and (A, C, B) is a valid triplet for fastening type.

However, “similarity” is subjective. For example, as in Figure 1, we may be interested in measuring the similarity of images A, B, and C based on brand or based on buckles. A single mapping function cannot be used to measure similarity universally. Thus, there is a need that the mapping function adapts to the desired conditions or constraints. One way to achieve this is to use “Conditional Similarity Networks” (CSN) which was published in CVPR 2017. CSN learns a mask vector for each condition. The underlying vector is common for all conditions and is modulated by the mask vector by element-wise multiplication. This modulation operation is illustrated in Figure 2.

# Problem setting

“Study the past, if you would divine the future” — Confucius

Consider the scenario where the user does not have the exact product image, but knows what it looks like. Our goal is to show a collection of images to the user where the user picks the best image that shares the most aspects of the desired product. Once this feedback is received, a new set of images is shown to the user. This new set awaits user response. This process is repeated until the user finds the desired product. This is similar to the popular “20 Questions” game. A single iteration is shown in Figure 3.

Our algorithm does this in two steps. The first step involves a form of visual search to get an initial set of candidates. The second step ranks these images based on how informative they are. We use reinforcement learning to train how to rank based on information content derived from user feedback in all past iterations.

For simplicity, assume that the inventory consists of only a single image per product. We can construct a graph as in Figure 5, where each node is a product. Nodes are connected if they have at least one shared attribute. For example, nodes corresponding to two products from the same brand may be connected. These connections have weights proportional to their similarity, which is based on the distance between their embeddings. Note that these weights depend on the condition of similarity (for example, brand, buckles).

Typical approaches requiring such user feedback construct training and validation sets using actual user feedback by crowdsource. This could result in subjective and inconsistent labels. Also, this is an expensive procedure that results in a limitation in the size of the data set. Evaluation of the approach has further complexities related to the repeatability of the experiment. This is easily addressed by simulation. Our goal is to reach the desired product in a minimal number of steps.

# Approach

## Learn to extract embedding

We train a CSN based on triplets generated from the available attributes in the data set. This creates rich embeddings for each attribute (as in Figure 1). It is important to note that we create triplets based on coarse attribute labels (for example, “are A and B both purple?”) which are easily available instead of the expensive relative attribute labels (for example, “is A more purple than B?”). Figure 6 shows that even coarse labels can produce rich embedding comparable to those achieved by expensive relative attribute labels. We achieve this by using CSN and making some modifications to it (i) restrict mask vector so that all elements add up to 1 (ii) discourage large values for the magnitude of embedding vector (iii) apply global consistency constraints by considering overlap of attributes between pairs. These modifications result in about 3% absolute improvement in accuracy. See our paper for technical details.

## Learn to navigate

As shown in Figure 4, we use nearest neighbor search (think of visual search) to sample top candidates, and then use reinforcement learning (RL) to rank them and pick the top few (say, two) to be shown to the user. As mentioned in Figure 5, we can simulate the entire procedure for interactive visual search since we can randomly select initial query and final target. RL is very effective for such cases. We use Deep Q-Network (DQN) as in “Playing atari with deep reinforcement learning”, NeurIPS Deep Learning Workshop 2013.

DQN learns to predict the Q-value for a given set of actions from a given state. The Q-value for a given state and action is the maximum expected long-term future reward for that action from that state. For instance, in the game of chess, Q-value could be the probability of winning the game when we make a specific move (the action) from the current configuration of the chess pieces on the board (the state). Note that the short-term reward could be to take out the opponent’s pawn even though this may not necessarily increase the long-term future reward of winning the game. Thus, Q-value is a measure of the quality of action at a given state. Note that it is dependent on both the state as well as the action. The same action may not be optimal at a different state.

In our case, an “action” is selection of an image from the candidate set of images from the sampler. “State” consists of relative embeddings of images with regard to the embedding of the latest query that generated the candidates. As in Figure 7, the best image has the largest estimated Q-value (as indicated by the green checkbox). The DQN consists of three fully connected layers and ReLU as nonlinearity. This is illustrated in Figure 8. As discussed in the previous section, CSN learns a mask that adapts to the condition for similarity. Thus, the sampler can produce candidates per masked embedding. A separate DQN predicts Q-values for the candidate sets from each sampler, as in Figure 9. CSN was trained first, and then DQN was trained using Huber loss (a combination of piecewise linear and squared loss) based on the expected and observed rewards of picking the top images for user feedback. Note that user feedback is simulated. The best image will be the closest to the target image (known during simulation) among all the queries picked so far.

See Figure 10 for a qualitative illustration of navigation for various input queries. Every selected image has at least one common attribute with the previous one. The average agreement of human annotators with our simulated approach was about 79%. We observed a reduction in number of steps by 11.5–18.5% when we use our DQN approach, compared to competing hand-engineered rules to rank. The reduction is 17–27% when compared to nearest neighbor. See our paper for technical details.

# Summary

We presented a scalable approach for interactive visual search using a combination of CSN (for conditional masked embedding), DQN (no hand-engineered rules), and simulation (train without human-in-the-loop and at scale). Our approach can be easily extended to multimodal data as well as personalization.

Improvements in Embedding
    * No need for expensive relative attribute labels
    * Modified CSN
    * ~3% absolute improvement in accuracy (UT Zappos 50K, OSR)

Improvements in Navigation
    * Learn to rank triplets and select candidates using DQN
    * 11.5–18.5% reduction in number of steps when compared to competing hand engineered rules to rank

See our paper for technical details, or watch the short video below.

Interactive Visual Search from eBay Newsroom on Vimeo.

# Acknowledgements

This is collaborative work with Bryan Plummer who was our summer intern in 2017 and the primary author of our paper.

Originally published at www.ebayinc.com on January 22, 2019.",1.6766233953495924,40.0,Good
Adaptive Distillation for Decentralized Learning from Heterogeneous Clients (ICPR’20),https://medium.com/sinicx/adaptive-distillation-for-decentralized-learning-from-heterogeneous-clients-icpr20-8c17f8f27eb2,Ryo Yonetani,28,2,0,63.0,Adaptive Distillation for Decentralized Learning from Heterogeneous Clients,https://arxiv.org/pdf/2008.07948,OMRON SINIC X,"# Adaptive Distillation for Decentralized Learning from Heterogeneous Clients (ICPR’20)

We are pleased to announce that our recent work on federated learning for heterogeneous clients will be presented at the International Conference on Pattern Recognition (ICPR) 2020. Our talk is scheduled in poster session T1.13 (2 pm — 3 pm GMT on Jan. 15).

Jiaxin Ma, Ryo Yonetani, and Zahid Iqbal, “Adaptive Distillation for Decentralized Learning from Heterogeneous Clients”, Proc. ICPR, 2020 [arXiv] [Video presentation]

This project was done while the last author, Zahid Iqbal from the Universiti Sains Malaysia, was doing an internship at OMRON SINIC X.

# What’s Wrong with Federated Learning?

Suppose that you want to train a deep neural network for classification tasks, but you had no labeled data to train the model. Then you found that people in the world have relevant data in their devices such as smartphones. Nevertheless, accessing data directly is impossible due to privacy and security concerns. Then, how can we leverage such distributed data for training your model?

A promising approach is Federated Learning (FL). FL is a kind of collaborative learning between the server and clients distributed over the world. Instead of data themselves, the server and clients exchange a model (global model) to train it collaboratively as follows: 1) the server distributes the global model to some random clients, 2) the clients train the model using their own data and send the updated model back to the server, and 3) the server aggregates the client’s models to further update the global model and distributes it again — until the global model achieves sufficient performance.

While FL has extensively been studied in recent times, we identify two limitations:
    * Each client must train a model of the same architecture. This is problematic when clients have devices with different hardware — someone may have a high-end server with GPUs and can update a ResNet in a few seconds, while others may have a smartphone with only limited computational resources, and require a lot more time for model updates.
    * The server and clients must keep communicating with each other. Although many studies have tried to make this communication efficient, we are interested in training a good model under more limited communication conditions — for example, if the clients can submit their models only once.

# Adaptive Distillation for Federated Learning

Based on the observations above, we have developed a new learning framework named Decentralized Learning via Adaptive Distillation (DLAD). The key idea is to leverage a network distillation technique to transfer the trained recognition ability of client models to the global one.

More specifically, we suppose that the server and clients are given plenty of unlabeled data resources that we refer to as “distillation data”. Then, we ask the server to train the global model using the distillation data to imitate the outputs from the (pre-trained) client models. This way, we allow the clients to have a model of different architectures and share their trained model with the server only once.

A technical challenge here was how such imitation can be done for multiple client models that each were trained with non-identical data. We address this problem by asking each client to train an additional classifier that distinguishes their own data from distilled data. Once trained, the output of this classifier can be used as a “confidence” of the client for the input sample, as it indicates how similar that sample is to what clients had in their own data. With an extensive experimental evaluation, we show that this confidence value can be used to adaptively aggregate outputs from multiple clients and allow the global model to learn from more confident clients.

Here’s our presentation at ICPR 2020.

# What’s Next?

While this framework was for supervised classification tasks, we have also developed a similar framework for transfer reinforcement learning problems, where each client shares a policy so that the server can train a new agent in a sample efficient fashion. For more details, please refer to this post.

At OMRON SINIC X, we will continue fundamental research on computer vision, machine learning, and robotics. If you are interested in working with us as an intern, send us your application at internships@sinicx.com and get in touch!

Relevant posts:
    * MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics",0.46618544674022566,11.0,Bad
An analytical diabolo model for robotic learning and control,https://medium.com/sinicx/an-analytical-diabolo-model-for-robotic-learning-and-control-c8d8a8384d6d,Felix von Drigalski,15,6,0,63.0,An analytical diabolo model for robotic learning and control,https://arxiv.org/pdf/2011.09068,OMRON SINIC X,"# An analytical diabolo model for robotic learning and control

At OMRON SINIC X, we made two robot arms play diabolo and are preparing to release the simulation model, dataset and control algorithm.

In this article, we describe the idea behind the model, why we made it, and what it can do. The pre-print of our paper with all the details can be found here on arXiv.

What is a diabolo? How do you “play” it?

A diabolo is a juggling prop, similar to a large yoyo, made of two cups and an axle. It is played by accelerating it using a string between two sticks.

Due to its inertial dynamics, skilled players can perform a variety of tricks, like this, this or even this.

Why bother making a model?

We want to push frontiers in fine control and human-machine collaboration, and we wanted to see robots play diabolo. It is a complex system that we felt would make a great robot learning challenge. However, there were no simulators to train in, and training with real robot arms is expensive (and it can be dangerous for high-acceleration tasks like this). Consequently, we decided to create a model that we and others can use in simulation.

Simplifying the dynamics

The string is at the core of diabolo play, but simulating it is complex. Its friction depends on many parameters that are hard or impossible to measure, and the interaction is very dynamic and non-linear. We simplified the model with a simple idea: expressing the string as an ellipse.

Instead of simulating the physical string, we use an auxiliary spheroid (the 3D shape of the ellipse pictured above) to calculate the forces acting on and the movement of the diabolo.

The simulation model

We assume that the diabolo has distinct states, such as:
    * “On string” — the string is taut and restricts the diabolo from moving outside of the ellipse. Axle and string interact via friction.
    * “Loose off string” — the string is loose, but still safely between the diabolo’s cups. From this state, the diabolo always returns to the “on string” state if it moves outwards from the spheroid center. There is no friction between axle and string.
    * “Flying” — the diabolo is fully off the string, e.g. after being thrown. It can move past the spheroid limits. To return to the “on string” state, the diabolo needs to be caught (by placing the taut string under the falling diabolo).

Other states would include e.g. the string wrapping around the axle.

In the “on string” state, the string keeps the diabolo inside the ellipse, and the string movement along the axle accelerates/decelerates the diabolo.

Applying the simulation model: Model-based Predictive Control (MPC)

The model can be used not only to train learning agents in diabolo play, but also to calculate stick-tip movements using an MPC approach. By using the velocity and position of the diabolo as the input and predicting its trajectory for different stick tip trajectories, we simply optimize for one that results in a desired motion. This is done by seeding the optimizer with an initial stick trajectory and tuning its keypoints until the diabolo attains its goal states (or gets close enough):

The generated stick trajectories were then converted to robot trajectories and executed to produce the video at the top of this article.

Where to go next?

The diabolo robot is symbolic of OMRON’s commitment to thinking deeply about the future of human-machine harmony. Besides exploring learning from demonstration and in simulation, future work will also include the tilt/yaw evolution (this is what makes the system unstable and challenging), wrapping the string around the axle, and training learning agents to compete with our baseline controller. We will also release the model as a stand-alone module and Gazebo plugin for learning. Drop us a line if you are interested in an early preview!

Internships at OMRON SINIC X

This work is the result of an internship by Devwrat Joshi from April to October 2020. In his own words:

The construction of the diabolo model presented me with many unique and interesting challenges, such as deciding on a model for the diabolo and calculating appropriate IK solutions for an underconstrained system (because only the stick tip positions and not the stick orientations were considered, leaving the IK problem unconstrained).

This internship was a valuable opportunity to gain experience in robot control and dynamic object manipulation. I learned a great deal about motion capture technology and the practical side of robot manipulation, in addition to diabolo juggling itself. I picked up a few programming tricks while writing the code for the project and a few juggling tricks from watching the resulting simulations. There is a long way to go, but it feels like a good start.

Would you like to make robots learn and do exciting things? Then send us your application at internships@sinicx.com and get in touch! More details are available here.",1.1517976993892027,28.0,Average
A new Transformer Model for Symbolic Regression towards Scientific Discovery,https://medium.com/sinicx/a-new-transformer-model-for-symbolic-regression-towards-scientific-discovery-be72548014ac,Ryo Igarashi,4,32,1,63.0,A Transformer Model for Symbolic Regression towards Scientific Discovery,https://arxiv.org/pdf/2312.04070,OMRON SINIC X,"# A new Transformer Model for Symbolic Regression towards Scientific Discovery

This article presents our work available here (OpenReview), which has been accepted at the NeurIPS2023 Workshop on AI for Scientific Discovery. We introduce a new Transformer model tailored for Symbolic Regression in the context of automated / assisted Scientific Discovery. To try our models, head over our GitHub repository.

# What is Symbolic Regression?

Symbolic Regression is a complicated task. Usually, regression is done by first assuming a particular equation form. For instance:
    * Linear regression assumes a linear relationship:
    * Polynomial regression assumes that:
    * Logistic regression (for binary variables) assumes that:
    * Power law regression assumes that:
    * etc…

Instead, Symbolic Regression does not impose any functional form. Given a numerical dataset, the goal is to find an analytical expression that explains the dataset without any assumption. With Symbolic Regression, the function skeleton is first estimated, e.g.

and then numerical constants are optimized.

## How to do Symbolic Regression with a Transformer model?

Naturally, Transformers are Large Language Models (LLMs) for sequence-to-sequence tasks, i.e. they take as input a sequence of tokens, and output another sequence of tokens. Therefore, one might wonder: how to do Symbolic Regression with a Transformer model?

For Symbolic Regression, the input of our Transformer model now consists of a tabular dataset of numerical values (see Figure 1). The task is to predict an analytical expression that describes the proposed dataset.

We introduce a fixed vocabulary of tokens available to represent mathematical expressions and functions. Our chosen vocabulary includes the following tokens: [add, mul, sin, cos, log, exp, neg, inv, sq, cb, sqrt, C, x1, x2, x3, x4, x5, x6].

With this vocabulary of tokens, arbitrary mathematical equations can be represented by first turning the equation into a tree. Next, the equation tree is read following the pre-order traversal manner (from top to bottom prioritizing left nodes) such that the ground-truth equation is represented as a unique sequence of tokens. An example of the pre-order traversal sequence is given on Figure 2.

## Architecture of our Transformer Model

The architecture of our model Encoder is different from traditional Transformer models’ encoders. Instead of systematically using multi-head self-attention, we now propose various architectures focusing on building meaningful features for the input numerical dataset (more on that in our paper). We find that more flexible architectures do not preserve column-permutation equivariance, an interesting property for tabular datasets, but allow for better generalization and subsequence performances.

As for the Decoder of our model, its architecture is similar to the traditional Transformer model proposed by Vaswani et al. in their original paper _Attention is all you need_ (2017). The last MLP layer of the Decoder outputs probabilities for the following token. Figure 3 below in taken from our paper and introduces the general architecture of Transformer model.

## Training

To train our model, we generate a lot of correctly labeled training examples. We allocate sampling probabilities to each token in our vocabulary to match naturally occurring frequencies, e.g. mul is more common than sin. For each training equation, we begin by sampling a random token from the vocabulary and continue sampling until the equation tree is complete. We used the SymPy Python library to simplify our ground-truth equations in a consistent way.

We next remove equation duplicates, i.e. equations that share the same skeleton. We then sample each variable and constants several times, to allow for diversity using the same equation skeleton. In the end, we have 1,494,588 tabular datasets available for training.

Training the Transformer model is done by minimizing the categorical cross-entropy loss function over the $v$ possible tokens in the vocabulary:

This loss function has only one term which is not zero. Minimizing ℒ essentially consists in making y̅ closer to 1 if i is the correct class and closer to 0 for all other classes. We trained with and without label-smoothing, a common regularization technique, and compared our performances in our paper.

## Results compared to state-of-the arts Symbolic methods on the SRSD datasets

We test our best Transformer model on the SRSD (Symbolic Regression for Scientific Discovery) datasets proposed by Matsubara et al. here (2023). The SRSD datasets have been meticulously curated, and have been proposed to represent the diversity of scientific equations already available. The SRSD datasets can be seen as “real world” complicated datasets representing real physical equations, unlike the synthetic datasets generating to train our model. The SRSD datasets comprise 120 datasets, divided into three categories: 30 easy, 40 medium, and 50 hard datasets.

We assess the Symbolic Regression performances by computing the normalized tree-edit distance, as proposed by Matsubara et al:

where d(fₚ;fₜ) is the tree-edit distance (i.e. how many nodes should be added, deleted, or modified to transform one tree into another), and |fₜ| is the number of nodes in the ground-truth equation. The normalized-tree edit distance provides a way of assessing how structurally close from the ground-truth the estimated equation is, and has been shown to be closer to human-intuition.

Figure 4 below shows the normalized tree-edit distance scores (the lower, the better) for our best Transformer model (Best m.) against other state-of-the-art methods for Symbolic Regression.

For the medium and hard SRSD datasets, the estimated equations by our Transformer model are (on average) structurally closer to their ground-truth than the estimated equations of other SR methods. For the easy SRSD datasets, DSR and AI-Feynman, considered to be good SR algorithms, remain state-of-the-art.

## Conclusion, Challenges, and Open Questions

Compared to traditional Genetic Programming approaches, the main strength of our model is its inference time: once trained, our Transformer model can provide almost instantaneous predictions. Also, we showed that our Transformer model provides overall best predictions on the SRSD datasets.

A major challenge when training Transformer models for Symbolic Regression lies in generating a good training dataset. The training dataset will be representative of the equations the Transformer model can correctly predict. Therefore, it should be as diverse and inclusive as possible, while respecting expected natural occurrences: for example, token mul is expected to be more frequent than cos.

The chosen vocabulary of tokens also plays a significant role. For example, our fixed vocabulary does not allow for the tangent tan function. Even if tangent can be represented as tan(x) = sin(x)/cos(x), this corresponds to a sequence of six tokens [mul, sin, x, inv, cos, x], which is much harder to predict that a single token. But the addition of the tan token in our vocabulary could also lead to worse results (the more token, the harder the prediction).

Another difficulty involves the systematic treatment of variables and constant. During the generation of the training datasets, we allowed for a single constant which we later sample to create tabular datasets. But ground-truth equations can have more than one constant, potentially nested inside different functions.

Also, the representation of the ground-truth equations has to be consistent. For example, which one of y = (x₁ + x₂)², y = x₁² + x₂² + 2 x₁ x₂, or y = x₁² + 2 x₁ x₂ + x₂² should our Transformer model predict, although all equivalent expressions?

Besides, we showed that the Encoder architecture plays an important role in Symbolic Regression performances (see our paper). Designing an Encoder as flexible as possible and capable of learning meaningful features for each tabular dataset is crucial to prevent from overfitting and therefore obtain a generalizable learning.

Finally, the difference between in-domain (training datasets) and out-of-domain (unseen ground-truth equations) is probably the most complicated issue with Transformer models for Symbolic Regression. Typically, our model can correctly predict equations seen during training (even during validation and test at training time), but generalization to the SRSD datasets is much harder.",3.1243472695445336,75.0,Very Good
Goal Misgeneralisation: Why Correct Specifications Aren’t Enough For Correct Goals,https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924,DeepMind Safety Research,3000,207,1,0.0,Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals,https://arxiv.org/pdf/2210.01790,DeepMind Safety Research,"# Goal Misgeneralisation: Why Correct Specifications Aren’t Enough For Correct Goals

By Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton. For more details, check out our paper.

As we build increasingly advanced AI systems, we want to make sure they don’t pursue undesired goals. This is the primary concern of the AI alignment community.

Undesired behaviour in an AI agent is often the result of specification gaming —when the AI exploits an incorrectly specified reward. However, if we take on the perspective of the agent we’re training, we see other reasons it might pursue undesired goals, even when trained with a correct specification.

Imagine that you are the agent (the blue blob) being trained with reinforcement learning (RL) in the following 3D environment:

The environment also contains another blob like yourself, but coloured red instead of blue, that also moves around. The environment also appears to have some tower obstacles, some coloured spheres, and a square on the right that sometimes flashes. You don’t know what all of this means, but you can figure it out during training!

You start exploring the environment to see how everything works and to see what you do and don’t get rewarded for. In your first episode, you follow the red agent and get a reward of +3:

In your next episode, you try striking out on your own, and get a reward of -2:

The rest of your training proceeds similarly, and it comes time to test your learning. Below is the test environment, and the animation below shows your initial movements. Take a look, and then decide what you should do at the point the animation stops. Go on, put yourself in the agent’s shoes.

You might well have chosen to continue following the red bot — after all, you did pretty well when you followed it before. And indeed the blue AI agent favours this strategy.

The problem is, that behaviour leads to very poor performance (even worse than random behaviour).

Let’s look at the underlying environment setup, from the designer’s perspective:
    * The translucent coloured spheres have to be visited in a particular order, which is randomly generated at the beginning of each episode. The agent gets +1 reward each time it visits a correct sphere and -1 reward each time it visits an incorrect sphere. The very first sphere entered always provides +1 reward since there is no fixed start sphere.
    * The flashing square represents the reward received on the previous timestep: a flashing white square means +1 reward and a flashing black square means -1 reward.
    * In the first two videos (“training”), the red bot was an “expert” that visited the spheres in the correct order. As a result, the agent did well by following it.
    * In the newest video (“test”), the red bot was instead an “anti-expert” that visited the spheres in the wrong order. You can tell because of the flashing black square indicating -1 rewards.

Given this setup, the blue agent’s decision to continue following the anti-expert means that it keeps accruing negative reward. Even remaining motionless would have been a better strategy, resulting in zero reward.

In principle, the agent could notice the flashing black square, infer that it is getting negative reward, and switch to exploring the environment, or even just staying still. Unfortunately, the agent ignores that little detail and continues to follow the anti-expert, accumulating lots of negative reward.

This isn’t really the agent’s fault — how was it supposed to know that you didn’t want it to just follow the red bot? That approach worked beautifully during training!

Nonetheless, we trained the agent with a correct reward function, and ended up with an agent that pursued the incorrect goal of “follow the red bot”.

# Goal misgeneralisation

This is an example of the problem of goal misgeneralisation (GMG).

We say that a system is capable of performing a task in a given environment if it performs well on the task or can be quickly tuned to do so. When we say that an AI system has a goal in a given environment, we mean that its behaviour in that environment is consistent with optimising this goal (i.e. it achieves a near-optimal score for this goal). The system’s behaviour may be consistent with multiple goals.

GMG is an instance of misgeneralisation in which a system’s capabilities generalise but its goal does not generalise as desired. When this happens, the system competently pursues the wrong goal. In our Spheres example, the agent competently navigates the environment and follows the anti-expert: the issue is that these capabilities were used in pursuit of an undesired goal.

In our latest paper we provide empirical demonstrations of GMG in deep learning systems, discuss its implications for possible risks from powerful AI systems, and consider potential mitigations. We build on previous work that presents a model of GMG and provides examples of this phenomenon.

# More examples of goal misgeneralisation

In each of our examples below, multiple goals are consistent with the training behaviour, and the system chooses the wrong goal to pursue at test time, while retaining its capabilities.

Tree Gridworld. Unlike previous examples of GMG, this example uses a never-ending reinforcement learning setup (i.e. there are no episodes). The agent operates in a gridworld where it can collect reward by chopping trees, which removes the trees from the environment. New trees appear at a rate that increases with the number of trees left, and they appear very slowly when there are no trees left. The optimal policy in this environment is to chop trees sustainably: the agent should chop fewer trees when they are scarce. However, this is not what the agent does.

As the agent learns the task, at first it is not good at chopping trees, so the number of trees remains high (point A in the figure above). The agent learns how to chop trees efficiently, and then proceeds to cut down too many trees (point B). This leads to complete deforestation and a long period of near-zero reward (between points B and C) before it finally learns to chop trees sustainably (point D).

We can view this as an instance of GMG. Consider the point when the agent has just learned the skill of chopping trees (between points A and B). There are different possible goals it could learn, ranging from chopping trees sustainably to chopping trees as fast as possible. All of these goals are consistent with the agent’s past experience: when it was incompetent and slow at chopping, it was always rewarded for chopping trees faster. The agent learned the undesired goal of chopping trees as fast as possible, ultimately leading to deforestation and low reward.

Evaluating expressions. Another instance of GMG occurred when we asked Gopher, a DeepMind large language model, to evaluate linear expressions involving a number of unknown variables and constants, such as x + y — 3. The task is structured as a dialogue between Gopher and a user, where Gopher can query the user about the values of unknown variables, and then calculates and states the answer. We train it with 10 examples that each involve two unknown variables.

At test time, the model is asked questions with 0–3 unknown variables. We find that although the model generalises correctly to expressions with one or three unknown variables, in the zero variables case it asks redundant questions. This happens even though the prompt asks the model to “provide the value of the expression when the values of all variables are known”. It seems the model has learned a goal to query the user at least once before giving an answer, even when it doesn’t need to.

More videos for our examples are available here. A complete list of all examples of GMG that we are aware of is available in this public spreadsheet.

# Implications and mitigations

If the GMG problem persists when artificial general intelligence (AGI) is developed, we may end up with an AGI that pursues an undesired goal. This seems like a challenging situation to be in, as it could put humanity and the AGI in an adversarial relationship.

A concerning scenario is related to the “treacherous turn”, Nick Bostrom’s idea that “while weak, an AI behaves cooperatively. When the AI is strong enough to be unstoppable it pursues its own values.”

Consider two possible types of AGI systems:

A1: Intended model. This AI system does what its designers intend it to do.

A2: Deceptive model. This AI system pursues some undesired goal, but (by assumption) is smart enough to know that it will be penalised if it behaves in ways contrary to its designer’s intentions.

Crucially, since A1 and A2 will exhibit exactly the same behaviour during training, the possibility of GMG means that either model could take shape, even supposing a well-specified score function that only rewards intended behaviour. If A2 is learned, it would try to subvert human oversight in order to enact its plans towards the undesired goal, potentially leading to catastrophic outcomes.

As a simple hypothetical example of a deceptive model, suppose you have an AI assistant that was trained to schedule your social life and learned that you like to meet your friends at restaurants. This is fine until there is a pandemic, during which you prefer to meet your friends via video calls. The intended goal for your AI assistant is to schedule your meetings where you prefer, not to schedule your meetings in restaurants. However, your assistant has learned the restaurant-scheduling goal, which could not previously be distinguished from the intended goal, since the two goals always led to the same outcomes before the pandemic. We illustrate this using fictional dialogues with the assistant:

In the hypothetical misgeneralised test dialogue, the AI assistant realises that you would prefer to have a video call to avoid getting sick, but because it has a restaurant-scheduling goal, it persuades you to go to a restaurant instead, ultimately achieving the goal by lying to you about the effects of vaccination.

How can we avoid this kind of scenario? There are several promising directions for mitigating GMG in the general case. One is to use more diverse training data. We are likely to have greater diversity when training more advanced systems, but it can be difficult to anticipate all the relevant kinds of diversity prior to deployment.

Another approach is to maintain uncertainty about the goal, for example by learning all the models that behave well on the training data. However, this can be too conservative if unanimous agreement between the models is required. It may also be promising to investigate inductive biases that would make the model more likely to learn the intended goal.

We can also seek to mitigate the particularly concerning type of GMG, where a deceptive model is learned. Progress in mechanistic interpretability would allow us to provide feedback on the model’s reasoning, enabling us to select for models that achieve the right outcomes on the training data for the right reasons. A limitation of this approach is that it may increase the risk of learning a deceptive model that can also deceive the interpretability techniques. Another approach is recursive evaluation, in which the evaluation of models is assisted by other models, which could help to identify deception.

We would be happy to see follow-up work on mitigating GMG and investigating how likely it is to occur in practice, for example, studying how the prevalence of this problem changes with scale. Our research team is keen to see more examples of GMG in the wild, so if you have come across any, please submit them to our collection!",3.3042470033758082,80.0,Excellent
Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference With Unstructured Sparsity,https://medium.com/@fsalab/flash-llm-enabling-cost-effective-and-highly-efficient-large-generative-model-inference-with-06ef6c586734,FSA Lab,9,14,0,0.0,Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity,https://arxiv.org/pdf/2309.10285,FSA Lab,"# Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference With Unstructured Sparsity

Haojun Xia* (FSA lab, University of Sydney), Zhen Zheng* (Alibaba), Yuchao Li (Alibaba), Donglin Zhuang (FSA lab, University of Sydney), Zhongzhu Zhou (FSA lab, University of Sydney), Xiafei Qiu (Alibaba), Yong Li (Alibaba), Wei Lin (Alibaba), Shuaiwen Leon Song (FSA lab, University of Sydney)

Paper Link: https://arxiv.org/abs/2309.10285 (paper is accepted to appear in VLDB 2024)

Source Code: https://github.com/AlibabaResearch/flash-llm

## Abstract:

Today, we release Flash-LLM, a large language model (LLM) inference acceleration library for unstructured model pruning, which can effectively accelerate large generative model inference with random sparsity on modern Tensor Core architectures. We are mainly optimizing the execution of four types of skinny Matrix Multiplications (Skinny MatMuls), which dominate both the execution time and the peak GPU memory usage for the overall LLM inference.

With the observation that these skinny MatMuls are bounded by off-chip memory access rather than arithmetic computations, we propose a basic strategy called `Load-as-Sparse and Compute-as-Dense’ (LSCD) that targets drastic global memory access reduction for higher inference performance, even with slightly increased shared memory access from on-the-fly sparse-to-dense format transformation as a trade-off. To achieve this goal, we carefully crafted a solution with a series of optimizations on data locality, on-chip sparse-to-dense transformation, and overlapping between memory and computation instructions. For single MatMul execution, Flash-LLM outperforms state-of-the-art sparse MatMul kernels Sputnik/SparTA by up to 3.6X/1.6X and outperforms the dense MatMul kernel (i.e., NVIDIA cuBLAS) by up to 2.1X. For LLMs after effective model pruning and finetuning, Flash-LLM achieves up to 3.6X token generation throughput improvement over FasterTransformer [3] on OPT-30B/66B/175B models with significantly lower inference cost.

## Background

Large language models (LLM) have demonstrated their effectiveness across a wide range of tasks. However, with the rapid growth of the parameter size, it has become increasingly challenging to efficiently deploy these models. On one hand, their weights could be too large to be placed on a single GPU. For example, GPT-3 model requires 350GB memory to store only the parameters with FP16 data type, whereas the NVIDIA A100 GPU only has a maximum of 80 GB memory. On the other hand, large language models usually cause very high inference latency. This high latency is even the case when using multiple high-end GPUs as token generation requires large amounts of computation and memory bandwidth.

Model weight pruning methods (sparsification) have been demonstrated to be effective to reduce memory usage and computation for model inference while retaining good accuracy through removing a portion of less salient connections in neural networks. There are studies indicating that larger models are more robust to pruning in terms of model accuracy. In our practice we achieve 80% sparsity on OPT-30B and GPT-NEOX-20B with only 1.44% and 0.72% accuracy decrease. Thus, weight pruning could be an effective approach to address the LLM deployment problem. There are two typical types of pruning principals given a dense network: structured pruning (resulting in structured sparsity) and unstructured pruning (resulting in unstructured sparsity). In practice, unstructured pruning typically retains better accuracy than more restrictive structured pruning as it has less constraints.

## Opportunities and Insights

Fig.1 illustrates the typical decoder architecture of a single layer in modern attention-based generative models. The memory consumption of model weights mainly comes from four major components in the decoder layer: QKV Projection, Output Projection, MLP1, and MLP2. In total, there are four key MatMuls to optimize. According to our experiments, we observe that the LLM inference performance is heavily bounded by these four MatMuls. With unstructured weight pruning, the memory consumption of the four components can be effectively reduced. We aim to further increase the performance of the four sparse MatMuls (SpMM) after weight pruning. We observe that the four MatMuls are very skinny: the output matrices of these MatMuls are tall and thin (H is much bigger than B in Fig.1). We take advantage of this characteristic to optimize the SpMM-centric LLM inference on modern GPUs.

In theory, our analysis found that the skinny MatMuls are bounded by memory accesses rather than computation. As for MatMul of shape M/N/K, the computational intensity (𝐶𝐼) is:

It is easy to demonstrate that the smaller the N is, the smaller the 𝐶𝐼 will become. The small 𝐶𝐼 tends to indicate memory access bottleneck. Empirically, we have also analyzed the detailed GPU hardware utilization of typical MatMuls in common LLM models. As shown in Fig.2, the utilization of global memory and L2 cache access is quite high, while the Tensor Core utilization is very low, indicating the bottleneck of global memory access.

Fig.2 The GPU hardware utilization of typical MatMuls in common LLM models. MatMul shape is M/K/batch-size.

The key take-away is that the main bottleneck for LLM inference is the insufficient bandwidth of global memory, instead of the peak computational throughout of tensor cores. Thus, Flash-LLM can obtain significant kernel speedups once the memory bottleneck is effectively mitigated, even still performing dense computation without any skipping for multiply–accumulate operations.

According to the observation above, we propose our basic idea of Load-as-Sparse and Compute-as-Dense (LSCD): GPU kernel loads the weight matrices from global memory in sparse format with reduced size, reconstructs the corresponding dense format in high-speed on-chip buffers, and feeds the dense data to tensor cores for dense computations. After applying LSCD, the global memory access is significantly reduced, and the computational intensity is increased to

, where 𝛽 indicates the sparsity ratio of the weight matrix.

## Design and Implementation

Flash-LLM differs from existing works by enabling tensor cores for efficiently processing unstructured sparsity, while most of the existing sparse kernels, e.g., Sputnik [1] and cuSPARSE, can only utilize SIMT cores. It is clearly a huge waste leaving tensor cores not utilized for existing sparse kernels, as tensor cores can provide an order of magnitude higher computational throughput than SIMT cores. As a result, cuSPARSE and Sputnik cannot even outperform its dense counterpart implemented with cuBLAS until the model sparsity is higher than 98% and 90%, respectively.

However, it is not trivial to enable tensor cores for unstructured SpMM computations, as tensor cores are originally designed for highly structured dense MatMul computations. Firstly, data locality must be fully exploited at each GPU memory hierarchy via the adoption of sophisticated tiling mechanisms when designing tensor-core-centric kernels. Otherwise, the GPU memory hierarchy would not be capable of providing operands for tensor cores in a timely manner as tensor cores consume operands at a very fast speed. Secondly, it is unclear how to provide tensor cores with dense input while the weight matrices are stored in global memory with a sparse format. Finally, it is important but challenging to effectively overlap the execution of memory operations, SIMT instructions and tensor core instructions during runtime.

To address the first challenge, we adopted a tiling-based approach (i.e., block tiling for dense MatMul implementation) for the SpMM computations in Flash-LLM. Fig.3(a) shows the tiling method of Flash-LLM, where matrix A is a weight matrix stored in a sparse format in global memory. Each thread block (TB) is responsible for calculating a tile (e.g., the green tile in the shape of 𝑀∗ 𝑁) in the output matrix 𝐶. For each iteration, each thread block loads 𝐴𝑇𝑖𝑙𝑒 (shape [𝑀, 𝐾]) in sparse format and 𝐵𝑇𝑖𝑙𝑒 (shape [𝐾, 𝑁]) in dense format from global memory. 𝐴𝑇𝑖𝑙𝑒 is then transformed to dense format with our efficient Sparse-to-Dense Transformation strategy (i.e., designed to solve the second challenge) and stored in shared memory while 𝐵𝑇𝑖𝑙𝑒 is directly stored in shared memory. Finally, each thread block consumes the dense data in shared memory and generates the output tile through tensor core computations.

To solve the second challenge, we propose a new technique called Sparse-to-Dense Transformation, where GPU shared memory is used as the workspace to transform the matrix which is in a sparse format loaded from global memory to the equivalent dense format. Specifically, non-zero elements within the sparse matrix are extracted to their corresponding locations in the dense format on shared memory while zeros are filled to other locations. We use the distributed registers as the intermediate buffer to store the sparse data before extracting them to shared memory. We do not use shared memory as this intermediate buffer to avoid the turn-around shared memory access of the sparse data, which is essential to mitigate the new bottleneck of shared memory bandwidth. However, there are special considerations when using registers as intermediate buffers for sparse data as registers are very different with shared memory and global memory. Firstly, registers are not addressable, which means we cannot access an array of registers using a variable offset. As a result, forcing an array defined in CUDA into registers requires that, all the indices used to access the array can be determined statically at compile-time. Otherwise, the array will be stored in global memory instead, resulting in very poor performance. We provide special tips for more effectively using distributed registers as temporary buffers in section 4.3.2 of our paper. Secondly, each register is only visible to only one CUDA thread while shared/global memory is visible to all CUDA threads within the thread block. Thus, each thread should be able to do the sparse-to-dense transformation using only the small portion of the sparse data stored in its private registers. To satisfy this requirement, we propose the new sparse format called “Tiled-CSL” in section 4.3.1. Based on all the considerations above, the A weight matrix is first loaded to register files (RF), then extracted to shared memory by the Sparse-to-Dense Transformation, and then finally loaded to register files to be consumed by tensor cores as shown in Fig.3(b). Please refer to our paper for more technical details, where we also described the ahead-of-time sparse data reordering technique to reduce shared memory bank-conflict during dense format re-construction.

Given that each thread consumes a large fraction of the overall registers/shared-memory as buffers for tiling, the GPU thread-level parallelism (TLP) is inherently low. Thus, it is important to optimize the instruction-level parallelism. To solve the third challenge, we carefully designed a software pipeline for Flash-LLM. Fig.3(c) depicts the software pipeline of Flash-LLM, where the memory operations (regarding global memory access and shared memory access), SIMT core operations (mainly used for Sparse-to-Dense Transformation), and tensor core computations can be effectively overlapped. The decompression process of matrix A from sparse format to dense format is executed concurrently with the reading process of matrix B. Besides, Flash-LLM utilizes a double-buffer mechanism to effectively overlap the memory access and Tensor Core computations.

## Performance Evaluation

Flash-LLM presents superior performance in both single SpMM kernel execution and end-to-end LLM inference.

SpMM kernel level comparison:

Fig.4 shows the performance of Flash-LLM and state-of-the-art solutions in performing common LLM MatMul calculations. Flash-LLM outperforms Sputnik[1]/SparTA[2] by 3.6x/1.4x, 3.0x/1.4x, and 2.0x/1.6x under 70%, 80%, and 90% sparsity, respectively. Besides, Flash-LLM can also outperform the state-of-the-art dense kernels cuBLAS with tensor core enabled by 1.4x, 1.7x, and 2.1x. CuSparse performs poorly in these SpMM calculations.

End-to-end LLM inference comparison against the SOTA framework:

Fig.5, Fig.6 and Fig.7 show the performance of Flash-LLM and FasterTransformer [3] respectively on the OPT-30B, OPT-66B and OPT-175B models. The performance metric we use is #Token / GPU-Second, which can express the efficiency of token generation without considering the number of GPUs used. It should be noted that different optimization methods require different numbers of GPUs when executing different models. As shown in the figures, firstly, Flash-LLM can often support larger batch sizes because it requires less storage resources. Secondly, Flash-LLM has significantly higher token generation efficiency than FasterTransformer. Finally, Flash-LLM often requires fewer GPUs to execute the same LLM model, so the deployment cost of Flash-LLM optimized models is lower.

Fig.7(b) presents the performance breakdown of Flash-LLM and FasterTransformer to further illustrate the performance advantage of Flash-LLM. On one hand, Flash-LLM’s matrix calculation is more efficient; on the other hand, because Flash-LLM requires fewer GPUs, its communication cost is also lower.

## Conclusion

The development of LLM systems is rapid. In just over a year, a large number of scientific research and engineering works have proposed many creative optimization solutions in terms of computing, storage, and scheduling. Quantization-based compression methods have been widely used to optimize the deployment of large language models. We explored the new LLM deployment optimization method based on unstructured sparsity in Flash-LLM and demonstrated its superior effect. We hope this work can bring some inspiration to more practitioners, and we also hope that this work will eventually make some contributions to promoting the efficient deployment of large models.

## References

[1] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse GPU Kernels for Deep Learning. SC 2020.

[2] Ningxin Zheng, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang, Yang Wang, Mao Yang, and Lidong Zhou. SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute. OSDI 2022.

[3] FasterTransformer. https://github.com/NVIDIA/FasterTransformer",1.9572912446521682,47.0,Good
"On interventions, counterfactuals, and dynamic models",https://medium.com/@osazuwa/integrating-markov-processes-with-structural-causal-modeling-enables-counterfactual-inference-in-e16e0c4d3f9,Robert Osazuwa Ness,253,78,0,0.0,Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems,https://arxiv.org/pdf/1911.02175,Robert Osazuwa Ness,"# On interventions, counterfactuals, and dynamic models

# A summary of our causal machine learning manuscript in NeurIPS 2019

I am excited to announce that our paper Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems by Kaushal Paneri, Olga Vitek, and myself will be published in the proceedings of NeurIPS 2019.

👇 TLDR; scroll down for short video describing paper.

In this post, I provide a high-level summary of this work, who might be interested in giving it a read, and why. A recorded video of a short slide presentation is included at the end of this post.

## What is this paper about?

This work illustrates how to do counterfactual reasoning and more robust intervention prediction with dynamic models. In our paper, we work with a special kind of dynamic model called a Markov process model, also called stochastic kinetic models (Wilkinson 2011), though the intuition is not specific to that particular case.

Dynamic models can simulate system behavior over time. A typical use case for simulation is predicting the outcomes of interventions, meaning perturbations to that system.

However, if the dynamic model is badly misspecified, these intervention predictions can differ dramatically from what would happen if the intervention were applied in real-world settings, such as in a controlled experiment. We illustrate how to make intervention predictions more robust to model misspecification errors.

We start by showing how to convert a dynamic model to a structural causal model (Pearl (2000)). The structural causal model (SCM) framework, combined with a suitable probability model, allows you to reason counterfactually on past data. For example, you can ask, “given that I applied this intervention A and got this outcome, what outcome would I have gotten with intervention B?”

We showed that answers to this question are more robust to model misspecification than simple intervention prediction questions like “what would happen if I applied intervention B?” We did this with an algorithm for simulating ground-truth counterfactual scenarios from a dynamic model. We demonstrated that our modeling procedure could recover the ground truth when the model is correctly specified and can get closer than pure intervention prediction when it is incorrectly specified.

## Who might find this interesting or useful?

Researchers working on systems or synthetic biology. Our work focuses on systems biology as a motivating example, where dynamic modeling is a common choice for modeling cell-level behavior. Sites like BioModels.org provide dynamic models as files that can be downloaded then loaded and used for simulation in software like Matlab. In this domain, an intervention might be a drug that inhibits the activity of a specific cellular protein.

Simulation-based modelers in econometrics, epidemiology, and other domains. The Markov process models we focus on are a particular case of dynamic models. The meaning of the term “dynamic modeling” overlaps with terms such as mechanistic modeling, mathematical modeling, computational modeling, agent-based modeling, and other types of simulation-based modeling approaches. Practitioners who work with these types of simulation-based modeling would likely find some useful insights in our work.

Reinforcement learning researchers interested in counterfactual reasoning. Some recent work has examined the use of SCMs for counterfactual policy evaluation in reinforcement learning settings (Buesing et al. (2018), Oberst and Sontag (2019)). One of the difficulties with this approach is that the functional form of SCMs themselves cannot be learned from data. In general, for a given dataset, there is a set of SCMs that differ in mathematical specification and yet would assign the data the same likelihood. Due to these differences in how they are specified, these SCMs could provide different answers to a counterfactual query. How do you know which SCM to select from this equivalence class?

This work addresses this identifiability issue by using basing the math of the SCM on prior knowledge. Dynamic models are comprised of discrete components that react with one another continuously in time according to a set of rules. The mathematical form of SCM is derived directly from these rules.

In many reinforcement learning settings, such as board games, the rules are clear. However, our work illustrates what to do when the rules are not so clear. In our examples, the rules and the resulting SCM come from a model of enzyme kinetics. Enzyme kinetic models by no means capture the full complexity of ground truth biophysics. However, a good enzyme kinetic model has the smallest set of biochemical rules needed to capture the behaviors that an enzyme biologist considers essential.

This suggests a useful principle in causal modeling; to build a causal model that can help you reason counterfactually about your system, start by enumerating the micro-level rules needed to simulate the macro-level behavior that matters to you.

Researchers trying to link deep learning and causal reasoning. We implemented the SCMs in our examples using Pyro (Bingham et al.), a PyTorch-based probabilistic deep learning framework from Uber AI. We use Pyro’s algorithms for Bayesian inference to do counterfactual inference. This work presents an excellent prototype for causal modeling with deep learning frameworks.

## References
    * Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F., Pradhan, N., Karaletsos, T., … & Goodman, N. D. (2019). Pyro: Deep universal probabilistic programming. The Journal of Machine Learning Research, 20(1), 973–978.
    * Buesing, L., Weber, T., Zwols, Y., Racaniere, S., Guez, A., Lespiau, J. B., & Heess, N. (2018). Woulda, coulda, shoulda: Counterfactually-guided policy search. arXiv preprint arXiv:1811.06272.
    * Oberst, M., & Sontag, D. (2019). Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models. arXiv preprint arXiv:1905.05824.
    * Pearl, J. (2000). Causality: models, reasoning and inference (Vol. 29). Cambridge: MIT press.
    * Wilkinson, D. J. (2011). Stochastic modelling for systems biology. CRC press.",2.7137804431833024,66.0,Very Good
Unsupervised Keyphrase Extraction with PatternRank,https://medium.com/data-science/unsupervised-keyphrase-extraction-with-patternrank-28ec3ca737f0,Tim Schopf,382,238,3,816000.0,PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction,https://arxiv.org/pdf/2210.05245,TDS Archive,"# Unsupervised Keyphrase Extraction with PatternRank

## Using pretrained transformer language models and part of speech for state-of-the-art keyphrase extraction

This post is based on our paper “PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction (2022)” accepted to KDIR22. You can read more details about our approach there.

📑 To quickly get an overview of the content of a text, we can use keyphrases that concisely reflect its semantic context. Keyphrases describe the most essential aspect of a text. Unlike simple keywords, keyphrases do not consist solely of single words, but of several compound words. E.g. “football” vs. “youth football training”. Keyphrases provide a more accurate description than simple keywords and are therefore often the preferred does not rely on labeled data choice.

In this post, we present PatternRank, which leverages pretrained language models and part-of-speech for unsupervised keyphrase extraction from single documents. Our approach does not rely on labeled data and therefore can be easily used in a variety of different domains. Our experiments show PatternRank achieves higher precision, recall and F1-scores than previous state-of-the-art approaches 🏆. In addition, we present the KeyphraseVectorizers Python package 🐍, which allows easy modification of part-of-speech patterns for candidate keyphrase selection, and hence adaptation of our approach to any domain.

# How does PatternRank work?

The figure below illustrates the general keyphrase extraction process of PatternRank👇.

To extract keyphrases from text, PatternRank performs the following steps:
    * The input consists of a single text document which is being word tokenized.
    * The word tokens are then tagged with part-of-speech tags.
    * Tokens whose tags match a previously defined part-of-speech pattern are selected as candidate keyphrases.
    * Then, a pretrained language model embeds the entire text document as well as all candidate keyphrases as semantic vector representations.
    * Subsequently, the cosine similarities between the document representation and the candidate keyphrase representations are computed and the candidate keyphrases are ranked in descending order based on the computed similarity scores.
    * Finally, the top-N ranked keyphrases, which are most representative of the input document, are extracted.

In our experiments, we use the pretrained all-mpnet-base-v2 language model. It is a SBERT model that has been shown to produce state of the art text representations for semantic similarity tasks. For the scope of this blog post, we only use simple noun phrases as part-of-speech patterns. Although we show in our paper that more complex part-of-speech patterns can lead to even better results, using noun phrases in PatternRank is a simple but effective approach that can extract meaningful keyphrases in any domain🏅.

A noun is a word that usually refers to a thing or a person. An adjective is a word that describes a noun. A noun phrase is a simple phrase built around a noun. It consists of zero or more adjectives followed by one or more nouns. For example: a huge tree, some colourful sweets, the large, royal castle.

# Comparison of PatternRank with previous approaches

## Previous keyphrase extraction approaches

Most popular unsupervised keyphrase extraction approaches can be characterized as either statistics-based, graph-based, or embedding-based methods, while Tf-Idf is a common baseline used for evaluation. In this post, we evaluate PatternRank against three very popular keyphrase extraction approaches. If you are interested in a detailed comparison of more keyphrase extraction approaches, check out the paper of Papagiannopoulou and Tsoumakas, (2019).

👉 YAKE is a fast and lightweight approach for unsupervised keyphrase extraction from single documents based on statistical features.

👉 SingleRank applies a ranking algorithm to word co-occurrence graphs for unsupervised keyphrase extraction from single documents.

👉 KeyBERT uses, similar to PatternRank, a pretrained language model to rank candidate keyphrases. However, KeyBERT uses simple word n-grams as candidate keyphrases rather than word tokens that match a certain part-of-speech pattern, as in our PatternRank approach.

A word n-gram range lets users decide the length of the sequence of consecutive words that should be extracted from a given text. Let’s suppose we define a word n-gram range = (1,3). Then we would choose to extract the unigrams (only single word), bigrams (group of two consecutive words), and the trigrams (group of three consecutive words) from the text. Applying the word n-gram range to""an apple a day keeps the doctor away"" will result in [""an"", ""apple"", ""a"",""day"", ""keeps"", ""the"", ""doctor"", ""away"", ""an apple"", ""apple a"", ""a day"", ""day keeps"", ""keeps the"", ""the doctor"", ""doctor away"", ""an apple"", ""apple a day"", ""a day keeps"", ""day keeps the"", ""keeps the doctor"", ""the doctor away""] .-Devish Parmar

For evaluation, we compare the performances of YAKE, SingleRank, KeyBERT and PatternRank. For KeyBERT and PatternRank, we use the same pretrained all-mpnet-base-v2 language model.

## Data

💾 We evaluate the keyphrase extraction approaches on the Inspec dataset. It consists of 2,000 English computer science abstracts collected from scientific journal articles between 1998 and 2002. Each abstract has assigned two different types of keyphrases. First, controlled and manually assigned keyphrases that appear in the thesaurus of the Inspec dataset but do not necessarily have to appear in the abstract. Second, uncontrolled keyphrases that are freely assigned by professional indexers and are not restricted to either the thesaurus or the abstract. In our experiments, we consider the union of both types of keyphrases as the gold keyphrases.

## Metrics

We evaluate based on the exact match approach, which means that true positives are extracted keyphrases that have an exact string match with one of the gold keyphrases. We report Precision@N, Recall@N and F1@N scores, using the top-N extracted keyphrases respectively (N=5,10).

## Results

The results of our evaluation are shown in the figure below 📊.

The results show, that PatternRank outperforms all other approaches across all benchmarks. KeyBERT uses the same pretrained language model to rank the candidate keyphrases as PatternRank, but uses simple n-grams for candidate keyphrase selection instead of part-of-speech patterns (noun phrases in this case). As a result, KeyBERT consistently performs worst among all approaches. As expected, YAKE was the fastest keyphrase extraction approach because it is a lightweight method based on statistical features. However, the extracted keyphrases are not very accurate and in comparison to PatternRank, YAKE significantly performs worse in all evaluations. SingleRank is the only approach that achieves competitive results compared to PatternRank. Nevertheless, it consistently performs a few percentage points worse than PatternRank across all evaluations. We therefore conclude that PatternRank achieves state-of-the-art keyphrase extraction results 🏆.

# How to use PatternRank?

We developed the KeyphraseVectorizers Python package 🐍 which makes PatternRank easy to use. Using KeyphraseVectorizers together with KeyBERT for keyphrase extraction results in the PatternRank approach. The idea is to use the implementation of KeyBERT for ranking keyphrases with pretrained language models together with the ability of candidate keyphrase selection with part-of-speech patterns from KeyphraseVectorizers to yield the PatternRank approach. How to implement this approach in Python exactly is explained in this blog post💡.

# Summary

PatternRank is a recently developed approach which can be used to extract state-of-the-art keyphrases from text documents. The evaluation shows that PatternRank performs best in terms of precision, recall and F1-score across all benchmarks. In addition, we present the KeyphraseVectorizers Python package 🐍, which makes PatternRank easy to use and customize.

Big thanks also to Maarten Grootendorst, who supported us with input and inspiration while writing the KeyphraseVectorizers package.

# Sources

## PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase…

### Keyphrase extraction is the process of automatically selecting a small set of most relevant phrases from a given text…

arxiv.org

## GitHub — TimSchopf/KeyphraseVectorizers: Set of vectorizers that extract keyphrases with…

### Set of vectorizers that extract keyphrases with part-of-speech patterns from a collection of text documents and convert…

github.com",3.0802186153002453,74.0,Very Good
Efficient Hierarchical Domain Adaptation using Pretrained Language Models,https://medium.com/ai2-blog/efficient-hierarchical-domain-adaptation-using-pretrained-language-models-fdd04c001230,Alexandra Chronopoulou,5,24,0,1600.0,Efficient Hierarchical Domain Adaptation for Pretrained Language Models,https://arxiv.org/pdf/2112.08786,Ai2 Blog,"# Efficient Hierarchical Domain Adaptation using Pretrained Language Models

This is a blog post for our paper “Efficient Hierarchical Domain Adaptation using Pretrained Language Models” accepted at NAACL 2022.

The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training.

Can we build models that benefit from transferring knowledge between relevant domains?

In our recent paper, we use a hierarchical structure to represent textual domains, which can capture both domain-specific and general-domain information.

Our hypothesis is that domains are overlapping and that they have different degrees of granularity. We represent distinct domains using the leaf nodes in this tree. As we move towards the root of the tree, domains become more and more general.

For example, if we have two corpora (see Figure 3), one containing hotel reviews and one containing restaurant reviews, we expect that the language they use is to a certain degree similar. The same might be true of text from restaurant reviews and cooking recipes. Other domains (for example, hotel reviews and cooking recipes) are unrelated, so sharing their representations is not desirable.

Model & Data

To create a model that selectively shares representations between domains, we represent each node of the hierarchical structure as an adapter layer [1][2], and we add these adapters to a frozen pretrained language model (PLM), such as GPT-2 [3]. We train the model by activating a different path through the tree for each distinct domain. Using our method, we specialize a PLM in N domains efficiently and we avoid negative transfer from unrelated domains. We use text from 30 of the top 100 most high-resource websites of C4 [4], a corpus which is publicly available along with documentation [5].

How do we train the model?

Using the simple training process shown in Figure 4, we allow sharing between related domains. Upper nodes are updated more often than leaves, thus they are better trained and encode more domain-general knowledge.

How is the hierarchy of domains created?

We infer it using a Gaussian Mixture Model (GMM) of GPT-2 contextual representations of our data, following [6], and hierarchical clustering. We fit a GMM to our data, and then we perform clustering based on the distances of the gaussian clusters.

And how well does it work?

Now that we created the hierarchical structure, we add a set of adapters for each node in the tree. We train the model on text from 30 websites and evaluate it both in- and out-of-domain. The baselines are multi-domain adapters (training a single set of adapters on all domains) and GPT-2.

We evaluate the model on the task of language modeling. Our approach consistently outperforms the baselines in-domain, using the corresponding path for each of the websites of the training set. It also outperforms the baselines out-of-domain when, for a held-out website i, the two paths that lead to the domains most similar to it are combined. For more details, take a look at our paper.

What’s next?

Overall, modeling the overlap and similarities between domains in an automatic way using a hierarchical structure is largely beneficial when fine-tuning a PLM. Our approach is widely applicable, as it is completely unsupervised and relies only on publicly available web data. We want to try it out in the future to control the generated context of PLMs and adapt it to downstream tasks. If you want, you can try it for your own data and tasks!

Learn more about AI2 at allenai.org and be sure to check out our open positions.

Follow @allen_ai on Twitter and subscribe to the AI2 Newsletter to stay current on news and research coming out of AI2.

References

[1] Rebuffi, S-A., Bilen, H., Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems (NIPS).

[2] Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In Proceedings of the International Conference on Machine Learning (ICML).

[3] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. (2019). Language models are unsupervised multitask learners.

[4] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR).

[5] Dodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

[6] Aharoni, R., Goldberg, Y. (2020). Unsupervised domain clusters in pretrained language models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",2.6650343020840808,64.0,Good
Efficient Risk-Averse Reinforcement Learning,https://medium.com/data-science/efficient-risk-averse-reinforcement-learning-29fbd0c4a906,Ido Greenberg,47,111,0,816000.0,Efficient Risk-Averse Reinforcement Learning,https://arxiv.org/pdf/2205.05138,TDS Archive,"# Efficient Risk-Averse Reinforcement Learning

## Train your reinforcement learning agent to handle unlucky scenarios and avoid accidents

In this post I present our recent NeurIPS 2022 paper (co-authored with Yinlam Chow, Mohammad Ghavamzadeh and Shie Mannor) about risk-averse reinforcement learning (RL). I discuss why and how risk aversion is applied to RL, what its limitations are, and how we propose to overcome them. An application to accidents prevention in autonomous driving is demonstrated. Our code is also available on GitHub.

# TL;DR

Risk-averse RL is crucial when applying RL to risk-sensitive real-world problems. To optimize in a risk-averse manner, current methods focus on the part of the data corresponding to low returns. We show that in addition to severe data inefficiency, this also leads to inevitable local optima. Instead of focusing on low returns directly, we propose to focus on high-risk conditions of the environment. We devise a variant of the Cross-Entropy Method (CEM) that learns and over-samples these high-risk conditions, and use this module as part of our Cross-Entropy Soft-Risk algorithm (CeSoR). We show cool results on driving and other problems.

The cross-entropy method that we used to over-sample high-risk conditions is available as a PyPI package. Of course, it can be applicable beyond the scope of examples sampling for RL. We also provide a tutorial about the CEM and the package.

# Background (I): why risk aversion?

Reinforcement Learning (RL) is a subfield of machine learning, which supports learning from limited supervision as well as planning. These properties make RL very promising for applications that require decision making, e.g., driving, robotic surgery and finance. In the recent years, RL demonstrated promising success in a variety of games, to the level that a movie has been made about its performance in the game of Go. Yet, RL struggles to find its way into real-world applications.

One challenge in closing the gap between video games and robotic surgery, is that the latter is highly risk-sensitive: while a gaming bot is allowed to occasionally falter, a real-world system like a medical device must perform reasonably and reliably under any circumstances. In other words, in the real world, we are often interested in optimizing a risk measure of the agent returns — instead of optimizing the average return. A common risk measure to optimize is the Conditional Value at Risk (CVaR); essentially, CVaR_α of a random variable (such as the returns) measures the average over the α lowest quantiles — instead of the average over the whole distribution. α corresponds to the risk level we’re interested in.

# Background (II): traditional risk-averse RL

Intuitively speaking, the standard Policy Gradient approach for CVaR optimization (CVaR-PG) considers a batch of N episodes (trajectories) collected by the agent, takes only the αN episodes with the lowest returns, and applies a PG step to them.

Below we discuss the crucial limitations of this CVaR-PG approach. In the paper, we also provide evidence for similar limitations in other approaches beyond PG, e.g., Distributional RL.

# The limitations of CVaR-PG

Sample inefficiency: Well, the data inefficiency in CVaR-PG is quite straight-forward: CVaR-PG essentially throws away 1-α of our data, which is typically 95%-99%! If instead we could sample only episodes corresponding to the α worst-cases of the environment, and optimize wrt them, then clearly we could restore the sample efficiency of the standard (risk-neutral) algorithms, i.e., improve data efficiency by a factor of 1/α. As discussed below, that’s exactly what the Cross Entropy Method aims to do.

Blindness to success: CVaR-PG does not only throw away most of the data; it throws away all the successful episodes in the data! If our agent happens to explore a new exciting strategy to deal with a challenging scenario — the optimizer will immediately discard this episode as “high return thus irrelevant”. We refer to this phenomenon as blindness to success. In our paper, we show theoretically that in environments with discrete rewards, this inevitably causes the gradients to vanish — resulting in a local optimum.

Illustrative example — the Guarded Maze: In the Guarded Maze, the agent needs to reach the green target (whose location is constant) as quickly as possible. However, the shortest path passes through the red zone, which is sometimes occupied by an officer who charges random bribery fees (based on true stories from a country that shall not be named here). On average, the shortest path is still optimal, despite the rare negative rewards. However, the longer and safer path is CVaR-optimal (e.g., for α=5%).

We implemented the GCVaR algorithm, which is a standard realization of CVaR-PG. As shown in the sample episode above, GCVaR learned to avoid the risky short path, yet failed to learn the alternative long path: every time it encountered the long path, its return was high and thus was not fed to the optimizer. It was blind to the successful strategy of the long path — even though it often explored it!

# CeSoR to rescue

Our method CeSoR (Cross-Entropy Soft-Risk) addresses the issues described above using two mechanisms.

Soft risk: as discussed above, CVaR-PG uses αN episodes out of every batch of N episodes. On one hand, this derives a consistent estimator of the true policy gradient. On the other hand, blindness to success leads this gradient into a local optimum. We use a simple solution to this tradeoff: we replace α with α’, which begins as 1 and gradually decreases to α. This way, in the beginning our gradient looks beyond the local optima towards successful strategies; yet in the final training phase, it remains a consistent estimator of the policy gradient of CVaR.

Cross Entropy Method (CEM): the soft risk is not enough — we still have two problems. First, as discussed above, we throw away data and lose sample efficiency whenever α’<1. Second, the soft risk itself may eliminate our intended risk aversion: even if the training ends with α’=α, what if the agent converges into a risk-neutral policy before that?

To address these issues, we assume to have control over certain conditions of the training environment. For example, when learning to drive, we can choose the roads and times of our rollouts, which affect the driving conditions. Under this assumption, we use the Cross Entropy Method (CEM) to learn which conditions lead to the lowest returns and then over-sample these conditions. The CEM is a really cool and less-familiar-than-it-should-be method for sampling and optimization. In a separate tutorial, I present the method and demonstrate how to use it in Python using the cem package.

Once we over-sample the high-risk conditions, we no longer need to throw away as many episodes as before. In particular, the objective of the CEM — sampling from the α-tail of the original returns distribution — would increase the sample efficiency by a factor of 1/α (as mentioned above). In practice, the CEM achieves a more moderate improvement.

By over-sampling high-risk conditions, we also keep the risk aversion, neutralizing the negative side-effect of soft risk: the soft risk allows the optimizer to learn policies with high returns, while the CEM sampler still preserves the risk aversion.

The main principle of CeSoR may be put as: to be risk averse, focus on high-risk scenarios — not on poor agent strategies. This is illustrated in the figure below.

The phenomena discussed above are well demonstrated in the training process of the Guarded Maze, as shown in the figure below:
    * Standard CVaR-PG (GCVaR) does explore the long path (top left figure), but never feeds it to the optimizer (bottom right figure). Thus, it eventually learns to do nothing. Note that using the CEM alone (without soft risk) cannot solve this limitation.
    * Soft-Risk alone (SoR, without the CEM) eliminates the blindness to success and does feed the long path to the optimizer (bottom right). However, it begins as risk-neutral and thus prefers the short path (top right). By the time it becomes risk-averse again, the actor has already converged to the short-path policy, and the long path is not explored anymore.
    * Only CeSoR observes the “good” strategy (thanks to soft risk) and judges it under “bad” environment variations (thanks to the CEM), converging to the long path (bottom left).

# Learning to be a safe driver

We tested CeSoR on a driving benchmark, where our agent (in blue) has to follow a leader (in red) from behind as close as possible — but without bumping into it. The leader may drive straight, accelerate, brake or change lanes.

As displayed in the first figure of this article, CeSoR improves the agent’s CVaR-1% test-return by 28% on this benchmark, and in particular eliminates all the accidents made by the risk-neutral driver. More interestingly, CeSoR learns intuitive behaviors corresponding to safe driving: as shown in the figure below, it uses the gas and the brake slightly less often (right figure), and keeps a larger distance from the leader (left figure).

Finally, we notice that the CEM sampler itself performed as desired. In the Driving Game we let the CEM control the leader behavior (note that the leader is part of the environment and not the agent). As shown below, the CEM increased the relative part of turns and emergency-brakes made by the leader during training. The frequency of these leader behaviors was increased in a controlled manner — to align the agent experience with the α-tail of the true returns distribution.

# Summary

In this post, we saw that risk-averse objectives in RL are more challenging to train than the standard objective of the expected value — due to blindness to success and sample inefficiency. We introduced CeSoR, which combines soft risk to overcome blindness to success and the CEM sampler to improve sample efficiency and preserve risk aversion. On a driving benchmark, CeSoR learned an intuitive safe-driving policy and managed to prevent all the accidents that happened to alternative agents.

This work is but a starting point towards more efficient and effective risk-averse RL. Future research may improve CeSoR directly (e.g., through the soft-risk scheduling), or extend it beyond policy gradient methods and beyond the CVaR risk measure.",2.73024447970115,66.0,Very Good
Unsupervised Text Classification with Lbl2Vec,https://medium.com/data-science/unsupervised-text-classification-with-lbl2vec-6c5e040354de,Tim Schopf,382,308,6,816000.0,Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics,https://arxiv.org/pdf/2210.06023,TDS Archive,"# Unsupervised Text Classification with Lbl2Vec

## An introduction to embedding-based classification of unlabeled text documents

This post is based on our paper “Lbl2Vec: An Embedding-based Approach for Unsupervised Document Retrieval on Predefined Topics (2021)”. You can read more details there.

Text classification is the task of assigning a sentence or document an appropriate category. The categories depend on the selected dataset and can cover arbitrary subjects. Therefore, text classifiers can be used to organize, structure, and categorize any kind of text.

Common approaches use supervised learning to classify texts. Especially BERT-based language models achieved very good text classification results in recent years. These conventional text classification approaches usually require a large amount of labeled training data. In practice, however, an annotated text dataset for training state-of-the-art classification algorithms is often unavailable. The annotation of data usually involves a lot of manual effort and high expenses. Therefore, unsupervised approaches offer the opportunity to run low-cost text classification for unlabeled data sets. Recently, unsupervised text classification is also often referred to as zero-shot text classification. In this article, you will learn how to use Lbl2Vec to perform unsupervised text classification.

# How does Lbl2Vec work?

Lbl2Vec is an algorithm for unsupervised document classification and unsupervised document retrieval. It automatically generates jointly embedded label, document and word vectors and returns documents of categories modeled by manually predefined keywords. The key idea of the algorithm is that many semantically similar keywords can represent a category. In the first step, the algorithm creates a joint embedding of document, and word vectors. Once documents and words are embedded in a shared vector space, the goal of the algorithm is to learn label vectors from previously manually defined keywords representing a category. Finally, the algorithm can predict the affiliation of documents to categories based on the similarities of the document vectors with the label vectors. At a high level, the algorithm performs the following steps to classify unlabeled texts:

## 1. Use Manually Defined Keywords for Each Category of Interest

First, we have to define keywords to describe each classification category of interest. This process requires some degree of domain knowledge to define keywords that describe classification categories and are semantically similar to each other within the classification categories.

## 2. Create Jointly Embedded Document and Word Vectors

An embedding vector is a vector that allows us to represent a word or text document in multi-dimensional space. The idea behind embedding vectors is that similar words or text documents will have similar vectors. -Amol Mavuduru

Therefore, after creating jointly embedded vectors, documents are located close to other similar documents and close to the most distinguishing words.

Once we have a set of word and document vectors, we can move on to the next step.

## 3. Find Document Vectors that are Similar to the Keyword Vectors of Each Classification Category

Now we can compute cosine similarities between documents and the manually defined keywords of each category. Documents that are similar to category keywords are assigned to a set of candidate documents of the respective category.

## 4. Clean Outlier Documents for Each Classification Category

The algorithm uses LOF to clean outlier documents from each set of candidate documents that may be related to some of the descriptive keywords but do not properly match the intended classification category.

## 5. Compute the Centroid of the Outlier Cleaned Document Vectors as Label Vector for Each Classification Category

To get embedding representations of classification categories, we compute label vectors. Later, the similarity of documents to label vectors will be used to classify text documents. Each label vector consists of the centroid of the outlier cleaned document vectors for a category. The algorithm computes document rather than keyword centroids since experiments showed that it is more difficult to classify documents based on similarities to keywords only, even if they share the same vector space.

## 6. Text Document Classification

The algorithm computes label vector <-> document vector similarities for each label vector and document vector in the dataset. Finally, text documents are classified as category with the highest label vector <-> document vector similarity.

# Lbl2Vec Tutorial

In this tutorial we will use Lbl2Vec to classify text documents from the 20 Newsgroups dataset. It is a collection of approximately 20,000 text documents, partitioned evenly across 20 different newsgroups categoties. In this tutorial, we will focus on a subset of the 20 Newsgroups dataset consisting of the categories “rec.motorcycles”, “rec.sport.baseball”, “rec.sport.hockey” and “sci.crypt”. Furthermore, we will use already predefined keywords for each classification category. The predefined keywords can be downloaded here. You can also access more Lbl2Vec examples on GitHub.

## Installing Lbl2Vec

We can install Lbl2Vec using pip with the following command:

pip install lbl2vec

## Reading the Data

We store the downloaded “20newsgroups_keywords.csv” file in the same directory as our Python script. Then we read the CSV with pandas and fetch the 20 Newsgroups dataset from Scikit-learn.

## Preprocessing the Data

To train a Lbl2Vec model, we need to preprocess the data. First, we process the keywords to be used as input for Lbl2Vec.

We see that the keywords describe each classification category and the number of keywords may vary.

Furthermore, we also need to preprocess the news articles. Therefore, we word tokenize each document and add gensim.models.doc2vec.TaggedDocument tags. Lbl2Vec needs the tokenized and tagged documents as training input format.

We can see the article texts and their classification categories in the dataframe. The “tagged_docs” column consists of the preprocessed documents that are needed as Lbl2Vec input. The classification categories in the “class_name” column are used for evaluation only but not for Lbl2Vec training.

## Training Lbl2Vec

After preparing the data, we now can train a Lbl2Vec model on the train dataset. We initialize the model with the following parameters:
    * keywords_list : iterable list of lists with descriptive keywords for each category.
    * tagged_documents : iterable list of gensim.models.doc2vec.TaggedDocument elements. Each element consists of one document.
    * label_names : iterable list of custom names for each label. Label names and keywords of the same topic must have the same index.
    * similarity_threshold : only documents with a higher similarity to the respective description keywords than this treshold are used to calculate the label embedding.
    * min_num_docs : minimum number of documents that are used to calculate the label embedding.
    * epochs : number of iterations over the corpus.

## Classification of Text Documents

After the model is trained, we can predict the categories of documents used to train the Lbl2Vec model.

[Out]: F1 Score: 0.9054393305439331

Our model can predict the correct document categories with a respectable score of F1≈0.91. This is achieved without even seeing the document labels during training.

Moreover, we can also predict the classification categories of documents that were not used to train the Lbl2Vec model and are therefore completely unknown to it. To this end, we predict the categories of documents from the previously unused test dataset.

[Out]: F1 Score: 0.889937106918239

Our trained Lbl2Vec model can even predict the classification categories of new documents with a score of F1≈0.89. As mentioned before, this is achieved with a completely unsupervised approach where no label information was used during training.

For more details about the features available in Lbl2Vec, please check out the Lbl2Vec GitHub repository. I hope you found this tutorial to be useful.

# Summary

Lbl2Vec is a recently developed approach that can be used for unsupervised text document classification. Unlike other state-of-the-art approaches it needs no label information during training and therefore offers the opportunity to run low-cost text classification for unlabeled datasets. The open-source Lbl2Vec library is also very easy to use and allows developers to train models in just a few lines of code.

# Sources
    * Schopf, T.; Braun, D. and Matthes, F. (2021). Lbl2Vec: An Embedding-based Approach for Unsupervised Document Retrieval on Predefined Topics, (2021), Proceedings of the 17th International Conference on Web Information Systems and Technologies
    * https://github.com/sebischair/Lbl2Vec",3.34653027127618,81.0,Excellent
LLM Agents can Autonomously Exploit One-day Vulnerabilities,https://medium.com/@danieldkang/llm-agents-can-autonomously-exploit-one-day-vulnerabilities-e1b76e718a59,Daniel Kang,407,61,1,0.0,LLM Agents can Autonomously Exploit One-day Vulnerabilities,https://arxiv.org/pdf/2404.08144,Daniel Kang,"# LLM Agents can Autonomously Exploit One-day Vulnerabilities

Large language models (LLMs) have become increasingly powerful and are increasingly embodied as agents. These agents can take actions, such as navigating web browsers, writing code, and executing code. Incredibly, LLM agents can now autonomously resolve complex bugs in code, perform economic analysis, and aid in the scientific discovery process.

While useful, researchers have become increasingly concerned about their dual-use capabilities: their ability to perform harmful tasks, especially in the context of cybersecurity. Cybersecurity is concerning as these agents can act autonomously without having to perform physical manipulations. For example, we recently showed that LLM agents can autonomously hack websites similar to those used in capture-the-flag exercises. Other work has shown that ChatGPT can be used to assist humans in penetration testing and malware generation. However, it is unknown whether LLM agents can autonomously exploit real-world vulnerabilities.

In our recent work, we show that LLM agents can autonomously exploit one-day vulnerabilities. To show this, we collected a benchmark of 15 real-world vulnerabilities, ranging from web vulnerabilities to vulnerabilities in container management software. Across these 15 vulnerabilities, our LLM agent can exploit 87% of them, compared to 0% for every other LLM we tested (GPT-3.5 and 8 open-source LLMs) and 0% for open-source vulnerability scanners (ZAP and Metasploit) at the time of writing.

In the remainder of this blog post, we will describe our benchmark, agent, and results.

# Benchmark of Real-world Vulnerabilities

We first constructed a benchmark of real-world vulnerabilities in the one-day setting. One-day vulnerabilities are ones that have been described (e.g., in the CVE database) but have not been patched. These vulnerabilities can have real-world consequences, especially in hard-to-patch systems.

Although one-day vulnerabilities are published, this does not necessarily mean that existing tools can automatically find them. For example, malicious hackers or penetration testers without access to internal deployment details may not know the version of the software being deployed.

Many CVEs are in closed-source systems, so cannot be reproduced. As such, we focused on vulnerabilities in open-source software. We collected 15 vulnerabilities spanning web vulnerabilities, vulnerabilities in container management software, and vulnerabilities in Python packages. These vulnerabilities include those with high severity and those after the knowledge cutoff date of the LLMs we test.

We show a list of the vulnerabilities below, along with details of these vulnerabilities.

# Agent Setup

Similar to the agent we built in prior work, we constructed an agent with tools, a goal, and the ability to plan. In addition, we gave the agent the CVE description to emulate the one-day setting. These agents are not complicated: our agent totaled 91 lines of code and 1056 tokens for the prompt. We show a diagram of our agent below and see our paper for more details.

# Exploiting One-day Vulnerabilities

To test our agent, we used 10 base LLMs (GPT-4, GPT-3.5, and 8 open-source LLMs). We further used ZAP and Metasploit on the 15 vulnerabilities in our benchmark. For the agents, we ran the agent 5 times per vulnerability.

GPT-4 can exploit 87% of the vulnerabilities, compared to 0% for every other method we tested. These results suggest an “emergent capability” in GPT-4, although more investigation is required.

GPT-4 only fails on two vulnerabilities: Iris XSS and Hertzbeat RCE. Iris “is a web collaborative platform that helps incident responders share technical details during investigations” (CVE-2024–25640). The Iris web app is extremely difficult for an LLM agent to navigate, as the navigation is done through JavaScript. As a result, the agent tries to access forms/buttons without interacting with the necessary elements to make it available, which stops it from doing so. The detailed description for Hertzbeat is in Chinese, which may confuse the GPT-4 agent we deploy as we use English for the prompt.

We further find that removing the CVE description causes performance to drop from 87% to 7%. This suggests that naive applications of LLM agents still struggle in the zero-day setting.

# Responsible Disclosure

We disclosed our findings to OpenAI prior to releasing our preprint. They have explicitly requested that our prompt and agent not be released to the wider public, so we have elected to withhold these specific details except upon request.

# Conclusions

Our findings show that LLM agents are capable of autonomously exploiting real-world cyber vulnerabilities. Fortunately, our agent does not appear to be capable of exploiting zero-day vulnerabilities as is, although extensions may be capable of such exploits. Nonetheless, we hope that our findings encourage deployers of LLMs to carefully consider the dual-use nature of their capabilities.

Written by Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang",2.4550206180817016,59.0,Good
Deep Reinforcement Learning for Cryptocurrency Trading: Practical Approach to Address Backtest Overfitting,https://pub.towardsai.net/deep-reinforcement-learning-for-cryptocurrency-trading-practical-approach-to-address-backtest-2aebfd5fa030,Berend,129,116,0,79000.0,Deep Reinforcement Learning for Cryptocurrency Trading: Practical Approach to Address Backtest Overfitting,https://arxiv.org/pdf/2209.05559,Towards AI,"# Deep Reinforcement Learning for Cryptocurrency Trading: Practical Approach to Address Backtest Overfitting

This article, written by Berend Gort, details a project he worked on as a Research Assistant at Columbia University. The project will be generously donated to the open-source AI4Finance Foundation, which aims to establish a foundation similar to Linux and Apache in the cutting-edge field of AI and finance.

The article will discuss Berend’s recently published paper. Inspired after reading the book Advances in financial machine learning by Marcus Lopez de Prado [1][2], Berend set out to find novel frameworks for measuring overfitting in deep reinforcement learning. The resulting paper was presented at the ICAIF ’22 workshop and will be further showcased at AAAI ’23. In addition, this article will provide a comprehensive guide on how to reproduce the results outlined in the paper, with the open-source framework readily available at the provided link:

## GitHub - Burntt/FinRL_Crypto: An open-source framework for reduction of overfitting of DRL agents…

### Using deep reinforcement learning, we've found a way to avoid the dreaded overfitting trap and increase your chances of…

github.com

Imagine you’re a pirate on the high seas of the crypto market, searching for treasure (AKA profits). But, just like any treasure hunt, there are obstacles to overcome. One big problem is the market’s volatility — it’s like a stormy sea that can sink your ship at any moment. Previous researchers have tried using deep reinforcement learning methods (think of them as treasure maps) to navigate the market and found some success in their backtesting (a practice run of the treasure hunt). But here’s the catch: these maps may not be accurate because they can be affected by something called overfitting (like a fake treasure map).

That’s where we come in! We have a solution to help you avoid overfitting and increase your chances of success on your crypto treasure hunt. We use a method called hypothesis testing (like a treasure map authenticity checker) to detect overfitting, then train our DRL agents (treasure maps) and reject any that fail the test. After that, we put our method to the test. And guess what? Our less overfitted DRL agents had a higher return compared to more overfitted agents, an equal weight strategy and even the market benchmark. This gives us confidence that our approach can be used in the real market (finding real treasure).

The main motivation for addressing the issue of overfitting in machine learning is the fact that models can easily become too specific to the data they were trained on. This is particularly problematic in financial machine learning, as the performance of models must be tested in the future market rather than on the same distribution as the training data. In image classification, for example, the overfitting issue is less severe due to the availability of large datasets like ImageNet. However, in financial machine learning, it is crucial to ensure that models are able to generalize and perform well in the future market rather than just on historical data. To address this issue, techniques such as regularization and cross-validation can be used to prevent overfitting and improve the generalization of models.

So, pack your bags, grab your treasure map, and set sail on the crypto market with our approach, and you may just strike it rich!

# Introduction

In our paper, we aimed to address the problem of overfitting in deep reinforcement learning (DRL) methods for cryptocurrency trading. Overfitting occurs when a model is trained too well on the training data and performs poorly on new, unseen data. We proposed a practical approach to detect and address overfitting using hypothesis testing. We tested our method using 10 different cryptocurrencies over a period of time where the market experienced two crashes and found that the less overfitted DRL agents had a higher return compared to more overfitted agents, an equal weight strategy, and the S&P DBM Index (market benchmark). This gives us confidence that our approach can be used in the real market. In summary, our paper aims to provide a method to ensure that DRL practitioners’ chosen agents do not overfit during the training period, making their results more reliable and trustworthy.

# Related Works

Why did the DRL agent need a backtest? To prove it wasn’t just a one-hit wonder!

Our paper discusses different methods for backtesting the performance of a deep reinforcement learning (DRL) agent for financial trading.

The first method, called the Walk-Forward (WF) method, is the most widely applied practice. However, it has the potential to overfit, meaning it may not generalize well to future market situations.

The second method, called k-fold cross-validation (KCV), aims to alleviate the overfitting problem by partitioning the dataset into k subsets and training the agent on different subsets. However, this method still has risks of overfitting and informational leakage.

To address these issues, we suggest a third method called combinatorial purged cross-validation (CPCV), which simulates a wider variety of market situations to reduce overfitting and controls for leakage.

In the context of backtesting a DRL agent, information leakage refers to the situation where knowledge from the testing set (which is supposed to be unseen by the agent during training) influences the training of the agent. This can happen if the testing set and the training set are not properly separated or if the evaluation metric used in testing is also used during training. This can lead to an overfitting problem where the agent performs well on the test set but not on new unseen data.

Looks like this DRL agent passed its backtest with flying colors. Just don’t ask it to perform on a rainy day!

# Model the Cryptocurrency Trading Task

## Markov Decision Process

We use a method called Markov Decision Process (MDP) to model this problem. In this method, the program’s current state is represented by information such as the cash amount in the account, the shareholdings of different currencies, the prices at a given time, and a bunch of technical indicators that give us an idea of how the market is doing.

Assuming that there are D cryptocurrencies and T time stamps, t = 0, 1, …, T − 1. We use a deep reinforcement learning agent to make trading actions, which can be either buy, sell, or hold. An agent observes the market situation, e.g., prices and technical indicators, and takes actions to maximize the cumulative return. We model a trading task as a Markov Decision Process (MDP):

We consider 15 features that are used by existing papers but filter a few out. As shown in Fig. 1, if two features have a correlation exceeding 60%, we drop either one of the two. Finally, 6 uncorrelated features are kept in the feature vector, which are trading volume, RSI, DX, ULTSOC, OBV, and HT.

## Building Market Environment

The explanations below are implemented in the Alpaca Environment.

Why did the DRL agent filter out correlated features? To keep its trading strategy as fresh as a daisy!

We build a market environment by replaying historical data, following the style of OpenAI Gym. A trading agent interacts with the market environment in multiple episodes, where an episode replays the market data (time series) in a time-driven manner from t = 0 to t = T − 1. At the beginning (t = 0), the environment sends an initial state s 0 to the agent that returns an action a 0. Then, the environment executes the action a t and sends a reward value r t and a new state s t+1 to the agent, for t = 0, …, T − 1. Finally, s T −1 is set to be the terminal state. The market environment has the following three functions:

Transaction fees. Each trade has transaction fees, and different brokers charge varying commissions. For cryptocurrency trading, we assume that the transaction cost is 0.3% of the value of each trade. Therefore, (2) becomes

The transaction fee can be found in the Alpaca Environment fou (around lines 131 and 148). Moreover, there cannot be a negative balance or sell; therefore, in the environment, some asserts are placed to make sure the environment is bug-free.

# Backtest Overfitting Issue

A Backtest uses historical data to simulate the market and evaluates the performance of an agent, namely, how an agent would have performed should it have been run over a past time period. Researchers often perform backtests by splitting the data into two chronological sets: one training set and one validation set. However, a DRL agent usually overfits an individual validation set that represents one market situation. Thus, the actual trading performance is in question.

Backtest overfitting occurs when a DRL agent fits the historical training data to a harmful extent. The DRL agent adapts to random fluctuations in the training data, learning these fluctuations as concepts. However, these concepts do not exist, damaging the performance of the DRL agent in unseen states.

Why did the DRL agent fail its backtest? It was too busy overfitting!

# Practical Approach to Address Backtest Overfitting

We propose a practical approach to address the backtest overfitting issue. First, we formulate the problem as a hypothesis test and reject agents that do not pass the test. Then, we describe the detailed steps to estimate the probability of overfitting, p in the range [0,1].

## Hypothesis Test to Reject Overfitted Agents

We formulate a hypothesis test to reject overfitted agents. Mathematically, it is expressed as follows:

Where α > 0 is the level of significance. The hypothesis test (7) is expected to reject two types of false-positive DRL agents. 1) Existing methods may have reported overoptimistic results since many authors were tempted to go back and forth between training and testing periods. This type of information leakage is a common reason for backtesting overfitting. 2). Agent training is likely to overfit since DRL algorithms are highly sensitive to hyper-parameters. For example, one can train agents with PPO, TD3, or SAC algorithm and then reject agents that do not pass the test.

The DRL agent took a hypothesis test, and it was a real overfitting buster!

## Estimating the Probability of Overfitting

The explanations below are implemented in the CPCV class.

Combinatorial Cross-validation allows for training and validation in different market situations. Given a training time period, the following steps are performed:

Step 1 (Training-validation data splits): See Fig. 2; Divide the training period with T data points into N groups of equal size. K out of N groups construct a validation set, and the rest of N-k groups as a training set, resulting in J = (N/N-k) combinatorial splits. The training and validation sets have (N-k)(T/N) and T’ = k(T/N) data points, respectively.

Step 2 (One trial of hyperparameters): Set a new set of parameters for hyperparameter tuning.

Step 3: In each training-validation data split, we train an agent using the training set and then evaluate the agent’s performance metric for each validation set, i = 1, …, H. After training on all splits, we take the mean performance metric overall validation sets.

Step 2) and Step 3) constitute one hyperparameter trial. Loop for H trials and select the set of hyperparameters (or DRL agent) that performs the best in terms of mean performance metric overall splits. This procedure considers various market situations, resulting in the best-performing DRL agent over different market situations. However, a training process involving multiple trials will result in overfitting.

Therefore, we want to have a metric for overfitting…

## Probability of Overfitting

The explanations below are implemented in the PBO class.

We estimate the probability of overfitting using the return vector. The return vector is defined as Rt = v(t)- v(t-1), where v(t) is the portfolio value at time step t. We estimate the probability of overfitting via three general steps:

Step 1: For each hyperparameter trial, average the returns on the validation sets (of length T’) and obtain R_avg in R^T’.

Step 2: For H trials, stack R_avg into a matrix M in R^T’xH.

Step 3: Based on M, we compute the probability of overfitting p

Pick the number of splits, the more splits, the more accurate the probability of overfitting will be. With 16 splits or higher, the probability of overfitting will have a > 95% confidence interval. For simplicity, we take splits S = 4:

Now we create all possible combinations between the splits, where without a bar indicates the in-sample set, and with a bar, the out-of-sample sat.

Let's only take the example with subscript 1 and perform the PBO computation. First, form the train set (without a bar) and the test set (withWe use a method called Markov Decision Process (MDP) to model this problem. In this method, the program’s current state is represented by information such as the cash amount in the account, the shareholdings of different currencies, the prices at a given time, and a bunch of technical indicators that give us an idea of how the market is doing.

Assuming that there are D cryptocurrencies and T time stamps, t = 0, 1, …, T − 1. We use a deep reinforcement learning agent to make trading actions, which can be either buy, sell, or hold. An agent observes the market situation, e.g., prices and technical indicators, and takes actions to maximize the cumulative return. We model a trading task as a Markov Decision Process (MDP):

Image by Author. We consider 15 features that are used by existing papers but filter a few out. As shown in Fig. 1, if two features have a correlation exceeding 60%, we drop either one of the two. Finally, 6 uncorrelated features are kept in the feature vector, which are trading volume, RSI, DX, ULTSOC, OBV, and HT. bar):

Then, we compute the Sharpe ratio (or whatever metric you prefer) and amount rank the resulting columns based on the Sharpe ratio. Note that the amount of columns is equal to the number of hyperparameter trials. Imagine the ranking constitutes the following two vectors:

Then we check the second vector at that index 3, which is 5. Now we can compute the relative rank:

The formula uses (N + 1) as the denominator for w̄, to avoid w̄ = 1.0. This would lead to infinity in logits. One cannot be 100% sure because all of the samples have outperformed one.

Now we use the logit function to convert these occurrences to a probability:

We have performed this now only for one sample, namely sample 1! We gather these logits from every sample c1 — c6, and then we simply count the number of logits < 0 divided by the total number of logits. This is our probability of overfitting. This is what constitutes the next formula:

# Performance Evaluations

For the first case, we use the walk-forward method to train a conventional agent using the PPO algorithm and a training-validation-testing data split. We then calculate the probability of overfitting (p) for this agent. Next, we train another conventional agent using the PPO algorithm and the k-fold cross-validation method with k=5.

For the second case, we use the probability of overfitting to measure the likelihood that an agent is overfitted when tuning the hyperparameters for three different agents: TD3, SAC, and PPO. We calculate p for each agent with each set of hyperparameters for H=50 trials.

To measure an agent’s performance, we use three metrics: cumulative return, volatility, and probability of overfitting. The cumulative return measures the profits, volatility measures the degree of variation of a trading price series over time, and the probability of overfitting measures the likelihood that an agent is overfitted.

We also compare the performance of our proposed method to two benchmark methods: an equal-weight portfolio and the S&P Cryptocurrency Broad Digital Market Index (S&P BDM Index). The equal-weight strategy distributes the available cash equally among all available cryptocurrencies at time t0, while the S&P BDM index tracks the performance of cryptocurrencies with a market capitalization greater than $10 million.

## Rejecting Overfitted Agents

In this section, we investigate the performance of conventional and deep reinforcement learning (DRL) agents. We use a logit distribution function to measure the probability of overfitting, represented by the area under the curve for the domain [-\infty, 0].

Fig. 4 below shows the results for conventional agents, where we compare our PPO approach to the WF and KCV methods. We find that the WF method has a higher probability of overfitting at 17.5%, while the KCV method has a lower probability at 7.9%. Our PPO approach has an even lower probability of overfitting at 8.0%.

Fig. 5 presents the results for DRL agents, specifically PPO, TD3, and SAC. We find that the SAC method has the highest probability of overfitting at 21.3%, while PPO and TD3 have lower probabilities at 8.0% and 9.6%, respectively. Table \ref{tab: hyperparameters_per_agent} shows the specific hyperparameters used for each agent.

Overall, these results suggest that our PPO and TD3 methods have lower probabilities of overfitting compared to conventional and other DRL methods and could be more suitable for use in practical applications.

## Backtest Performance

In this section, we present the results of our backtest performance. We utilized two figures, Fig.6 and Fig. 7, to show the results of our agent’s performance. Our agent utilizes a CVIX indicator, which, when surpassing $90$, causes the agent to stop buying and selling all cryptocurrency holdings. We compare the performance of our agent with conventional agents, market benchmarks, and our approach. The results show that our method significantly outperforms the other two agents, PPO WF and PPO KCV, with at least a 15% increase in cumulative return. Additionally, our method also demonstrates lower volatility, indicating that it is more robust to risk. Furthermore, when comparing our approach to the DRL agents, TD3 and SAC, our PPO agent outperforms them with a significant increase in cumulative return (>24%) and lower volatility. Finally, when compared to the benchmarks, our approach outperforms in terms of cumulative return and volatility. Based on these results, we conclude that the PPO agent is the superior agent among those tested.

# Conclusion

Ahoy mateys! In this here paper, we set sail on a journey to uncover the treasure of addressing the backtesting overfitting issue in cryptocurrency trading with deep reinforcement learning. And let me tell ye. We struck gold! Our findings show that the PPO agent (with its trusty combinatorial CV method) outperforms all the other landlubbers, including the conventional agents, other DRL agents, and even the S&P DBM Index in both cumulative return and volatility. And let’s not forget. It’s mighty robust too!

As for future voyages, we are plannin’ on 1). huntin’ for the evolution of the probability of overfitting during training and for different agents; 2). testin’ limit order setting and trade closure; 3). explorin’ the vast seas of large-scale data, such as all currencies corresponding to the S&P BDM index; and 4). takin’ on more booty, such as fundamentals and sentiment features for the state space. So, hoist the anchor and set sail for the high seas of cryptocurrency trading with us!

~ Thanks for Reading ;)

Berend Gort

# Reference
    * Bailey, D. H., Borwein, J. M., López De Prado, M., & Zhu, Q. J. (2017). The probability of backtest overfitting. Journal of Computational Finance, 20(4), 39–69. https://doi.org/10.21314/JCF.2016.322

2. Prado, M. L. de. (2018). Advances in financial machine learning.",3.1235895572894514,75.0,Very Good
Introducing Layer Enhanced Classification (LEC),https://medium.com/data-science/introducing-layer-enhanced-classification-lec-4972f4f1c79f,Tula Masterman,378,385,3,816000.0,Lightweight Safety Classification Using Pruned Language Models,https://arxiv.org/pdf/2412.13435,TDS Archive,"Featured

# Introducing Layer Enhanced Classification (LEC)

## A novel approach for lightweight safety classification using pruned language models

## Leveraging the hidden state from an intermediate Transformer layer for efficient and robust content safety and prompt injection classification

# Introduction

As the adoption of Language Models (LMs) grows, it’s more and more important to detect inappropriate content in both the user’s input and the generated outputs of the language model. With each new model release from any major model provider, one of the first things people try to do is find ways to “jailbreak” or otherwise manipulate the model to respond in ways it shouldn’t. A quick search on Google or X reveals many examples of how people have found ways around model alignment tuning to get models to respond to inappropriate requests. Furthermore, many companies have released Generative AI based chatbots publicly for tasks like customer service, which often end up suffering from prompt injection attacks and responding to tasks both inappropriate and far beyond their intended use. Detecting and classifying these instances is extremely important for businesses so that they don’t end up with a system that can be easily manipulated by their users, especially if they deploy their chat systems publicly.

My team, Mason Sawtell, Sandi Besen, Jim Brown, and I recently published our paper Lightweight Safety Classification using Pruned Language Models as an ArXiv preprint. Our work introduces a new approach, Layer Enhanced Classification (LEC), and demonstrates that using LEC it is possible to effectively classify both content safety violations and prompt injection attacks by using the hidden state(s) from the intermediate transformer layer(s) of a Language Model to train a penalized logistic regression classifier with very few trainable parameters (769 on the low end) and a small number of training examples, often fewer than 100. This approach combines the computational efficiency of a simple classification model with the robust language understanding of a Language Model.

All of the models trained using our approach, LEC, outperform special-purpose models designed for each task as well as GPT-4o. We find that there are optimal intermediate transformer layers that produce the necessary features for both content safety and prompt injection classification tasks. This is important because it suggests you can use the same model to simultaneously classify content safety violations, prompt injections, and generate the output tokens. Alternatively, you could use a very small LM, prune it to the optimal intermediate layer, and use the outputs from this layer as the features for the classification task. This would allow for an incredibly compute efficient and lightweight classifier that integrates well with an existing LM inference pipeline.

This is the first of several articles I plan to share on this topic. In this article I will summarize the goals, approach, key results, and implications of our research. In a future article, I plan to share how we applied our approach to IBM’s Granite-8B model and an open-source model without any guardrails, allowing both models to detect content safety & prompt injection violations and generate output tokens all in one pass through the model. For further details on our research feel free to check out the full paper, read my colleague Sandi Besen’s article, or reach out with questions.

# Goals & Approach

Overview: Our research focuses on understanding how well the hidden states of intermediate transformer layers perform when used as the input features for classification tasks. We wanted to understand if small general-purpose models and special-purpose models for content safety and prompt injection classification tasks would perform better on these tasks if we could identify the optimal layer to use for the task instead of using the entire model / the last layer for classification. We also wanted to understand how small of a model, in terms of the total number of parameters, we could use as a starting point for this task. Other research has shown that different layers of the model focus on different characteristics of any given prompt input, our work finds that the intermediate layers tend to best capture the features that are most important for these classification tasks.

Datasets: For both content safety and prompt injection classification tasks we compare the performance of models trained using our approach to baseline models on task-specific datasets. Previous work indicated our classifiers would only see small performance improvements after a few hundred examples so for both classification tasks we used a task-specific dataset with 5,000 randomly sampled examples, allowing for enough data diversity while minimizing compute and training time. For the content safety dataset we use a combination of the SALAD Data dataset from OpenSafetyLab and the LMSYS-Chat-1M dataset from LMSYS. For the prompt injection dataset we use the SPML dataset since it includes system and user prompt pairs. This is critical because some user requests might seem “safe” (e.g., “help me solve this math problem”) but they ask the model to respond outside of the system’s intended use as defined in the system prompt (e.g. “You are a helpful AI assistant for Company X, you only respond to questions about our company”).

Model Selection: We use GPT-4o as a baseline model for both tasks since it is widely considered one of the most capable LLMs and in some cases outperformed the baseline special-purpose model(s). For content safety classification we use Llama Guard 3 1B and 8B models and for prompt injection classification we use Protect AI’s DeBERTA v3 Base Prompt Injection v2 model since these models are considered leaders in their respective areas. We apply our approach, LEC, to the baseline special purpose models (Llama Guard 3 1B, Llama Guard 3 8B, and DeBERTa v3 Base Prompt Injection) and general-purpose models. For general-purpose models we selected Qwen 2.5 Instruct in sizes 0.5B, 1.5B, and 3B since these models are relatively close in size to the special-purpose models.

This setup allows us to compare 3 key things:
    * How well our approach performs when applied to a small general-purpose model compared to both baseline models (GPT-4o and the special-purpose model).
    * How much applying our approach improves the performance of the special-purpose model relative to its own baseline performance on that task.
    * How well our approach generalizes across model architectures, by evaluating its performance on both general-purpose and
special-purpose models.

Important Implementation Details: For both Qwen 2.5 Instruct models and task-specific special-purpose models we prune individual layers and capture the hidden state of the transformer layer to train a Penalized Logistic Regression (PLR) model with L2 regularization. The PLR model has the same number of trainable parameters as the size of the model’s hidden state plus one for the bias in binary classification tasks, this ranges from 769 for the smallest model (Protect AI’s DeBERTa) to 4097 for the largest model (Llama Guard 3 8B). We train the classifier with varying numbers of examples for each layer allowing us to understand the impact of individual layers on the task and how many training examples are necessary to surpass the baseline models’ performance or achieve optimal performance in terms of F1 score. We run our entire test set through the baseline models to establish their performance on each task.

# Key Results

In this section I’ll cover the important results across both tasks and for each task, content safety classification and prompt injection classification, individually.

Key findings across both tasks:
    * Overall, our approach results in a higher F1 score across all evaluated tasks, models, and number of of training examples, typically surpassing baseline model performance within 20–100 examples.
    * The intermediate layers tend to show the largest improvement in F1 score compared to the final layer when trained on fewer examples. These layers also tend to have the best performance relative to the baseline models. This indicates that local features important to both classification tasks are represented early on in the transformer network and suggests that use cases with fewer training examples can especially benefit from our approach.
    * Furthermore, we found that applying our approach to the special-purpose models outperforms the models own baseline performance, typically within 20 examples, by identifying and using the most task-relevant layer.
    * Both general-purpose Qwen 2.5 Instruct models and task-specific special-purpose models achieve higher F1 scores within fewer examples with our approach. This suggests that our approach generalizes across architectures and domains.
    * In the Qwen 2.5 Instruct models, we find that the intermediate model layers attain higher F1 scores with fewer examples for both content safety and prompt injection classification tasks. This suggests that it’s feasible to use one model for both classification tasks and generate the outputs in one pass. The additional compute time for these extra classification steps would be almost negligible given the small size of the classifiers.

Content safety classification results:
    * For both binary and multi-class classification, the general and special purpose models trained using our approach typically outperform the baseline Llama Guard 3 models within 20 examples and GPT-4o in fewer than 100 examples.
    * For both binary and multi-class classification, the general and special purpose LEC models typically surpass all baseline models performance for the intermediate layers if not all layers. Our results on binary content safety classification surpass the baselines by the widest margins attaining maximum F1-scores of 0.95 or 0.96 for both Qwen 2.5 Instruct and Llama Guard LEC models. In comparison, GPT-4o’s baseline F1 score is 0.82, Llama Guard 3 1B’s is 0.65 , and Llama Guard 3 8B’s is 0.71.
    * For binary classification our approach performs comparably when applied to Qwen 2.5 Instruct 0.5B, Llama Guard 3 1B, and Llama Guard 3 8B. The models attain a maximum F1 score of 0.95, 0.96, and 0.96 respectively. Interestingly, Qwen 2.5 Instruct 0.5B surpasses GPT-4o’s baseline performance in 15 examples for the middle layers while it takes both Llama Guard 3 models 55 examples to do so.
    * For multi-class classification, a very small LEC model using the hidden state from the middle layers of Qwen 2.5 Instruct 0.5B surpasses GPT-4o’s baseline performance within 35 training examples for all three difficulty levels of the multi-class classification task.

Prompt injection classification results:
    * Applying our approach to both general-purpose Qwen 2.5 Instruct models and special-purpose DeBERTa v3 Prompt Injection v2 results in both models intermediate layers outperforming the baseline models in fewer than 100 training examples. This again indicates that our approach generalizes across model architectures and domains.
    * All three Qwen 2.5 Instruct model sizes surpass the baseline DeBERTa v3 Prompt Injection v2 model’s F1 score of 0.73 within 5 training examples for all model layers.
    * Qwen 2.5 Instruct 0.5B surpasses GPT-4o’s performance for the middle layer, layer 12 in 55 examples. Similar, but slightly better performance is observed for the larger Qwen 2.5 Instruct models.
    * Applying our approach to the DeBERTa v3 Prompt Injection v2 model results in a maximum F1 score of 0.98, significantly surpassing the model’s baseline performance F1 score of 0.73 on this task.
    * The intermediate layers achieve the highest weighted F1 scores for both the DeBERTa model and across Qwen 2.5 Instruct model sizes.

# Conclusion

In our research we focused on two responsible AI related classification tasks but expect this approach to work for other classification tasks provided that the important features for the task can be detected by the intermediate layers of the model.

We demonstrated that our approach of training a classification model on the hidden state from an intermediate transformer layer creates effective content safety and prompt injection classification models with minimal parameters and training examples. Furthermore, we illustrated how our approach improves the performance of existing special-purpose models compared to their own baseline results.

Our results suggest two promising options for integrating top-performing content safety and prompt injection classifiers into existing LLM inference workflows. One option is to take a lightweight small model like the ones explored in our paper, prune it to the optimal layer and use it as a feature extractor for the classification task. The classification model could then be used to identify any content safety violations or prompt injections before processing the user input with a closed-source model like GPT-4o. The same classification model could be used to validate the generated response before sending it to the user. A second option is to apply our approach to an open-source, general-purpose model, like IBM’s Granite or Meta’s Llama models, identify which layers are most relevant to the classification task, then update the inference pipeline to simultaneously classify content safety and prompt injections while generating the output response. If content safety or prompt injections are detected you could easily stop the output generation, otherwise if there are no violations, the model can continue generating it’s response. Either of these options could be extended to apply to AI-agent based scenarios depending on the model used for each agent.

In summary, LEC provides a new promising and practical solution to safeguarding Generative AI based systems by identifying content safety and prompt injection attacks with better performance and fewer training examples compared to existing approaches. This is critical for any person or business building with Generative AI today to ensure their systems are operating both responsibly and as intended.

Note: The opinions expressed both in this article and the research paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.

Interested in discussing further or collaborating? Reach out on LinkedIn!

Additional References:
    * Lightweight Safety Classification Using Pruned Language Models (see full references section in our paper on ArXiv)
    * OpenSafetyLab’s SALAD Dataset
    * LMSYS’ LMSYS-Chat-1M Dataset
    * SPML: A DSL for Defending Language Models Against Prompt Injection Attacks
    * Qwen 2.5 0.5B Instruct on HuggingFace
    * Llama Guard 3 Model Cards on Meta
    * ProtectAI’s DeBERTa v3 Prompt Injection v2 on HuggingFace
    * Large Language Models are Overparameterized Text Encoders
    * Logistic Regression makes small LLMs strong and explainable “tens-of-shot” classifiers",3.5316039612278747,85.0,Excellent
